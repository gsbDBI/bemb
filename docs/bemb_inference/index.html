
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://example.com/bemb_inference/">
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.0, mkdocs-material-8.5.3">
    
    
      
        <title>Tutorial for Inference with BEMB - Bayesian Embedding (BEMB)</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.7a952b86.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.cbb835fc.min.css">
        
          
          
          <meta name="theme-color" content="#02a6f2">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="light-blue" data-md-color-accent="">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#inference-using-the-bemb-model" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Bayesian Embedding (BEMB)" class="md-header__button md-logo" aria-label="Bayesian Embedding (BEMB)" data-md-component="logo">
      
  
  <?xml version="1.0" encoding="UTF-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="241.9pt" height="365pt" viewBox="0 0 241.9 365" version="1.1">
<defs>
<clipPath id="clip1">
  <path d="M 0 0 L 241.898438 0 L 241.898438 365 L 0 365 Z M 0 0 "/>
</clipPath>
</defs>
<g id="surface1">
<g clip-path="url(#clip1)" clip-rule="nonzero">
<path style=" stroke:none;fill-rule:nonzero;fill:rgb(60.398865%,10.598755%,11.799622%);fill-opacity:1;" d="M 235.886719 119.484375 C 235.886719 118.5625 235.886719 117.898438 235.886719 117.230469 C 235.886719 96.425781 235.875 75.621094 235.917969 54.820313 C 235.921875 53.394531 235.460938 52.414063 234.449219 51.449219 C 219.242188 36.867188 204.058594 22.257813 188.898438 7.625 C 187.800781 6.566406 186.710938 6.074219 185.140625 6.078125 C 142.371094 6.121094 99.605469 6.117188 56.835938 6.078125 C 55.355469 6.078125 54.261719 6.46875 53.21875 7.492188 C 49.527344 11.125 45.773438 14.699219 42.039063 18.289063 C 33.683594 26.320313 25.324219 34.347656 16.96875 42.382813 C 13.335938 45.871094 9.71875 49.378906 6.113281 52.855469 C 6.113281 53.300781 6.113281 53.640625 6.113281 53.980469 C 6.113281 96.339844 6.117188 138.695313 6.085938 181.054688 C 6.085938 182.261719 6.496094 183.066406 7.324219 183.894531 C 20.417969 196.941406 33.492188 210.011719 46.535156 223.109375 C 47.472656 224.050781 48.410156 224.464844 49.75 224.457031 C 57.320313 224.398438 64.890625 224.425781 72.464844 224.433594 C 73.042969 224.433594 73.621094 224.488281 74.644531 224.539063 C 71.742188 227.257813 70.460938 230.371094 69.59375 234.179688 C 71.082031 234.234375 72.199219 234.359375 73.308594 234.300781 C 78.976563 233.992188 84.304688 232.523438 89.242188 229.652344 C 90.832031 228.726563 92.546875 228.898438 94.125 229.796875 C 95.808594 230.753906 96.222656 232.242188 95.316406 233.96875 C 94.910156 234.746094 94.429688 235.511719 93.875 236.195313 C 90.800781 239.984375 86.703125 241.890625 81.96875 242.726563 C 79.636719 243.136719 77.316406 243.722656 75.074219 244.484375 C 73.582031 244.992188 72.875 246.195313 73.230469 247.292969 C 73.503906 248.140625 74.152344 248.90625 74.765625 249.589844 C 75.394531 250.289063 76.183594 250.84375 76.902344 251.460938 C 76.828125 251.609375 76.753906 251.761719 76.679688 251.910156 C 53.242188 251.910156 29.804688 251.910156 6.164063 251.910156 C 6.164063 252.78125 6.164063 253.449219 6.164063 254.117188 C 6.144531 272.804688 6.144531 291.496094 6.082031 310.183594 C 6.074219 311.609375 6.515625 312.585938 7.527344 313.554688 C 16.839844 322.457031 26.117188 331.402344 35.398438 340.34375 C 41.289063 346.019531 47.175781 351.699219 53.039063 357.40625 C 54.113281 358.449219 55.214844 358.96875 56.800781 358.96875 C 99.640625 358.917969 142.476563 358.921875 185.3125 358.960938 C 186.71875 358.960938 187.746094 358.570313 188.742188 357.605469 C 195.199219 351.339844 201.703125 345.117188 208.1875 338.878906 C 216.886719 330.511719 225.566406 322.132813 234.292969 313.792969 C 235.394531 312.738281 235.929688 311.679688 235.925781 310.082031 C 235.878906 270.109375 235.878906 230.140625 235.917969 190.167969 C 235.921875 188.683594 235.5 187.640625 234.445313 186.59375 C 221.4375 173.652344 208.464844 160.679688 195.523438 147.671875 C 194.46875 146.609375 193.417969 146.195313 191.9375 146.203125 C 181.980469 146.261719 172.019531 146.242188 162.0625 146.21875 C 161.265625 146.214844 160.328125 146.230469 159.699219 145.84375 C 155.351563 143.167969 150.949219 140.546875 146.800781 137.585938 C 143.5625 135.273438 141.082031 132.121094 139.085938 128.667969 C 138.125 127.007813 137.503906 125.199219 138.632813 123.179688 C 139.15625 123.265625 139.660156 123.234375 140.050781 123.429688 C 146.277344 126.621094 153.035156 127.347656 159.871094 127.726563 C 162.050781 127.847656 164.191406 128.058594 166.242188 128.980469 C 168.035156 129.785156 169.601563 128.535156 169.25 126.59375 C 169.085938 125.695313 168.609375 124.796875 168.085938 124.027344 C 167.070313 122.539063 165.925781 121.144531 164.660156 119.484375 C 188.476563 119.484375 211.996094 119.484375 235.886719 119.484375 Z M 0 315.160156 C 0 291.964844 0 268.773438 0 245.578125 C 0.609375 245.621094 1.21875 245.703125 1.828125 245.707031 C 23.648438 245.710938 45.472656 245.710938 67.296875 245.707031 C 67.894531 245.707031 68.496094 245.648438 69.097656 245.617188 C 69.898438 243.046875 71.621094 241.488281 73.90625 240.480469 C 74.777344 240.097656 75.699219 239.824219 76.550781 239.402344 C 76.941406 239.207031 77.21875 238.773438 77.542969 238.449219 C 77.46875 238.320313 77.394531 238.191406 77.320313 238.058594 C 76.046875 238.160156 74.773438 238.320313 73.5 238.347656 C 72.214844 238.371094 70.894531 238.429688 69.648438 238.179688 C 66.917969 237.628906 65.417969 235.339844 65.773438 232.585938 C 65.847656 231.992188 65.910156 231.398438 66.003906 230.550781 C 65.113281 230.550781 64.382813 230.550781 63.648438 230.550781 C 58.261719 230.550781 52.875 230.503906 47.488281 230.578125 C 46.007813 230.601563 44.960938 230.179688 43.90625 229.117188 C 29.859375 215.003906 15.769531 200.929688 1.679688 186.855469 C 1.160156 186.335938 0.5625 185.890625 0 185.410156 C 0 140.253906 0 95.09375 0 49.933594 C 0.332031 49.714844 0.710938 49.539063 0.992188 49.265625 C 4.589844 45.828125 8.171875 42.375 11.757813 38.929688 C 22.8125 28.300781 33.875 17.683594 44.917969 7.042969 C 47.3125 4.738281 49.628906 2.351563 51.980469 0 C 97.957031 0 143.9375 0 189.914063 0 C 190.371094 0.496094 190.804688 1.023438 191.292969 1.492188 C 195.417969 5.460938 199.558594 9.414063 203.6875 13.382813 C 211.546875 20.941406 219.402344 28.507813 227.257813 36.074219 C 232.113281 40.753906 236.964844 45.441406 241.703125 50.015625 C 241.703125 75.300781 241.703125 100.425781 241.703125 125.695313 C 218.851563 125.695313 196.160156 125.695313 173.40625 125.695313 C 173.40625 126.246094 173.421875 126.648438 173.402344 127.050781 C 173.167969 131.855469 169.175781 134.410156 164.660156 132.78125 C 163.15625 132.238281 161.507813 132.0625 159.910156 131.832031 C 158.632813 131.648438 157.320313 131.703125 156.039063 131.535156 C 152.410156 131.058594 148.785156 130.523438 145.085938 130.003906 C 145.410156 130.421875 145.71875 130.84375 146.050781 131.246094 C 148.84375 134.617188 152.519531 136.839844 156.195313 139.074219 C 157.402344 139.808594 158.613281 140.144531 160.042969 140.136719 C 171.5 140.09375 182.957031 140.140625 194.414063 140.078125 C 195.976563 140.070313 197.0625 140.550781 198.15625 141.648438 C 212.15625 155.714844 226.191406 169.75 240.261719 183.746094 C 241.410156 184.890625 241.902344 186.011719 241.902344 187.652344 C 241.855469 229.324219 241.867188 270.992188 241.867188 312.664063 C 241.867188 313.410156 241.867188 314.15625 241.867188 314.941406 C 224.503906 331.652344 207.171875 348.335938 189.855469 365 C 143.90625 365 98.019531 365 52.15625 365 C 49.609375 362.539063 47.113281 360.121094 44.609375 357.707031 C 30.621094 344.226563 16.636719 330.742188 2.636719 317.273438 C 1.828125 316.496094 0.882813 315.859375 0 315.160156 "/>
</g>
<path style=" stroke:none;fill-rule:nonzero;fill:rgb(100%,100%,100%);fill-opacity:1;" d="M 12.75 258.398438 C 12.71875 259.09375 12.667969 259.6875 12.667969 260.285156 C 12.632813 275.835938 12.636719 291.386719 12.527344 306.933594 C 12.515625 308.765625 13.140625 309.972656 14.410156 311.164063 C 19.472656 315.925781 24.476563 320.757813 29.488281 325.574219 C 38.429688 334.175781 47.375 342.777344 56.285156 351.40625 C 57.03125 352.128906 57.757813 352.464844 58.796875 352.449219 C 62.960938 352.398438 67.121094 352.472656 71.28125 352.417969 C 76.671875 352.347656 81.402344 350.507813 85.453125 346.933594 C 88.347656 344.371094 90.582031 341.277344 92.527344 337.984375 C 94.738281 334.253906 97.441406 330.957031 100.652344 328.054688 C 102.167969 326.683594 103.621094 325.238281 105.199219 323.9375 C 107.410156 322.117188 108.589844 319.746094 109.335938 317.054688 C 110.398438 313.207031 110.707031 309.253906 110.75 305.3125 C 110.882813 292.765625 110.859375 280.21875 110.878906 267.667969 C 110.882813 267.03125 110.722656 266.390625 110.640625 265.789063 C 109.265625 265.410156 108.386719 266.027344 107.609375 266.632813 C 106.375 267.59375 105.183594 268.632813 104.097656 269.753906 C 102.339844 271.5625 100.289063 272.878906 97.871094 273.5 C 95.902344 274.007813 93.882813 274.367188 91.863281 274.640625 C 89.441406 274.96875 86.878906 274.75 84.578125 275.441406 C 76.71875 277.800781 69.570313 281.523438 63.78125 287.511719 C 62.996094 288.320313 62.191406 289.050781 60.941406 288.664063 C 59.738281 288.289063 59.351563 287.253906 59.046875 286.1875 C 58.933594 285.800781 58.933594 285.378906 58.914063 284.972656 C 58.757813 281.015625 62.476563 270.03125 65.003906 266.992188 C 67.679688 263.773438 71.144531 261.648438 74.863281 259.871094 C 75.628906 259.503906 76.363281 259.082031 77.109375 258.6875 C 77.0625 258.589844 77.019531 258.496094 76.972656 258.398438 C 55.605469 258.398438 34.238281 258.398438 12.75 258.398438 Z M 172.03125 241.433594 C 170.605469 241.496094 169.546875 241.628906 168.496094 241.574219 C 162.835938 241.273438 157.476563 239.898438 152.546875 236.996094 C 150.722656 235.925781 148.867188 236.140625 147.15625 237.328125 C 145.804688 238.265625 145.417969 239.660156 146.234375 241.046875 C 146.992188 242.324219 147.835938 243.601563 148.859375 244.667969 C 151.964844 247.890625 155.894531 249.433594 160.273438 250.089844 C 162.277344 250.390625 164.3125 250.828125 166.1875 251.574219 C 168.828125 252.628906 169.503906 254.75 168.191406 257.242188 C 167.660156 258.253906 166.914063 259.175781 166.152344 260.03125 C 164.253906 262.160156 164.160156 264.085938 166.082031 266.179688 C 166.710938 266.863281 167.550781 267.375 168.34375 267.894531 C 169.882813 268.898438 171.503906 269.78125 172.996094 270.851563 C 175.296875 272.492188 177.421875 274.425781 178.417969 277.109375 C 180.027344 281.445313 181.433594 285.863281 182.75 290.296875 C 183.144531 291.628906 183.097656 293.140625 183.003906 294.554688 C 182.902344 296.148438 181.867188 296.878906 180.316406 296.464844 C 179.082031 296.132813 177.855469 295.625 176.761719 294.96875 C 174.191406 293.425781 171.757813 291.652344 169.171875 290.144531 C 165.941406 288.257813 162.6875 286.386719 159.304688 284.800781 C 156.441406 283.457031 153.332031 283.042969 150.121094 283.484375 C 148.371094 283.726563 146.585938 283.675781 144.816406 283.777344 C 141.253906 283.980469 138.101563 282.945313 135.308594 280.730469 C 134.363281 279.984375 133.34375 278.957031 132.054688 279.496094 C 130.582031 280.113281 130.871094 281.648438 130.867188 282.914063 C 130.851563 291.234375 130.847656 299.554688 130.867188 307.878906 C 130.871094 310.875 130.78125 313.886719 131.015625 316.871094 C 131.722656 325.789063 135.359375 332.960938 143.585938 337.296875 C 147.066406 339.128906 149.996094 341.652344 152.550781 344.640625 C 153.296875 345.511719 154.160156 346.339844 155.121094 346.949219 C 160.089844 350.117188 165.496094 352.117188 171.414063 352.402344 C 175.359375 352.589844 179.320313 352.457031 183.273438 352.539063 C 184.339844 352.5625 185.039063 352.128906 185.765625 351.425781 C 190.457031 346.863281 195.179688 342.335938 199.898438 337.800781 C 209.332031 328.726563 218.761719 319.648438 228.222656 310.605469 C 229.132813 309.738281 229.511719 308.851563 229.507813 307.574219 C 229.476563 269.246094 229.476563 230.914063 229.507813 192.582031 C 229.511719 191.308594 229.113281 190.433594 228.222656 189.554688 C 216.273438 177.679688 204.347656 165.78125 192.449219 153.859375 C 191.609375 153.015625 190.777344 152.664063 189.59375 152.671875 C 182.636719 152.722656 175.679688 152.695313 168.722656 152.699219 C 168.191406 152.699219 167.660156 152.746094 166.984375 152.78125 C 167.207031 153.324219 167.355469 153.765625 167.554688 154.179688 C 169.433594 158.035156 170.238281 162.179688 170.753906 166.382813 C 171.03125 168.664063 169.851563 169.792969 167.597656 169.328125 C 166.15625 169.03125 164.785156 168.363281 163.40625 167.800781 C 162.335938 167.367188 161.332031 166.765625 160.25 166.371094 C 155.332031 164.566406 150.398438 162.808594 145.46875 161.035156 C 144.636719 160.734375 143.820313 160.363281 142.964844 160.15625 C 141.582031 159.824219 140.425781 160.347656 139.445313 161.320313 C 138.472656 162.28125 138.863281 163.34375 139.273438 164.359375 C 140.578125 167.625 142.609375 170.296875 145.757813 171.988281 C 147.253906 172.789063 148.78125 173.550781 150.335938 174.242188 C 154.101563 175.910156 157.644531 177.925781 160.648438 180.769531 C 161.382813 181.464844 162.089844 182.25 162.578125 183.125 C 163.484375 184.753906 162.628906 185.945313 160.753906 185.828125 C 160.484375 185.8125 160.191406 185.789063 159.953125 185.679688 C 157.648438 184.601563 155.296875 184.886719 152.945313 185.445313 C 149.5625 186.253906 146.328125 185.863281 143.175781 184.421875 C 141.878906 183.828125 140.53125 183.328125 139.175781 182.890625 C 138.585938 182.699219 137.910156 182.761719 137.308594 182.714844 C 136.777344 183.953125 137.230469 184.816406 137.675781 185.652344 C 138.851563 187.851563 140.457031 189.664063 142.5 191.121094 C 147.054688 194.367188 151.722656 197.421875 156.734375 199.941406 C 164.527344 203.855469 168.496094 210.597656 170.34375 218.820313 C 170.503906 219.53125 170.3125 220.320313 170.265625 221.527344 C 168.789063 220.925781 167.566406 220.71875 166.746094 220.035156 C 164.597656 218.246094 162.183594 217.707031 159.503906 217.875 C 155.089844 218.152344 151.332031 216.757813 148.269531 213.511719 C 147.871094 213.089844 147.207031 212.910156 146.679688 212.625 C 144.632813 216.027344 145.734375 220.753906 148.84375 222.5625 C 149.777344 223.105469 150.824219 223.457031 151.816406 223.90625 C 155.414063 225.53125 159.050781 227.085938 162.59375 228.832031 C 164.160156 229.609375 165.648438 230.652344 166.972656 231.804688 C 169.734375 234.207031 171.183594 237.375 172.03125 241.433594 Z M 95.757813 205.304688 C 93.78125 205.304688 92.03125 205.417969 90.300781 205.269531 C 88.816406 205.140625 87.359375 204.703125 85.886719 204.417969 C 82.757813 203.808594 79.675781 203.613281 76.707031 205.183594 C 76.097656 205.503906 75.308594 205.476563 74.683594 205.59375 C 73.894531 204.425781 74.425781 203.554688 74.8125 202.707031 C 77.011719 197.871094 80.933594 194.742188 85.480469 192.378906 C 90.523438 189.761719 95.242188 186.667969 99.816406 183.328125 C 101.625 182.007813 103.039063 180.332031 103.933594 178.332031 C 104.5 177.070313 104.796875 175.597656 104.839844 174.207031 C 104.902344 172.167969 103.5 171.414063 101.707031 172.335938 C 101.40625 172.488281 101.109375 172.667969 100.835938 172.867188 C 94.964844 177.140625 88.488281 177.3125 81.738281 175.683594 C 81.507813 174.40625 81.484375 174.195313 82.246094 173.335938 C 83.832031 171.546875 85.433594 169.769531 87.660156 168.730469 C 89.453125 167.894531 91.273438 167.113281 93.011719 166.183594 C 94.804688 165.222656 96.640625 164.261719 98.238281 163.023438 C 100.507813 161.269531 101.964844 158.886719 102.511719 156.019531 C 102.679688 155.144531 102.847656 154.238281 101.539063 153.535156 C 100.746094 153.921875 99.679688 154.242188 98.863281 154.878906 C 95.441406 157.53125 91.484375 158.292969 87.316406 158.269531 C 82.019531 158.234375 77.566406 159.871094 74.308594 164.265625 C 73.644531 165.164063 72.835938 166.082031 71.472656 166.101563 C 70.191406 165.113281 70.3125 163.683594 70.476563 162.367188 C 70.835938 159.460938 71.285156 156.5625 71.820313 153.683594 C 73.140625 146.550781 76.871094 141.175781 83.363281 137.710938 C 86.488281 136.046875 89.445313 134.066406 92.433594 132.164063 C 95.582031 130.164063 98.128906 127.492188 100.335938 124.53125 C 101.257813 123.292969 102.03125 121.863281 102.511719 120.402344 C 103.140625 118.5 103.152344 116.527344 101.441406 114.648438 C 100.640625 115.214844 99.890625 115.742188 99.148438 116.277344 C 97.996094 117.117188 96.964844 118.3125 95.679688 118.738281 C 91.734375 120.050781 87.699219 121.105469 83.449219 120.410156 C 80.894531 119.996094 78.421875 120.257813 76.085938 121.441406 C 75.839844 121.566406 75.589844 121.679688 75.335938 121.769531 C 73.550781 122.382813 72.113281 121.296875 72.382813 119.410156 C 72.503906 118.570313 72.875 117.683594 73.378906 117 C 74.140625 115.964844 75.050781 115.019531 76.007813 114.15625 C 80.847656 109.800781 86.300781 106.296875 91.925781 103.078125 C 96.242188 100.609375 100.738281 98.464844 105.046875 95.988281 C 106.480469 95.164063 107.773438 93.914063 108.796875 92.601563 C 110.34375 90.613281 109.386719 88.445313 106.878906 88.140625 C 105.78125 88.007813 104.605469 88.339844 103.492188 88.585938 C 102.171875 88.878906 100.894531 89.53125 99.570313 89.621094 C 97.261719 89.777344 94.933594 89.671875 92.621094 89.570313 C 91.863281 89.535156 91.121094 89.167969 89.996094 88.84375 C 90.585938 87.824219 90.898438 87.003906 91.445313 86.394531 C 93 84.644531 94.863281 83.277344 97.023438 82.328125 C 99.207031 81.371094 101.394531 80.421875 103.554688 79.417969 C 105.476563 78.53125 106.699219 77.035156 107.039063 74.925781 C 107.40625 72.660156 106.378906 71.558594 104.121094 71.730469 C 103.105469 71.808594 102.085938 71.996094 101.070313 71.984375 C 100.15625 71.972656 99.25 71.757813 98.054688 71.589844 C 98.320313 70.585938 98.359375 69.8125 98.71875 69.238281 C 99.292969 68.324219 100.011719 67.476563 100.789063 66.722656 C 103.664063 63.929688 106.925781 61.761719 110.84375 60.671875 C 112.089844 60.324219 113.316406 59.867188 113.65625 58.261719 C 112.875 57.246094 111.746094 57.019531 110.59375 56.898438 C 109.171875 56.746094 107.726563 56.738281 106.328125 56.472656 C 105.667969 56.347656 104.746094 55.882813 104.582031 55.367188 C 104.425781 54.878906 104.945313 53.9375 105.441406 53.519531 C 106.839844 52.335938 108.417969 51.363281 109.855469 50.21875 C 111.71875 48.738281 113.601563 47.265625 115.308594 45.617188 C 116.519531 44.445313 116.363281 43.636719 115.085938 42.519531 C 114.160156 41.714844 113.144531 41.011719 112.222656 40.203125 C 111.570313 39.636719 110.820313 39.0625 111.085938 37.746094 C 111.613281 37.605469 112.230469 37.300781 112.855469 37.296875 C 114.582031 37.285156 115.714844 36.394531 116.371094 34.933594 C 116.980469 33.574219 117.488281 32.152344 117.882813 30.714844 C 118.5 28.480469 118.964844 26.210938 119.535156 23.964844 C 119.722656 23.21875 119.757813 22.285156 121.035156 22.195313 C 121.210938 22.925781 121.382813 23.644531 121.550781 24.363281 C 122.089844 26.6875 122.535156 29.03125 123.183594 31.324219 C 123.8125 33.53125 124.90625 35.339844 127.660156 35.3125 C 128.226563 35.308594 128.804688 35.777344 129.363281 36.023438 C 128.964844 37.851563 127.558594 38.503906 126.589844 39.507813 C 124.914063 41.25 125.261719 43.148438 127.515625 43.992188 C 128.460938 44.34375 129.503906 44.433594 130.507813 44.597656 C 132.433594 44.914063 132.867188 45.851563 131.613281 47.363281 C 130.839844 48.296875 129.851563 49.058594 128.910156 49.839844 C 127.152344 51.296875 126.9375 52.183594 128.390625 53.9375 C 129.425781 55.1875 130.6875 56.257813 131.933594 57.304688 C 133.132813 58.3125 134.511719 59.101563 135.683594 60.136719 C 138.261719 62.417969 138.078125 63.71875 135.089844 65.28125 C 134.789063 65.441406 134.488281 65.605469 134.210938 65.800781 C 133.445313 66.339844 132.515625 66.753906 132.546875 68.1875 C 133.074219 68.484375 133.699219 68.84375 134.324219 69.195313 C 137.695313 71.070313 140.769531 73.320313 142.949219 76.5625 C 144.042969 78.183594 143.53125 79.15625 141.574219 79.246094 C 140.222656 79.304688 138.847656 79.179688 137.5 78.996094 C 135.007813 78.660156 134.472656 80.128906 134.652344 81.835938 C 134.742188 82.6875 135.035156 83.558594 135.425781 84.328125 C 136.105469 85.660156 137.335938 86.410156 138.675781 87.003906 C 141.601563 88.300781 144.597656 89.472656 147.410156 90.988281 C 149.234375 91.96875 150.976563 93.277344 152.414063 94.769531 C 153.910156 96.328125 153.269531 98.144531 151.160156 98.628906 C 149.800781 98.9375 148.332031 98.890625 146.921875 98.816406 C 144.820313 98.703125 142.730469 98.265625 140.632813 98.261719 C 138.828125 98.257813 137.011719 98.625 135.554688 99.796875 C 135.585938 101.222656 136.3125 102.042969 137.285156 102.570313 C 138.714844 103.347656 140.167969 104.175781 141.714844 104.636719 C 146.890625 106.179688 151.464844 108.835938 155.75 112.027344 C 156.804688 112.8125 157.855469 113.105469 159.140625 113.097656 C 166.027344 113.058594 172.917969 113.089844 179.808594 113.078125 C 195.632813 113.054688 211.457031 113.019531 227.28125 112.988281 C 227.945313 112.984375 228.605469 112.929688 229.480469 112.890625 C 229.480469 111.953125 229.480469 111.152344 229.480469 110.351563 C 229.480469 92.960938 229.453125 75.5625 229.527344 58.171875 C 229.535156 56.347656 228.933594 55.144531 227.667969 53.9375 C 220.550781 47.175781 213.5 40.34375 206.421875 33.539063 C 199.640625 27.019531 192.820313 20.535156 186.09375 13.960938 C 185.007813 12.898438 183.9375 12.550781 182.484375 12.550781 C 141.488281 12.578125 100.492188 12.574219 59.496094 12.574219 C 59.222656 12.574219 58.949219 12.597656 58.675781 12.570313 C 57.714844 12.464844 57.035156 12.875 56.34375 13.539063 C 42.207031 27.167969 28.054688 40.78125 13.867188 54.355469 C 12.949219 55.238281 12.546875 56.085938 12.550781 57.355469 C 12.578125 97.605469 12.578125 137.851563 12.546875 178.097656 C 12.546875 179.367188 12.941406 180.246094 13.828125 181.132813 C 25.699219 192.992188 37.550781 204.871094 49.359375 216.792969 C 50.367188 217.804688 51.371094 218.164063 52.769531 218.15625 C 62.25 218.101563 71.734375 218.015625 81.210938 218.183594 C 84.789063 218.246094 88 217.4375 91.269531 216.117188 C 96 214.210938 96.300781 210.089844 95.9375 205.519531 C 95.925781 205.363281 95.683594 205.222656 95.757813 205.304688 Z M 235.886719 119.484375 C 211.996094 119.484375 188.476563 119.484375 164.660156 119.484375 C 165.925781 121.144531 167.070313 122.539063 168.085938 124.027344 C 168.609375 124.796875 169.085938 125.695313 169.25 126.59375 C 169.601563 128.535156 168.035156 129.785156 166.242188 128.980469 C 164.191406 128.058594 162.050781 127.847656 159.871094 127.726563 C 153.035156 127.347656 146.277344 126.621094 140.050781 123.429688 C 139.660156 123.234375 139.15625 123.265625 138.632813 123.179688 C 137.503906 125.199219 138.125 127.007813 139.085938 128.667969 C 141.082031 132.121094 143.5625 135.273438 146.800781 137.585938 C 150.949219 140.546875 155.351563 143.167969 159.699219 145.84375 C 160.328125 146.230469 161.265625 146.214844 162.0625 146.21875 C 172.019531 146.242188 181.980469 146.261719 191.9375 146.203125 C 193.417969 146.195313 194.46875 146.609375 195.523438 147.671875 C 208.464844 160.679688 221.4375 173.652344 234.445313 186.59375 C 235.5 187.640625 235.921875 188.683594 235.917969 190.167969 C 235.878906 230.140625 235.878906 270.109375 235.925781 310.082031 C 235.929688 311.679688 235.394531 312.738281 234.292969 313.792969 C 225.566406 322.132813 216.886719 330.511719 208.1875 338.878906 C 201.703125 345.117188 195.199219 351.339844 188.742188 357.605469 C 187.746094 358.570313 186.71875 358.960938 185.3125 358.960938 C 142.476563 358.921875 99.640625 358.917969 56.800781 358.96875 C 55.214844 358.96875 54.113281 358.449219 53.039063 357.40625 C 47.175781 351.699219 41.289063 346.019531 35.398438 340.34375 C 26.117188 331.402344 16.839844 322.457031 7.527344 313.554688 C 6.515625 312.585938 6.074219 311.609375 6.082031 310.183594 C 6.144531 291.496094 6.144531 272.804688 6.164063 254.117188 C 6.164063 253.449219 6.164063 252.78125 6.164063 251.910156 C 29.804688 251.910156 53.242188 251.910156 76.679688 251.910156 C 76.753906 251.761719 76.828125 251.609375 76.902344 251.460938 C 76.183594 250.84375 75.394531 250.289063 74.765625 249.589844 C 74.152344 248.90625 73.503906 248.140625 73.230469 247.292969 C 72.875 246.195313 73.582031 244.992188 75.074219 244.484375 C 77.316406 243.722656 79.636719 243.136719 81.96875 242.726563 C 86.703125 241.890625 90.800781 239.984375 93.875 236.195313 C 94.429688 235.511719 94.910156 234.746094 95.316406 233.96875 C 96.222656 232.242188 95.808594 230.753906 94.125 229.796875 C 92.546875 228.898438 90.832031 228.726563 89.242188 229.652344 C 84.304688 232.523438 78.976563 233.992188 73.308594 234.300781 C 72.199219 234.359375 71.082031 234.234375 69.59375 234.179688 C 70.460938 230.371094 71.742188 227.257813 74.644531 224.539063 C 73.621094 224.488281 73.042969 224.433594 72.464844 224.433594 C 64.890625 224.425781 57.320313 224.398438 49.75 224.457031 C 48.410156 224.464844 47.472656 224.050781 46.535156 223.109375 C 33.492188 210.011719 20.417969 196.941406 7.324219 183.894531 C 6.496094 183.066406 6.085938 182.261719 6.085938 181.054688 C 6.117188 138.695313 6.113281 96.339844 6.113281 53.980469 C 6.113281 53.640625 6.113281 53.300781 6.113281 52.855469 C 9.71875 49.378906 13.335938 45.871094 16.96875 42.382813 C 25.324219 34.347656 33.683594 26.320313 42.039063 18.289063 C 45.773438 14.699219 49.527344 11.125 53.21875 7.492188 C 54.261719 6.46875 55.355469 6.078125 56.835938 6.078125 C 99.605469 6.117188 142.371094 6.121094 185.140625 6.078125 C 186.710938 6.074219 187.800781 6.566406 188.898438 7.625 C 204.058594 22.257813 219.242188 36.867188 234.449219 51.449219 C 235.460938 52.414063 235.921875 53.394531 235.917969 54.820313 C 235.875 75.621094 235.886719 96.425781 235.886719 117.230469 C 235.886719 117.898438 235.886719 118.5625 235.886719 119.484375 "/>
<path style=" stroke:none;fill-rule:nonzero;fill:rgb(60.398865%,10.598755%,11.799622%);fill-opacity:1;" d="M 95.757813 205.304688 C 95.683594 205.222656 95.925781 205.363281 95.9375 205.519531 C 96.300781 210.089844 96 214.210938 91.269531 216.117188 C 88 217.4375 84.789063 218.246094 81.210938 218.183594 C 71.734375 218.015625 62.25 218.101563 52.769531 218.15625 C 51.371094 218.164063 50.367188 217.804688 49.359375 216.792969 C 37.550781 204.871094 25.699219 192.992188 13.828125 181.132813 C 12.941406 180.246094 12.546875 179.367188 12.546875 178.097656 C 12.578125 137.851563 12.578125 97.605469 12.550781 57.355469 C 12.546875 56.085938 12.949219 55.238281 13.867188 54.355469 C 28.054688 40.78125 42.207031 27.167969 56.34375 13.539063 C 57.035156 12.875 57.714844 12.464844 58.675781 12.570313 C 58.949219 12.597656 59.222656 12.574219 59.496094 12.574219 C 100.492188 12.574219 141.488281 12.578125 182.484375 12.550781 C 183.9375 12.550781 185.007813 12.898438 186.09375 13.960938 C 192.820313 20.535156 199.640625 27.019531 206.421875 33.539063 C 213.5 40.34375 220.550781 47.175781 227.667969 53.9375 C 228.933594 55.144531 229.535156 56.347656 229.527344 58.171875 C 229.453125 75.5625 229.480469 92.960938 229.480469 110.351563 C 229.480469 111.152344 229.480469 111.953125 229.480469 112.890625 C 228.605469 112.929688 227.945313 112.984375 227.28125 112.988281 C 211.457031 113.019531 195.632813 113.054688 179.808594 113.078125 C 172.917969 113.089844 166.027344 113.058594 159.140625 113.097656 C 157.855469 113.105469 156.804688 112.8125 155.75 112.027344 C 151.464844 108.835938 146.890625 106.179688 141.714844 104.636719 C 140.167969 104.175781 138.714844 103.347656 137.285156 102.570313 C 136.3125 102.042969 135.585938 101.222656 135.554688 99.796875 C 137.011719 98.625 138.828125 98.257813 140.632813 98.261719 C 142.730469 98.265625 144.820313 98.703125 146.921875 98.816406 C 148.332031 98.890625 149.800781 98.9375 151.160156 98.628906 C 153.269531 98.144531 153.910156 96.328125 152.414063 94.769531 C 150.976563 93.277344 149.234375 91.96875 147.410156 90.988281 C 144.597656 89.472656 141.601563 88.300781 138.675781 87.003906 C 137.335938 86.410156 136.105469 85.660156 135.425781 84.328125 C 135.035156 83.558594 134.742188 82.6875 134.652344 81.835938 C 134.472656 80.128906 135.007813 78.660156 137.5 78.996094 C 138.847656 79.179688 140.222656 79.304688 141.574219 79.246094 C 143.53125 79.15625 144.042969 78.183594 142.949219 76.5625 C 140.769531 73.320313 137.695313 71.070313 134.324219 69.195313 C 133.699219 68.84375 133.074219 68.484375 132.546875 68.1875 C 132.515625 66.753906 133.445313 66.339844 134.210938 65.800781 C 134.488281 65.605469 134.789063 65.441406 135.089844 65.28125 C 138.078125 63.71875 138.261719 62.417969 135.683594 60.136719 C 134.511719 59.101563 133.132813 58.3125 131.933594 57.304688 C 130.6875 56.257813 129.425781 55.1875 128.390625 53.9375 C 126.9375 52.183594 127.152344 51.296875 128.910156 49.839844 C 129.851563 49.058594 130.839844 48.296875 131.613281 47.363281 C 132.867188 45.851563 132.433594 44.914063 130.507813 44.597656 C 129.503906 44.433594 128.460938 44.34375 127.515625 43.992188 C 125.261719 43.148438 124.914063 41.25 126.589844 39.507813 C 127.558594 38.503906 128.964844 37.851563 129.363281 36.023438 C 128.804688 35.777344 128.226563 35.308594 127.660156 35.3125 C 124.90625 35.339844 123.8125 33.53125 123.183594 31.324219 C 122.535156 29.03125 122.089844 26.6875 121.550781 24.363281 C 121.382813 23.644531 121.210938 22.925781 121.035156 22.195313 C 119.757813 22.285156 119.722656 23.21875 119.535156 23.964844 C 118.964844 26.210938 118.5 28.480469 117.882813 30.714844 C 117.488281 32.152344 116.980469 33.574219 116.371094 34.933594 C 115.714844 36.394531 114.582031 37.285156 112.855469 37.296875 C 112.230469 37.300781 111.613281 37.605469 111.085938 37.746094 C 110.820313 39.0625 111.570313 39.636719 112.222656 40.203125 C 113.144531 41.011719 114.160156 41.714844 115.085938 42.519531 C 116.363281 43.636719 116.519531 44.445313 115.308594 45.617188 C 113.601563 47.265625 111.71875 48.738281 109.855469 50.21875 C 108.417969 51.363281 106.839844 52.335938 105.441406 53.519531 C 104.945313 53.9375 104.425781 54.878906 104.582031 55.367188 C 104.746094 55.882813 105.667969 56.347656 106.328125 56.472656 C 107.726563 56.738281 109.171875 56.746094 110.59375 56.898438 C 111.746094 57.019531 112.875 57.246094 113.65625 58.261719 C 113.316406 59.867188 112.089844 60.324219 110.84375 60.671875 C 106.925781 61.761719 103.664063 63.929688 100.789063 66.722656 C 100.011719 67.476563 99.292969 68.324219 98.71875 69.238281 C 98.359375 69.8125 98.320313 70.585938 98.054688 71.589844 C 99.25 71.757813 100.15625 71.972656 101.070313 71.984375 C 102.085938 71.996094 103.105469 71.808594 104.121094 71.730469 C 106.378906 71.558594 107.40625 72.660156 107.039063 74.925781 C 106.699219 77.035156 105.476563 78.53125 103.554688 79.417969 C 101.394531 80.421875 99.207031 81.371094 97.023438 82.328125 C 94.863281 83.277344 93 84.644531 91.445313 86.394531 C 90.898438 87.003906 90.585938 87.824219 89.996094 88.84375 C 91.121094 89.167969 91.863281 89.535156 92.621094 89.570313 C 94.933594 89.671875 97.261719 89.777344 99.570313 89.621094 C 100.894531 89.53125 102.171875 88.878906 103.492188 88.585938 C 104.605469 88.339844 105.78125 88.007813 106.878906 88.140625 C 109.386719 88.445313 110.34375 90.613281 108.796875 92.601563 C 107.773438 93.914063 106.480469 95.164063 105.046875 95.988281 C 100.738281 98.464844 96.242188 100.609375 91.925781 103.078125 C 86.300781 106.296875 80.847656 109.800781 76.007813 114.15625 C 75.050781 115.019531 74.140625 115.964844 73.378906 117 C 72.875 117.683594 72.503906 118.570313 72.382813 119.410156 C 72.113281 121.296875 73.550781 122.382813 75.335938 121.769531 C 75.589844 121.679688 75.839844 121.566406 76.085938 121.441406 C 78.421875 120.257813 80.894531 119.996094 83.449219 120.410156 C 87.699219 121.105469 91.734375 120.050781 95.679688 118.738281 C 96.964844 118.3125 97.996094 117.117188 99.148438 116.277344 C 99.890625 115.742188 100.640625 115.214844 101.441406 114.648438 C 103.152344 116.527344 103.140625 118.5 102.511719 120.402344 C 102.03125 121.863281 101.257813 123.292969 100.335938 124.53125 C 98.128906 127.492188 95.582031 130.164063 92.433594 132.164063 C 89.445313 134.066406 86.488281 136.046875 83.363281 137.710938 C 76.871094 141.175781 73.140625 146.550781 71.820313 153.683594 C 71.285156 156.5625 70.835938 159.460938 70.476563 162.367188 C 70.3125 163.683594 70.191406 165.113281 71.472656 166.101563 C 72.835938 166.082031 73.644531 165.164063 74.308594 164.265625 C 77.566406 159.871094 82.019531 158.234375 87.316406 158.269531 C 91.484375 158.292969 95.441406 157.53125 98.863281 154.878906 C 99.679688 154.242188 100.746094 153.921875 101.539063 153.535156 C 102.847656 154.238281 102.679688 155.144531 102.511719 156.019531 C 101.964844 158.886719 100.507813 161.269531 98.238281 163.023438 C 96.640625 164.261719 94.804688 165.222656 93.011719 166.183594 C 91.273438 167.113281 89.453125 167.894531 87.660156 168.730469 C 85.433594 169.769531 83.832031 171.546875 82.246094 173.335938 C 81.484375 174.195313 81.507813 174.40625 81.738281 175.683594 C 88.488281 177.3125 94.964844 177.140625 100.835938 172.867188 C 101.109375 172.667969 101.40625 172.488281 101.707031 172.335938 C 103.5 171.414063 104.902344 172.167969 104.839844 174.207031 C 104.796875 175.597656 104.5 177.070313 103.933594 178.332031 C 103.039063 180.332031 101.625 182.007813 99.816406 183.328125 C 95.242188 186.667969 90.523438 189.761719 85.480469 192.378906 C 80.933594 194.742188 77.011719 197.871094 74.8125 202.707031 C 74.425781 203.554688 73.894531 204.425781 74.683594 205.59375 C 75.308594 205.476563 76.097656 205.503906 76.707031 205.183594 C 79.675781 203.613281 82.757813 203.808594 85.886719 204.417969 C 87.359375 204.703125 88.816406 205.140625 90.300781 205.269531 C 92.03125 205.417969 93.78125 205.304688 95.757813 205.304688 "/>
<path style=" stroke:none;fill-rule:nonzero;fill:rgb(60.398865%,10.598755%,11.799622%);fill-opacity:1;" d="M 172.03125 241.433594 C 171.183594 237.375 169.734375 234.207031 166.972656 231.804688 C 165.648438 230.652344 164.160156 229.609375 162.59375 228.832031 C 159.050781 227.085938 155.414063 225.53125 151.816406 223.90625 C 150.824219 223.457031 149.777344 223.105469 148.84375 222.5625 C 145.734375 220.753906 144.632813 216.027344 146.679688 212.625 C 147.207031 212.910156 147.871094 213.089844 148.269531 213.511719 C 151.332031 216.757813 155.089844 218.152344 159.503906 217.875 C 162.183594 217.707031 164.597656 218.246094 166.746094 220.035156 C 167.566406 220.71875 168.789063 220.925781 170.265625 221.527344 C 170.3125 220.320313 170.503906 219.53125 170.34375 218.820313 C 168.496094 210.597656 164.527344 203.855469 156.734375 199.941406 C 151.722656 197.421875 147.054688 194.367188 142.5 191.121094 C 140.457031 189.664063 138.851563 187.851563 137.675781 185.652344 C 137.230469 184.816406 136.777344 183.953125 137.308594 182.714844 C 137.910156 182.761719 138.585938 182.699219 139.175781 182.890625 C 140.53125 183.328125 141.878906 183.828125 143.175781 184.421875 C 146.328125 185.863281 149.5625 186.253906 152.945313 185.445313 C 155.296875 184.886719 157.648438 184.601563 159.953125 185.679688 C 160.191406 185.789063 160.484375 185.8125 160.753906 185.828125 C 162.628906 185.945313 163.484375 184.753906 162.578125 183.125 C 162.089844 182.25 161.382813 181.464844 160.648438 180.769531 C 157.644531 177.925781 154.101563 175.910156 150.335938 174.242188 C 148.78125 173.550781 147.253906 172.789063 145.757813 171.988281 C 142.609375 170.296875 140.578125 167.625 139.273438 164.359375 C 138.863281 163.34375 138.472656 162.28125 139.445313 161.320313 C 140.425781 160.347656 141.582031 159.824219 142.964844 160.15625 C 143.820313 160.363281 144.636719 160.734375 145.46875 161.035156 C 150.398438 162.808594 155.332031 164.566406 160.25 166.371094 C 161.332031 166.765625 162.335938 167.367188 163.40625 167.800781 C 164.785156 168.363281 166.15625 169.03125 167.597656 169.328125 C 169.851563 169.792969 171.03125 168.664063 170.753906 166.382813 C 170.238281 162.179688 169.433594 158.035156 167.554688 154.179688 C 167.355469 153.765625 167.207031 153.324219 166.984375 152.78125 C 167.660156 152.746094 168.191406 152.699219 168.722656 152.699219 C 175.679688 152.695313 182.636719 152.722656 189.59375 152.671875 C 190.777344 152.664063 191.609375 153.015625 192.449219 153.859375 C 204.347656 165.78125 216.273438 177.679688 228.222656 189.554688 C 229.113281 190.433594 229.511719 191.308594 229.507813 192.582031 C 229.476563 230.914063 229.476563 269.246094 229.507813 307.574219 C 229.511719 308.851563 229.132813 309.738281 228.222656 310.605469 C 218.761719 319.648438 209.332031 328.726563 199.898438 337.800781 C 195.179688 342.335938 190.457031 346.863281 185.765625 351.425781 C 185.039063 352.128906 184.339844 352.5625 183.273438 352.539063 C 179.320313 352.457031 175.359375 352.589844 171.414063 352.402344 C 165.496094 352.117188 160.089844 350.117188 155.121094 346.949219 C 154.160156 346.339844 153.296875 345.511719 152.550781 344.640625 C 149.996094 341.652344 147.066406 339.128906 143.585938 337.296875 C 135.359375 332.960938 131.722656 325.789063 131.015625 316.871094 C 130.78125 313.886719 130.871094 310.875 130.867188 307.878906 C 130.847656 299.554688 130.851563 291.234375 130.867188 282.914063 C 130.871094 281.648438 130.582031 280.113281 132.054688 279.496094 C 133.34375 278.957031 134.363281 279.984375 135.308594 280.730469 C 138.101563 282.945313 141.253906 283.980469 144.816406 283.777344 C 146.585938 283.675781 148.371094 283.726563 150.121094 283.484375 C 153.332031 283.042969 156.441406 283.457031 159.304688 284.800781 C 162.6875 286.386719 165.941406 288.257813 169.171875 290.144531 C 171.757813 291.652344 174.191406 293.425781 176.761719 294.96875 C 177.855469 295.625 179.082031 296.132813 180.316406 296.464844 C 181.867188 296.878906 182.902344 296.148438 183.003906 294.554688 C 183.097656 293.140625 183.144531 291.628906 182.75 290.296875 C 181.433594 285.863281 180.027344 281.445313 178.417969 277.109375 C 177.421875 274.425781 175.296875 272.492188 172.996094 270.851563 C 171.503906 269.78125 169.882813 268.898438 168.34375 267.894531 C 167.550781 267.375 166.710938 266.863281 166.082031 266.179688 C 164.160156 264.085938 164.253906 262.160156 166.152344 260.03125 C 166.914063 259.175781 167.660156 258.253906 168.191406 257.242188 C 169.503906 254.75 168.828125 252.628906 166.1875 251.574219 C 164.3125 250.828125 162.277344 250.390625 160.273438 250.089844 C 155.894531 249.433594 151.964844 247.890625 148.859375 244.667969 C 147.835938 243.601563 146.992188 242.324219 146.234375 241.046875 C 145.417969 239.660156 145.804688 238.265625 147.15625 237.328125 C 148.867188 236.140625 150.722656 235.925781 152.546875 236.996094 C 157.476563 239.898438 162.835938 241.273438 168.496094 241.574219 C 169.546875 241.628906 170.605469 241.496094 172.03125 241.433594 "/>
<path style=" stroke:none;fill-rule:nonzero;fill:rgb(60.398865%,10.598755%,11.799622%);fill-opacity:1;" d="M 12.75 258.398438 C 34.238281 258.398438 55.605469 258.398438 76.972656 258.398438 C 77.019531 258.496094 77.0625 258.589844 77.109375 258.6875 C 76.363281 259.082031 75.628906 259.503906 74.863281 259.871094 C 71.144531 261.648438 67.679688 263.773438 65.003906 266.992188 C 62.476563 270.03125 58.757813 281.015625 58.914063 284.972656 C 58.933594 285.378906 58.933594 285.800781 59.046875 286.1875 C 59.351563 287.253906 59.738281 288.289063 60.941406 288.664063 C 62.191406 289.050781 62.996094 288.320313 63.78125 287.511719 C 69.570313 281.523438 76.71875 277.800781 84.578125 275.441406 C 86.878906 274.75 89.441406 274.96875 91.863281 274.640625 C 93.882813 274.367188 95.902344 274.007813 97.871094 273.5 C 100.289063 272.878906 102.339844 271.5625 104.097656 269.753906 C 105.183594 268.632813 106.375 267.59375 107.609375 266.632813 C 108.386719 266.027344 109.265625 265.410156 110.640625 265.789063 C 110.722656 266.390625 110.882813 267.03125 110.878906 267.667969 C 110.859375 280.21875 110.882813 292.765625 110.75 305.3125 C 110.707031 309.253906 110.398438 313.207031 109.335938 317.054688 C 108.589844 319.746094 107.410156 322.117188 105.199219 323.9375 C 103.621094 325.238281 102.167969 326.683594 100.652344 328.054688 C 97.441406 330.957031 94.738281 334.253906 92.527344 337.984375 C 90.582031 341.277344 88.347656 344.371094 85.453125 346.933594 C 81.402344 350.507813 76.671875 352.347656 71.28125 352.417969 C 67.121094 352.472656 62.960938 352.398438 58.796875 352.449219 C 57.757813 352.464844 57.03125 352.128906 56.285156 351.40625 C 47.375 342.777344 38.429688 334.175781 29.488281 325.574219 C 24.476563 320.757813 19.472656 315.925781 14.410156 311.164063 C 13.140625 309.972656 12.515625 308.765625 12.527344 306.933594 C 12.636719 291.386719 12.632813 275.835938 12.667969 260.285156 C 12.667969 259.6875 12.71875 259.09375 12.75 258.398438 "/>
</g>
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Bayesian Embedding (BEMB)
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Tutorial for Inference with BEMB
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Bayesian Embedding (BEMB)" class="md-nav__button md-logo" aria-label="Bayesian Embedding (BEMB)" data-md-component="logo">
      
  
  <?xml version="1.0" encoding="UTF-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="241.9pt" height="365pt" viewBox="0 0 241.9 365" version="1.1">
<defs>
<clipPath id="clip1">
  <path d="M 0 0 L 241.898438 0 L 241.898438 365 L 0 365 Z M 0 0 "/>
</clipPath>
</defs>
<g id="surface1">
<g clip-path="url(#clip1)" clip-rule="nonzero">
<path style=" stroke:none;fill-rule:nonzero;fill:rgb(60.398865%,10.598755%,11.799622%);fill-opacity:1;" d="M 235.886719 119.484375 C 235.886719 118.5625 235.886719 117.898438 235.886719 117.230469 C 235.886719 96.425781 235.875 75.621094 235.917969 54.820313 C 235.921875 53.394531 235.460938 52.414063 234.449219 51.449219 C 219.242188 36.867188 204.058594 22.257813 188.898438 7.625 C 187.800781 6.566406 186.710938 6.074219 185.140625 6.078125 C 142.371094 6.121094 99.605469 6.117188 56.835938 6.078125 C 55.355469 6.078125 54.261719 6.46875 53.21875 7.492188 C 49.527344 11.125 45.773438 14.699219 42.039063 18.289063 C 33.683594 26.320313 25.324219 34.347656 16.96875 42.382813 C 13.335938 45.871094 9.71875 49.378906 6.113281 52.855469 C 6.113281 53.300781 6.113281 53.640625 6.113281 53.980469 C 6.113281 96.339844 6.117188 138.695313 6.085938 181.054688 C 6.085938 182.261719 6.496094 183.066406 7.324219 183.894531 C 20.417969 196.941406 33.492188 210.011719 46.535156 223.109375 C 47.472656 224.050781 48.410156 224.464844 49.75 224.457031 C 57.320313 224.398438 64.890625 224.425781 72.464844 224.433594 C 73.042969 224.433594 73.621094 224.488281 74.644531 224.539063 C 71.742188 227.257813 70.460938 230.371094 69.59375 234.179688 C 71.082031 234.234375 72.199219 234.359375 73.308594 234.300781 C 78.976563 233.992188 84.304688 232.523438 89.242188 229.652344 C 90.832031 228.726563 92.546875 228.898438 94.125 229.796875 C 95.808594 230.753906 96.222656 232.242188 95.316406 233.96875 C 94.910156 234.746094 94.429688 235.511719 93.875 236.195313 C 90.800781 239.984375 86.703125 241.890625 81.96875 242.726563 C 79.636719 243.136719 77.316406 243.722656 75.074219 244.484375 C 73.582031 244.992188 72.875 246.195313 73.230469 247.292969 C 73.503906 248.140625 74.152344 248.90625 74.765625 249.589844 C 75.394531 250.289063 76.183594 250.84375 76.902344 251.460938 C 76.828125 251.609375 76.753906 251.761719 76.679688 251.910156 C 53.242188 251.910156 29.804688 251.910156 6.164063 251.910156 C 6.164063 252.78125 6.164063 253.449219 6.164063 254.117188 C 6.144531 272.804688 6.144531 291.496094 6.082031 310.183594 C 6.074219 311.609375 6.515625 312.585938 7.527344 313.554688 C 16.839844 322.457031 26.117188 331.402344 35.398438 340.34375 C 41.289063 346.019531 47.175781 351.699219 53.039063 357.40625 C 54.113281 358.449219 55.214844 358.96875 56.800781 358.96875 C 99.640625 358.917969 142.476563 358.921875 185.3125 358.960938 C 186.71875 358.960938 187.746094 358.570313 188.742188 357.605469 C 195.199219 351.339844 201.703125 345.117188 208.1875 338.878906 C 216.886719 330.511719 225.566406 322.132813 234.292969 313.792969 C 235.394531 312.738281 235.929688 311.679688 235.925781 310.082031 C 235.878906 270.109375 235.878906 230.140625 235.917969 190.167969 C 235.921875 188.683594 235.5 187.640625 234.445313 186.59375 C 221.4375 173.652344 208.464844 160.679688 195.523438 147.671875 C 194.46875 146.609375 193.417969 146.195313 191.9375 146.203125 C 181.980469 146.261719 172.019531 146.242188 162.0625 146.21875 C 161.265625 146.214844 160.328125 146.230469 159.699219 145.84375 C 155.351563 143.167969 150.949219 140.546875 146.800781 137.585938 C 143.5625 135.273438 141.082031 132.121094 139.085938 128.667969 C 138.125 127.007813 137.503906 125.199219 138.632813 123.179688 C 139.15625 123.265625 139.660156 123.234375 140.050781 123.429688 C 146.277344 126.621094 153.035156 127.347656 159.871094 127.726563 C 162.050781 127.847656 164.191406 128.058594 166.242188 128.980469 C 168.035156 129.785156 169.601563 128.535156 169.25 126.59375 C 169.085938 125.695313 168.609375 124.796875 168.085938 124.027344 C 167.070313 122.539063 165.925781 121.144531 164.660156 119.484375 C 188.476563 119.484375 211.996094 119.484375 235.886719 119.484375 Z M 0 315.160156 C 0 291.964844 0 268.773438 0 245.578125 C 0.609375 245.621094 1.21875 245.703125 1.828125 245.707031 C 23.648438 245.710938 45.472656 245.710938 67.296875 245.707031 C 67.894531 245.707031 68.496094 245.648438 69.097656 245.617188 C 69.898438 243.046875 71.621094 241.488281 73.90625 240.480469 C 74.777344 240.097656 75.699219 239.824219 76.550781 239.402344 C 76.941406 239.207031 77.21875 238.773438 77.542969 238.449219 C 77.46875 238.320313 77.394531 238.191406 77.320313 238.058594 C 76.046875 238.160156 74.773438 238.320313 73.5 238.347656 C 72.214844 238.371094 70.894531 238.429688 69.648438 238.179688 C 66.917969 237.628906 65.417969 235.339844 65.773438 232.585938 C 65.847656 231.992188 65.910156 231.398438 66.003906 230.550781 C 65.113281 230.550781 64.382813 230.550781 63.648438 230.550781 C 58.261719 230.550781 52.875 230.503906 47.488281 230.578125 C 46.007813 230.601563 44.960938 230.179688 43.90625 229.117188 C 29.859375 215.003906 15.769531 200.929688 1.679688 186.855469 C 1.160156 186.335938 0.5625 185.890625 0 185.410156 C 0 140.253906 0 95.09375 0 49.933594 C 0.332031 49.714844 0.710938 49.539063 0.992188 49.265625 C 4.589844 45.828125 8.171875 42.375 11.757813 38.929688 C 22.8125 28.300781 33.875 17.683594 44.917969 7.042969 C 47.3125 4.738281 49.628906 2.351563 51.980469 0 C 97.957031 0 143.9375 0 189.914063 0 C 190.371094 0.496094 190.804688 1.023438 191.292969 1.492188 C 195.417969 5.460938 199.558594 9.414063 203.6875 13.382813 C 211.546875 20.941406 219.402344 28.507813 227.257813 36.074219 C 232.113281 40.753906 236.964844 45.441406 241.703125 50.015625 C 241.703125 75.300781 241.703125 100.425781 241.703125 125.695313 C 218.851563 125.695313 196.160156 125.695313 173.40625 125.695313 C 173.40625 126.246094 173.421875 126.648438 173.402344 127.050781 C 173.167969 131.855469 169.175781 134.410156 164.660156 132.78125 C 163.15625 132.238281 161.507813 132.0625 159.910156 131.832031 C 158.632813 131.648438 157.320313 131.703125 156.039063 131.535156 C 152.410156 131.058594 148.785156 130.523438 145.085938 130.003906 C 145.410156 130.421875 145.71875 130.84375 146.050781 131.246094 C 148.84375 134.617188 152.519531 136.839844 156.195313 139.074219 C 157.402344 139.808594 158.613281 140.144531 160.042969 140.136719 C 171.5 140.09375 182.957031 140.140625 194.414063 140.078125 C 195.976563 140.070313 197.0625 140.550781 198.15625 141.648438 C 212.15625 155.714844 226.191406 169.75 240.261719 183.746094 C 241.410156 184.890625 241.902344 186.011719 241.902344 187.652344 C 241.855469 229.324219 241.867188 270.992188 241.867188 312.664063 C 241.867188 313.410156 241.867188 314.15625 241.867188 314.941406 C 224.503906 331.652344 207.171875 348.335938 189.855469 365 C 143.90625 365 98.019531 365 52.15625 365 C 49.609375 362.539063 47.113281 360.121094 44.609375 357.707031 C 30.621094 344.226563 16.636719 330.742188 2.636719 317.273438 C 1.828125 316.496094 0.882813 315.859375 0 315.160156 "/>
</g>
<path style=" stroke:none;fill-rule:nonzero;fill:rgb(100%,100%,100%);fill-opacity:1;" d="M 12.75 258.398438 C 12.71875 259.09375 12.667969 259.6875 12.667969 260.285156 C 12.632813 275.835938 12.636719 291.386719 12.527344 306.933594 C 12.515625 308.765625 13.140625 309.972656 14.410156 311.164063 C 19.472656 315.925781 24.476563 320.757813 29.488281 325.574219 C 38.429688 334.175781 47.375 342.777344 56.285156 351.40625 C 57.03125 352.128906 57.757813 352.464844 58.796875 352.449219 C 62.960938 352.398438 67.121094 352.472656 71.28125 352.417969 C 76.671875 352.347656 81.402344 350.507813 85.453125 346.933594 C 88.347656 344.371094 90.582031 341.277344 92.527344 337.984375 C 94.738281 334.253906 97.441406 330.957031 100.652344 328.054688 C 102.167969 326.683594 103.621094 325.238281 105.199219 323.9375 C 107.410156 322.117188 108.589844 319.746094 109.335938 317.054688 C 110.398438 313.207031 110.707031 309.253906 110.75 305.3125 C 110.882813 292.765625 110.859375 280.21875 110.878906 267.667969 C 110.882813 267.03125 110.722656 266.390625 110.640625 265.789063 C 109.265625 265.410156 108.386719 266.027344 107.609375 266.632813 C 106.375 267.59375 105.183594 268.632813 104.097656 269.753906 C 102.339844 271.5625 100.289063 272.878906 97.871094 273.5 C 95.902344 274.007813 93.882813 274.367188 91.863281 274.640625 C 89.441406 274.96875 86.878906 274.75 84.578125 275.441406 C 76.71875 277.800781 69.570313 281.523438 63.78125 287.511719 C 62.996094 288.320313 62.191406 289.050781 60.941406 288.664063 C 59.738281 288.289063 59.351563 287.253906 59.046875 286.1875 C 58.933594 285.800781 58.933594 285.378906 58.914063 284.972656 C 58.757813 281.015625 62.476563 270.03125 65.003906 266.992188 C 67.679688 263.773438 71.144531 261.648438 74.863281 259.871094 C 75.628906 259.503906 76.363281 259.082031 77.109375 258.6875 C 77.0625 258.589844 77.019531 258.496094 76.972656 258.398438 C 55.605469 258.398438 34.238281 258.398438 12.75 258.398438 Z M 172.03125 241.433594 C 170.605469 241.496094 169.546875 241.628906 168.496094 241.574219 C 162.835938 241.273438 157.476563 239.898438 152.546875 236.996094 C 150.722656 235.925781 148.867188 236.140625 147.15625 237.328125 C 145.804688 238.265625 145.417969 239.660156 146.234375 241.046875 C 146.992188 242.324219 147.835938 243.601563 148.859375 244.667969 C 151.964844 247.890625 155.894531 249.433594 160.273438 250.089844 C 162.277344 250.390625 164.3125 250.828125 166.1875 251.574219 C 168.828125 252.628906 169.503906 254.75 168.191406 257.242188 C 167.660156 258.253906 166.914063 259.175781 166.152344 260.03125 C 164.253906 262.160156 164.160156 264.085938 166.082031 266.179688 C 166.710938 266.863281 167.550781 267.375 168.34375 267.894531 C 169.882813 268.898438 171.503906 269.78125 172.996094 270.851563 C 175.296875 272.492188 177.421875 274.425781 178.417969 277.109375 C 180.027344 281.445313 181.433594 285.863281 182.75 290.296875 C 183.144531 291.628906 183.097656 293.140625 183.003906 294.554688 C 182.902344 296.148438 181.867188 296.878906 180.316406 296.464844 C 179.082031 296.132813 177.855469 295.625 176.761719 294.96875 C 174.191406 293.425781 171.757813 291.652344 169.171875 290.144531 C 165.941406 288.257813 162.6875 286.386719 159.304688 284.800781 C 156.441406 283.457031 153.332031 283.042969 150.121094 283.484375 C 148.371094 283.726563 146.585938 283.675781 144.816406 283.777344 C 141.253906 283.980469 138.101563 282.945313 135.308594 280.730469 C 134.363281 279.984375 133.34375 278.957031 132.054688 279.496094 C 130.582031 280.113281 130.871094 281.648438 130.867188 282.914063 C 130.851563 291.234375 130.847656 299.554688 130.867188 307.878906 C 130.871094 310.875 130.78125 313.886719 131.015625 316.871094 C 131.722656 325.789063 135.359375 332.960938 143.585938 337.296875 C 147.066406 339.128906 149.996094 341.652344 152.550781 344.640625 C 153.296875 345.511719 154.160156 346.339844 155.121094 346.949219 C 160.089844 350.117188 165.496094 352.117188 171.414063 352.402344 C 175.359375 352.589844 179.320313 352.457031 183.273438 352.539063 C 184.339844 352.5625 185.039063 352.128906 185.765625 351.425781 C 190.457031 346.863281 195.179688 342.335938 199.898438 337.800781 C 209.332031 328.726563 218.761719 319.648438 228.222656 310.605469 C 229.132813 309.738281 229.511719 308.851563 229.507813 307.574219 C 229.476563 269.246094 229.476563 230.914063 229.507813 192.582031 C 229.511719 191.308594 229.113281 190.433594 228.222656 189.554688 C 216.273438 177.679688 204.347656 165.78125 192.449219 153.859375 C 191.609375 153.015625 190.777344 152.664063 189.59375 152.671875 C 182.636719 152.722656 175.679688 152.695313 168.722656 152.699219 C 168.191406 152.699219 167.660156 152.746094 166.984375 152.78125 C 167.207031 153.324219 167.355469 153.765625 167.554688 154.179688 C 169.433594 158.035156 170.238281 162.179688 170.753906 166.382813 C 171.03125 168.664063 169.851563 169.792969 167.597656 169.328125 C 166.15625 169.03125 164.785156 168.363281 163.40625 167.800781 C 162.335938 167.367188 161.332031 166.765625 160.25 166.371094 C 155.332031 164.566406 150.398438 162.808594 145.46875 161.035156 C 144.636719 160.734375 143.820313 160.363281 142.964844 160.15625 C 141.582031 159.824219 140.425781 160.347656 139.445313 161.320313 C 138.472656 162.28125 138.863281 163.34375 139.273438 164.359375 C 140.578125 167.625 142.609375 170.296875 145.757813 171.988281 C 147.253906 172.789063 148.78125 173.550781 150.335938 174.242188 C 154.101563 175.910156 157.644531 177.925781 160.648438 180.769531 C 161.382813 181.464844 162.089844 182.25 162.578125 183.125 C 163.484375 184.753906 162.628906 185.945313 160.753906 185.828125 C 160.484375 185.8125 160.191406 185.789063 159.953125 185.679688 C 157.648438 184.601563 155.296875 184.886719 152.945313 185.445313 C 149.5625 186.253906 146.328125 185.863281 143.175781 184.421875 C 141.878906 183.828125 140.53125 183.328125 139.175781 182.890625 C 138.585938 182.699219 137.910156 182.761719 137.308594 182.714844 C 136.777344 183.953125 137.230469 184.816406 137.675781 185.652344 C 138.851563 187.851563 140.457031 189.664063 142.5 191.121094 C 147.054688 194.367188 151.722656 197.421875 156.734375 199.941406 C 164.527344 203.855469 168.496094 210.597656 170.34375 218.820313 C 170.503906 219.53125 170.3125 220.320313 170.265625 221.527344 C 168.789063 220.925781 167.566406 220.71875 166.746094 220.035156 C 164.597656 218.246094 162.183594 217.707031 159.503906 217.875 C 155.089844 218.152344 151.332031 216.757813 148.269531 213.511719 C 147.871094 213.089844 147.207031 212.910156 146.679688 212.625 C 144.632813 216.027344 145.734375 220.753906 148.84375 222.5625 C 149.777344 223.105469 150.824219 223.457031 151.816406 223.90625 C 155.414063 225.53125 159.050781 227.085938 162.59375 228.832031 C 164.160156 229.609375 165.648438 230.652344 166.972656 231.804688 C 169.734375 234.207031 171.183594 237.375 172.03125 241.433594 Z M 95.757813 205.304688 C 93.78125 205.304688 92.03125 205.417969 90.300781 205.269531 C 88.816406 205.140625 87.359375 204.703125 85.886719 204.417969 C 82.757813 203.808594 79.675781 203.613281 76.707031 205.183594 C 76.097656 205.503906 75.308594 205.476563 74.683594 205.59375 C 73.894531 204.425781 74.425781 203.554688 74.8125 202.707031 C 77.011719 197.871094 80.933594 194.742188 85.480469 192.378906 C 90.523438 189.761719 95.242188 186.667969 99.816406 183.328125 C 101.625 182.007813 103.039063 180.332031 103.933594 178.332031 C 104.5 177.070313 104.796875 175.597656 104.839844 174.207031 C 104.902344 172.167969 103.5 171.414063 101.707031 172.335938 C 101.40625 172.488281 101.109375 172.667969 100.835938 172.867188 C 94.964844 177.140625 88.488281 177.3125 81.738281 175.683594 C 81.507813 174.40625 81.484375 174.195313 82.246094 173.335938 C 83.832031 171.546875 85.433594 169.769531 87.660156 168.730469 C 89.453125 167.894531 91.273438 167.113281 93.011719 166.183594 C 94.804688 165.222656 96.640625 164.261719 98.238281 163.023438 C 100.507813 161.269531 101.964844 158.886719 102.511719 156.019531 C 102.679688 155.144531 102.847656 154.238281 101.539063 153.535156 C 100.746094 153.921875 99.679688 154.242188 98.863281 154.878906 C 95.441406 157.53125 91.484375 158.292969 87.316406 158.269531 C 82.019531 158.234375 77.566406 159.871094 74.308594 164.265625 C 73.644531 165.164063 72.835938 166.082031 71.472656 166.101563 C 70.191406 165.113281 70.3125 163.683594 70.476563 162.367188 C 70.835938 159.460938 71.285156 156.5625 71.820313 153.683594 C 73.140625 146.550781 76.871094 141.175781 83.363281 137.710938 C 86.488281 136.046875 89.445313 134.066406 92.433594 132.164063 C 95.582031 130.164063 98.128906 127.492188 100.335938 124.53125 C 101.257813 123.292969 102.03125 121.863281 102.511719 120.402344 C 103.140625 118.5 103.152344 116.527344 101.441406 114.648438 C 100.640625 115.214844 99.890625 115.742188 99.148438 116.277344 C 97.996094 117.117188 96.964844 118.3125 95.679688 118.738281 C 91.734375 120.050781 87.699219 121.105469 83.449219 120.410156 C 80.894531 119.996094 78.421875 120.257813 76.085938 121.441406 C 75.839844 121.566406 75.589844 121.679688 75.335938 121.769531 C 73.550781 122.382813 72.113281 121.296875 72.382813 119.410156 C 72.503906 118.570313 72.875 117.683594 73.378906 117 C 74.140625 115.964844 75.050781 115.019531 76.007813 114.15625 C 80.847656 109.800781 86.300781 106.296875 91.925781 103.078125 C 96.242188 100.609375 100.738281 98.464844 105.046875 95.988281 C 106.480469 95.164063 107.773438 93.914063 108.796875 92.601563 C 110.34375 90.613281 109.386719 88.445313 106.878906 88.140625 C 105.78125 88.007813 104.605469 88.339844 103.492188 88.585938 C 102.171875 88.878906 100.894531 89.53125 99.570313 89.621094 C 97.261719 89.777344 94.933594 89.671875 92.621094 89.570313 C 91.863281 89.535156 91.121094 89.167969 89.996094 88.84375 C 90.585938 87.824219 90.898438 87.003906 91.445313 86.394531 C 93 84.644531 94.863281 83.277344 97.023438 82.328125 C 99.207031 81.371094 101.394531 80.421875 103.554688 79.417969 C 105.476563 78.53125 106.699219 77.035156 107.039063 74.925781 C 107.40625 72.660156 106.378906 71.558594 104.121094 71.730469 C 103.105469 71.808594 102.085938 71.996094 101.070313 71.984375 C 100.15625 71.972656 99.25 71.757813 98.054688 71.589844 C 98.320313 70.585938 98.359375 69.8125 98.71875 69.238281 C 99.292969 68.324219 100.011719 67.476563 100.789063 66.722656 C 103.664063 63.929688 106.925781 61.761719 110.84375 60.671875 C 112.089844 60.324219 113.316406 59.867188 113.65625 58.261719 C 112.875 57.246094 111.746094 57.019531 110.59375 56.898438 C 109.171875 56.746094 107.726563 56.738281 106.328125 56.472656 C 105.667969 56.347656 104.746094 55.882813 104.582031 55.367188 C 104.425781 54.878906 104.945313 53.9375 105.441406 53.519531 C 106.839844 52.335938 108.417969 51.363281 109.855469 50.21875 C 111.71875 48.738281 113.601563 47.265625 115.308594 45.617188 C 116.519531 44.445313 116.363281 43.636719 115.085938 42.519531 C 114.160156 41.714844 113.144531 41.011719 112.222656 40.203125 C 111.570313 39.636719 110.820313 39.0625 111.085938 37.746094 C 111.613281 37.605469 112.230469 37.300781 112.855469 37.296875 C 114.582031 37.285156 115.714844 36.394531 116.371094 34.933594 C 116.980469 33.574219 117.488281 32.152344 117.882813 30.714844 C 118.5 28.480469 118.964844 26.210938 119.535156 23.964844 C 119.722656 23.21875 119.757813 22.285156 121.035156 22.195313 C 121.210938 22.925781 121.382813 23.644531 121.550781 24.363281 C 122.089844 26.6875 122.535156 29.03125 123.183594 31.324219 C 123.8125 33.53125 124.90625 35.339844 127.660156 35.3125 C 128.226563 35.308594 128.804688 35.777344 129.363281 36.023438 C 128.964844 37.851563 127.558594 38.503906 126.589844 39.507813 C 124.914063 41.25 125.261719 43.148438 127.515625 43.992188 C 128.460938 44.34375 129.503906 44.433594 130.507813 44.597656 C 132.433594 44.914063 132.867188 45.851563 131.613281 47.363281 C 130.839844 48.296875 129.851563 49.058594 128.910156 49.839844 C 127.152344 51.296875 126.9375 52.183594 128.390625 53.9375 C 129.425781 55.1875 130.6875 56.257813 131.933594 57.304688 C 133.132813 58.3125 134.511719 59.101563 135.683594 60.136719 C 138.261719 62.417969 138.078125 63.71875 135.089844 65.28125 C 134.789063 65.441406 134.488281 65.605469 134.210938 65.800781 C 133.445313 66.339844 132.515625 66.753906 132.546875 68.1875 C 133.074219 68.484375 133.699219 68.84375 134.324219 69.195313 C 137.695313 71.070313 140.769531 73.320313 142.949219 76.5625 C 144.042969 78.183594 143.53125 79.15625 141.574219 79.246094 C 140.222656 79.304688 138.847656 79.179688 137.5 78.996094 C 135.007813 78.660156 134.472656 80.128906 134.652344 81.835938 C 134.742188 82.6875 135.035156 83.558594 135.425781 84.328125 C 136.105469 85.660156 137.335938 86.410156 138.675781 87.003906 C 141.601563 88.300781 144.597656 89.472656 147.410156 90.988281 C 149.234375 91.96875 150.976563 93.277344 152.414063 94.769531 C 153.910156 96.328125 153.269531 98.144531 151.160156 98.628906 C 149.800781 98.9375 148.332031 98.890625 146.921875 98.816406 C 144.820313 98.703125 142.730469 98.265625 140.632813 98.261719 C 138.828125 98.257813 137.011719 98.625 135.554688 99.796875 C 135.585938 101.222656 136.3125 102.042969 137.285156 102.570313 C 138.714844 103.347656 140.167969 104.175781 141.714844 104.636719 C 146.890625 106.179688 151.464844 108.835938 155.75 112.027344 C 156.804688 112.8125 157.855469 113.105469 159.140625 113.097656 C 166.027344 113.058594 172.917969 113.089844 179.808594 113.078125 C 195.632813 113.054688 211.457031 113.019531 227.28125 112.988281 C 227.945313 112.984375 228.605469 112.929688 229.480469 112.890625 C 229.480469 111.953125 229.480469 111.152344 229.480469 110.351563 C 229.480469 92.960938 229.453125 75.5625 229.527344 58.171875 C 229.535156 56.347656 228.933594 55.144531 227.667969 53.9375 C 220.550781 47.175781 213.5 40.34375 206.421875 33.539063 C 199.640625 27.019531 192.820313 20.535156 186.09375 13.960938 C 185.007813 12.898438 183.9375 12.550781 182.484375 12.550781 C 141.488281 12.578125 100.492188 12.574219 59.496094 12.574219 C 59.222656 12.574219 58.949219 12.597656 58.675781 12.570313 C 57.714844 12.464844 57.035156 12.875 56.34375 13.539063 C 42.207031 27.167969 28.054688 40.78125 13.867188 54.355469 C 12.949219 55.238281 12.546875 56.085938 12.550781 57.355469 C 12.578125 97.605469 12.578125 137.851563 12.546875 178.097656 C 12.546875 179.367188 12.941406 180.246094 13.828125 181.132813 C 25.699219 192.992188 37.550781 204.871094 49.359375 216.792969 C 50.367188 217.804688 51.371094 218.164063 52.769531 218.15625 C 62.25 218.101563 71.734375 218.015625 81.210938 218.183594 C 84.789063 218.246094 88 217.4375 91.269531 216.117188 C 96 214.210938 96.300781 210.089844 95.9375 205.519531 C 95.925781 205.363281 95.683594 205.222656 95.757813 205.304688 Z M 235.886719 119.484375 C 211.996094 119.484375 188.476563 119.484375 164.660156 119.484375 C 165.925781 121.144531 167.070313 122.539063 168.085938 124.027344 C 168.609375 124.796875 169.085938 125.695313 169.25 126.59375 C 169.601563 128.535156 168.035156 129.785156 166.242188 128.980469 C 164.191406 128.058594 162.050781 127.847656 159.871094 127.726563 C 153.035156 127.347656 146.277344 126.621094 140.050781 123.429688 C 139.660156 123.234375 139.15625 123.265625 138.632813 123.179688 C 137.503906 125.199219 138.125 127.007813 139.085938 128.667969 C 141.082031 132.121094 143.5625 135.273438 146.800781 137.585938 C 150.949219 140.546875 155.351563 143.167969 159.699219 145.84375 C 160.328125 146.230469 161.265625 146.214844 162.0625 146.21875 C 172.019531 146.242188 181.980469 146.261719 191.9375 146.203125 C 193.417969 146.195313 194.46875 146.609375 195.523438 147.671875 C 208.464844 160.679688 221.4375 173.652344 234.445313 186.59375 C 235.5 187.640625 235.921875 188.683594 235.917969 190.167969 C 235.878906 230.140625 235.878906 270.109375 235.925781 310.082031 C 235.929688 311.679688 235.394531 312.738281 234.292969 313.792969 C 225.566406 322.132813 216.886719 330.511719 208.1875 338.878906 C 201.703125 345.117188 195.199219 351.339844 188.742188 357.605469 C 187.746094 358.570313 186.71875 358.960938 185.3125 358.960938 C 142.476563 358.921875 99.640625 358.917969 56.800781 358.96875 C 55.214844 358.96875 54.113281 358.449219 53.039063 357.40625 C 47.175781 351.699219 41.289063 346.019531 35.398438 340.34375 C 26.117188 331.402344 16.839844 322.457031 7.527344 313.554688 C 6.515625 312.585938 6.074219 311.609375 6.082031 310.183594 C 6.144531 291.496094 6.144531 272.804688 6.164063 254.117188 C 6.164063 253.449219 6.164063 252.78125 6.164063 251.910156 C 29.804688 251.910156 53.242188 251.910156 76.679688 251.910156 C 76.753906 251.761719 76.828125 251.609375 76.902344 251.460938 C 76.183594 250.84375 75.394531 250.289063 74.765625 249.589844 C 74.152344 248.90625 73.503906 248.140625 73.230469 247.292969 C 72.875 246.195313 73.582031 244.992188 75.074219 244.484375 C 77.316406 243.722656 79.636719 243.136719 81.96875 242.726563 C 86.703125 241.890625 90.800781 239.984375 93.875 236.195313 C 94.429688 235.511719 94.910156 234.746094 95.316406 233.96875 C 96.222656 232.242188 95.808594 230.753906 94.125 229.796875 C 92.546875 228.898438 90.832031 228.726563 89.242188 229.652344 C 84.304688 232.523438 78.976563 233.992188 73.308594 234.300781 C 72.199219 234.359375 71.082031 234.234375 69.59375 234.179688 C 70.460938 230.371094 71.742188 227.257813 74.644531 224.539063 C 73.621094 224.488281 73.042969 224.433594 72.464844 224.433594 C 64.890625 224.425781 57.320313 224.398438 49.75 224.457031 C 48.410156 224.464844 47.472656 224.050781 46.535156 223.109375 C 33.492188 210.011719 20.417969 196.941406 7.324219 183.894531 C 6.496094 183.066406 6.085938 182.261719 6.085938 181.054688 C 6.117188 138.695313 6.113281 96.339844 6.113281 53.980469 C 6.113281 53.640625 6.113281 53.300781 6.113281 52.855469 C 9.71875 49.378906 13.335938 45.871094 16.96875 42.382813 C 25.324219 34.347656 33.683594 26.320313 42.039063 18.289063 C 45.773438 14.699219 49.527344 11.125 53.21875 7.492188 C 54.261719 6.46875 55.355469 6.078125 56.835938 6.078125 C 99.605469 6.117188 142.371094 6.121094 185.140625 6.078125 C 186.710938 6.074219 187.800781 6.566406 188.898438 7.625 C 204.058594 22.257813 219.242188 36.867188 234.449219 51.449219 C 235.460938 52.414063 235.921875 53.394531 235.917969 54.820313 C 235.875 75.621094 235.886719 96.425781 235.886719 117.230469 C 235.886719 117.898438 235.886719 118.5625 235.886719 119.484375 "/>
<path style=" stroke:none;fill-rule:nonzero;fill:rgb(60.398865%,10.598755%,11.799622%);fill-opacity:1;" d="M 95.757813 205.304688 C 95.683594 205.222656 95.925781 205.363281 95.9375 205.519531 C 96.300781 210.089844 96 214.210938 91.269531 216.117188 C 88 217.4375 84.789063 218.246094 81.210938 218.183594 C 71.734375 218.015625 62.25 218.101563 52.769531 218.15625 C 51.371094 218.164063 50.367188 217.804688 49.359375 216.792969 C 37.550781 204.871094 25.699219 192.992188 13.828125 181.132813 C 12.941406 180.246094 12.546875 179.367188 12.546875 178.097656 C 12.578125 137.851563 12.578125 97.605469 12.550781 57.355469 C 12.546875 56.085938 12.949219 55.238281 13.867188 54.355469 C 28.054688 40.78125 42.207031 27.167969 56.34375 13.539063 C 57.035156 12.875 57.714844 12.464844 58.675781 12.570313 C 58.949219 12.597656 59.222656 12.574219 59.496094 12.574219 C 100.492188 12.574219 141.488281 12.578125 182.484375 12.550781 C 183.9375 12.550781 185.007813 12.898438 186.09375 13.960938 C 192.820313 20.535156 199.640625 27.019531 206.421875 33.539063 C 213.5 40.34375 220.550781 47.175781 227.667969 53.9375 C 228.933594 55.144531 229.535156 56.347656 229.527344 58.171875 C 229.453125 75.5625 229.480469 92.960938 229.480469 110.351563 C 229.480469 111.152344 229.480469 111.953125 229.480469 112.890625 C 228.605469 112.929688 227.945313 112.984375 227.28125 112.988281 C 211.457031 113.019531 195.632813 113.054688 179.808594 113.078125 C 172.917969 113.089844 166.027344 113.058594 159.140625 113.097656 C 157.855469 113.105469 156.804688 112.8125 155.75 112.027344 C 151.464844 108.835938 146.890625 106.179688 141.714844 104.636719 C 140.167969 104.175781 138.714844 103.347656 137.285156 102.570313 C 136.3125 102.042969 135.585938 101.222656 135.554688 99.796875 C 137.011719 98.625 138.828125 98.257813 140.632813 98.261719 C 142.730469 98.265625 144.820313 98.703125 146.921875 98.816406 C 148.332031 98.890625 149.800781 98.9375 151.160156 98.628906 C 153.269531 98.144531 153.910156 96.328125 152.414063 94.769531 C 150.976563 93.277344 149.234375 91.96875 147.410156 90.988281 C 144.597656 89.472656 141.601563 88.300781 138.675781 87.003906 C 137.335938 86.410156 136.105469 85.660156 135.425781 84.328125 C 135.035156 83.558594 134.742188 82.6875 134.652344 81.835938 C 134.472656 80.128906 135.007813 78.660156 137.5 78.996094 C 138.847656 79.179688 140.222656 79.304688 141.574219 79.246094 C 143.53125 79.15625 144.042969 78.183594 142.949219 76.5625 C 140.769531 73.320313 137.695313 71.070313 134.324219 69.195313 C 133.699219 68.84375 133.074219 68.484375 132.546875 68.1875 C 132.515625 66.753906 133.445313 66.339844 134.210938 65.800781 C 134.488281 65.605469 134.789063 65.441406 135.089844 65.28125 C 138.078125 63.71875 138.261719 62.417969 135.683594 60.136719 C 134.511719 59.101563 133.132813 58.3125 131.933594 57.304688 C 130.6875 56.257813 129.425781 55.1875 128.390625 53.9375 C 126.9375 52.183594 127.152344 51.296875 128.910156 49.839844 C 129.851563 49.058594 130.839844 48.296875 131.613281 47.363281 C 132.867188 45.851563 132.433594 44.914063 130.507813 44.597656 C 129.503906 44.433594 128.460938 44.34375 127.515625 43.992188 C 125.261719 43.148438 124.914063 41.25 126.589844 39.507813 C 127.558594 38.503906 128.964844 37.851563 129.363281 36.023438 C 128.804688 35.777344 128.226563 35.308594 127.660156 35.3125 C 124.90625 35.339844 123.8125 33.53125 123.183594 31.324219 C 122.535156 29.03125 122.089844 26.6875 121.550781 24.363281 C 121.382813 23.644531 121.210938 22.925781 121.035156 22.195313 C 119.757813 22.285156 119.722656 23.21875 119.535156 23.964844 C 118.964844 26.210938 118.5 28.480469 117.882813 30.714844 C 117.488281 32.152344 116.980469 33.574219 116.371094 34.933594 C 115.714844 36.394531 114.582031 37.285156 112.855469 37.296875 C 112.230469 37.300781 111.613281 37.605469 111.085938 37.746094 C 110.820313 39.0625 111.570313 39.636719 112.222656 40.203125 C 113.144531 41.011719 114.160156 41.714844 115.085938 42.519531 C 116.363281 43.636719 116.519531 44.445313 115.308594 45.617188 C 113.601563 47.265625 111.71875 48.738281 109.855469 50.21875 C 108.417969 51.363281 106.839844 52.335938 105.441406 53.519531 C 104.945313 53.9375 104.425781 54.878906 104.582031 55.367188 C 104.746094 55.882813 105.667969 56.347656 106.328125 56.472656 C 107.726563 56.738281 109.171875 56.746094 110.59375 56.898438 C 111.746094 57.019531 112.875 57.246094 113.65625 58.261719 C 113.316406 59.867188 112.089844 60.324219 110.84375 60.671875 C 106.925781 61.761719 103.664063 63.929688 100.789063 66.722656 C 100.011719 67.476563 99.292969 68.324219 98.71875 69.238281 C 98.359375 69.8125 98.320313 70.585938 98.054688 71.589844 C 99.25 71.757813 100.15625 71.972656 101.070313 71.984375 C 102.085938 71.996094 103.105469 71.808594 104.121094 71.730469 C 106.378906 71.558594 107.40625 72.660156 107.039063 74.925781 C 106.699219 77.035156 105.476563 78.53125 103.554688 79.417969 C 101.394531 80.421875 99.207031 81.371094 97.023438 82.328125 C 94.863281 83.277344 93 84.644531 91.445313 86.394531 C 90.898438 87.003906 90.585938 87.824219 89.996094 88.84375 C 91.121094 89.167969 91.863281 89.535156 92.621094 89.570313 C 94.933594 89.671875 97.261719 89.777344 99.570313 89.621094 C 100.894531 89.53125 102.171875 88.878906 103.492188 88.585938 C 104.605469 88.339844 105.78125 88.007813 106.878906 88.140625 C 109.386719 88.445313 110.34375 90.613281 108.796875 92.601563 C 107.773438 93.914063 106.480469 95.164063 105.046875 95.988281 C 100.738281 98.464844 96.242188 100.609375 91.925781 103.078125 C 86.300781 106.296875 80.847656 109.800781 76.007813 114.15625 C 75.050781 115.019531 74.140625 115.964844 73.378906 117 C 72.875 117.683594 72.503906 118.570313 72.382813 119.410156 C 72.113281 121.296875 73.550781 122.382813 75.335938 121.769531 C 75.589844 121.679688 75.839844 121.566406 76.085938 121.441406 C 78.421875 120.257813 80.894531 119.996094 83.449219 120.410156 C 87.699219 121.105469 91.734375 120.050781 95.679688 118.738281 C 96.964844 118.3125 97.996094 117.117188 99.148438 116.277344 C 99.890625 115.742188 100.640625 115.214844 101.441406 114.648438 C 103.152344 116.527344 103.140625 118.5 102.511719 120.402344 C 102.03125 121.863281 101.257813 123.292969 100.335938 124.53125 C 98.128906 127.492188 95.582031 130.164063 92.433594 132.164063 C 89.445313 134.066406 86.488281 136.046875 83.363281 137.710938 C 76.871094 141.175781 73.140625 146.550781 71.820313 153.683594 C 71.285156 156.5625 70.835938 159.460938 70.476563 162.367188 C 70.3125 163.683594 70.191406 165.113281 71.472656 166.101563 C 72.835938 166.082031 73.644531 165.164063 74.308594 164.265625 C 77.566406 159.871094 82.019531 158.234375 87.316406 158.269531 C 91.484375 158.292969 95.441406 157.53125 98.863281 154.878906 C 99.679688 154.242188 100.746094 153.921875 101.539063 153.535156 C 102.847656 154.238281 102.679688 155.144531 102.511719 156.019531 C 101.964844 158.886719 100.507813 161.269531 98.238281 163.023438 C 96.640625 164.261719 94.804688 165.222656 93.011719 166.183594 C 91.273438 167.113281 89.453125 167.894531 87.660156 168.730469 C 85.433594 169.769531 83.832031 171.546875 82.246094 173.335938 C 81.484375 174.195313 81.507813 174.40625 81.738281 175.683594 C 88.488281 177.3125 94.964844 177.140625 100.835938 172.867188 C 101.109375 172.667969 101.40625 172.488281 101.707031 172.335938 C 103.5 171.414063 104.902344 172.167969 104.839844 174.207031 C 104.796875 175.597656 104.5 177.070313 103.933594 178.332031 C 103.039063 180.332031 101.625 182.007813 99.816406 183.328125 C 95.242188 186.667969 90.523438 189.761719 85.480469 192.378906 C 80.933594 194.742188 77.011719 197.871094 74.8125 202.707031 C 74.425781 203.554688 73.894531 204.425781 74.683594 205.59375 C 75.308594 205.476563 76.097656 205.503906 76.707031 205.183594 C 79.675781 203.613281 82.757813 203.808594 85.886719 204.417969 C 87.359375 204.703125 88.816406 205.140625 90.300781 205.269531 C 92.03125 205.417969 93.78125 205.304688 95.757813 205.304688 "/>
<path style=" stroke:none;fill-rule:nonzero;fill:rgb(60.398865%,10.598755%,11.799622%);fill-opacity:1;" d="M 172.03125 241.433594 C 171.183594 237.375 169.734375 234.207031 166.972656 231.804688 C 165.648438 230.652344 164.160156 229.609375 162.59375 228.832031 C 159.050781 227.085938 155.414063 225.53125 151.816406 223.90625 C 150.824219 223.457031 149.777344 223.105469 148.84375 222.5625 C 145.734375 220.753906 144.632813 216.027344 146.679688 212.625 C 147.207031 212.910156 147.871094 213.089844 148.269531 213.511719 C 151.332031 216.757813 155.089844 218.152344 159.503906 217.875 C 162.183594 217.707031 164.597656 218.246094 166.746094 220.035156 C 167.566406 220.71875 168.789063 220.925781 170.265625 221.527344 C 170.3125 220.320313 170.503906 219.53125 170.34375 218.820313 C 168.496094 210.597656 164.527344 203.855469 156.734375 199.941406 C 151.722656 197.421875 147.054688 194.367188 142.5 191.121094 C 140.457031 189.664063 138.851563 187.851563 137.675781 185.652344 C 137.230469 184.816406 136.777344 183.953125 137.308594 182.714844 C 137.910156 182.761719 138.585938 182.699219 139.175781 182.890625 C 140.53125 183.328125 141.878906 183.828125 143.175781 184.421875 C 146.328125 185.863281 149.5625 186.253906 152.945313 185.445313 C 155.296875 184.886719 157.648438 184.601563 159.953125 185.679688 C 160.191406 185.789063 160.484375 185.8125 160.753906 185.828125 C 162.628906 185.945313 163.484375 184.753906 162.578125 183.125 C 162.089844 182.25 161.382813 181.464844 160.648438 180.769531 C 157.644531 177.925781 154.101563 175.910156 150.335938 174.242188 C 148.78125 173.550781 147.253906 172.789063 145.757813 171.988281 C 142.609375 170.296875 140.578125 167.625 139.273438 164.359375 C 138.863281 163.34375 138.472656 162.28125 139.445313 161.320313 C 140.425781 160.347656 141.582031 159.824219 142.964844 160.15625 C 143.820313 160.363281 144.636719 160.734375 145.46875 161.035156 C 150.398438 162.808594 155.332031 164.566406 160.25 166.371094 C 161.332031 166.765625 162.335938 167.367188 163.40625 167.800781 C 164.785156 168.363281 166.15625 169.03125 167.597656 169.328125 C 169.851563 169.792969 171.03125 168.664063 170.753906 166.382813 C 170.238281 162.179688 169.433594 158.035156 167.554688 154.179688 C 167.355469 153.765625 167.207031 153.324219 166.984375 152.78125 C 167.660156 152.746094 168.191406 152.699219 168.722656 152.699219 C 175.679688 152.695313 182.636719 152.722656 189.59375 152.671875 C 190.777344 152.664063 191.609375 153.015625 192.449219 153.859375 C 204.347656 165.78125 216.273438 177.679688 228.222656 189.554688 C 229.113281 190.433594 229.511719 191.308594 229.507813 192.582031 C 229.476563 230.914063 229.476563 269.246094 229.507813 307.574219 C 229.511719 308.851563 229.132813 309.738281 228.222656 310.605469 C 218.761719 319.648438 209.332031 328.726563 199.898438 337.800781 C 195.179688 342.335938 190.457031 346.863281 185.765625 351.425781 C 185.039063 352.128906 184.339844 352.5625 183.273438 352.539063 C 179.320313 352.457031 175.359375 352.589844 171.414063 352.402344 C 165.496094 352.117188 160.089844 350.117188 155.121094 346.949219 C 154.160156 346.339844 153.296875 345.511719 152.550781 344.640625 C 149.996094 341.652344 147.066406 339.128906 143.585938 337.296875 C 135.359375 332.960938 131.722656 325.789063 131.015625 316.871094 C 130.78125 313.886719 130.871094 310.875 130.867188 307.878906 C 130.847656 299.554688 130.851563 291.234375 130.867188 282.914063 C 130.871094 281.648438 130.582031 280.113281 132.054688 279.496094 C 133.34375 278.957031 134.363281 279.984375 135.308594 280.730469 C 138.101563 282.945313 141.253906 283.980469 144.816406 283.777344 C 146.585938 283.675781 148.371094 283.726563 150.121094 283.484375 C 153.332031 283.042969 156.441406 283.457031 159.304688 284.800781 C 162.6875 286.386719 165.941406 288.257813 169.171875 290.144531 C 171.757813 291.652344 174.191406 293.425781 176.761719 294.96875 C 177.855469 295.625 179.082031 296.132813 180.316406 296.464844 C 181.867188 296.878906 182.902344 296.148438 183.003906 294.554688 C 183.097656 293.140625 183.144531 291.628906 182.75 290.296875 C 181.433594 285.863281 180.027344 281.445313 178.417969 277.109375 C 177.421875 274.425781 175.296875 272.492188 172.996094 270.851563 C 171.503906 269.78125 169.882813 268.898438 168.34375 267.894531 C 167.550781 267.375 166.710938 266.863281 166.082031 266.179688 C 164.160156 264.085938 164.253906 262.160156 166.152344 260.03125 C 166.914063 259.175781 167.660156 258.253906 168.191406 257.242188 C 169.503906 254.75 168.828125 252.628906 166.1875 251.574219 C 164.3125 250.828125 162.277344 250.390625 160.273438 250.089844 C 155.894531 249.433594 151.964844 247.890625 148.859375 244.667969 C 147.835938 243.601563 146.992188 242.324219 146.234375 241.046875 C 145.417969 239.660156 145.804688 238.265625 147.15625 237.328125 C 148.867188 236.140625 150.722656 235.925781 152.546875 236.996094 C 157.476563 239.898438 162.835938 241.273438 168.496094 241.574219 C 169.546875 241.628906 170.605469 241.496094 172.03125 241.433594 "/>
<path style=" stroke:none;fill-rule:nonzero;fill:rgb(60.398865%,10.598755%,11.799622%);fill-opacity:1;" d="M 12.75 258.398438 C 34.238281 258.398438 55.605469 258.398438 76.972656 258.398438 C 77.019531 258.496094 77.0625 258.589844 77.109375 258.6875 C 76.363281 259.082031 75.628906 259.503906 74.863281 259.871094 C 71.144531 261.648438 67.679688 263.773438 65.003906 266.992188 C 62.476563 270.03125 58.757813 281.015625 58.914063 284.972656 C 58.933594 285.378906 58.933594 285.800781 59.046875 286.1875 C 59.351563 287.253906 59.738281 288.289063 60.941406 288.664063 C 62.191406 289.050781 62.996094 288.320313 63.78125 287.511719 C 69.570313 281.523438 76.71875 277.800781 84.578125 275.441406 C 86.878906 274.75 89.441406 274.96875 91.863281 274.640625 C 93.882813 274.367188 95.902344 274.007813 97.871094 273.5 C 100.289063 272.878906 102.339844 271.5625 104.097656 269.753906 C 105.183594 268.632813 106.375 267.59375 107.609375 266.632813 C 108.386719 266.027344 109.265625 265.410156 110.640625 265.789063 C 110.722656 266.390625 110.882813 267.03125 110.878906 267.667969 C 110.859375 280.21875 110.882813 292.765625 110.75 305.3125 C 110.707031 309.253906 110.398438 313.207031 109.335938 317.054688 C 108.589844 319.746094 107.410156 322.117188 105.199219 323.9375 C 103.621094 325.238281 102.167969 326.683594 100.652344 328.054688 C 97.441406 330.957031 94.738281 334.253906 92.527344 337.984375 C 90.582031 341.277344 88.347656 344.371094 85.453125 346.933594 C 81.402344 350.507813 76.671875 352.347656 71.28125 352.417969 C 67.121094 352.472656 62.960938 352.398438 58.796875 352.449219 C 57.757813 352.464844 57.03125 352.128906 56.285156 351.40625 C 47.375 342.777344 38.429688 334.175781 29.488281 325.574219 C 24.476563 320.757813 19.472656 315.925781 14.410156 311.164063 C 13.140625 309.972656 12.515625 308.765625 12.527344 306.933594 C 12.636719 291.386719 12.632813 275.835938 12.667969 260.285156 C 12.667969 259.6875 12.71875 259.09375 12.75 258.398438 "/>
</g>
</svg>

    </a>
    Bayesian Embedding (BEMB)
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../intro/" class="md-nav__link">
        About
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="https://gsbdbi.github.io/torch-choice/" class="md-nav__link">
        Torch Choice Documentation
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="https://gsbdbi.github.io/torch-choice/intro/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="https://gsbdbi.github.io/torch-choice/" class="md-nav__link">
        Documentation for Torch-Choice
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="https://gsbdbi.github.io/torch-choice/data_management/" class="md-nav__link">
        Tutorial for Data Management
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="https://gsbdbi.github.io/torch-choice/easy_data_management/" class="md-nav__link">
        Tutorial for Easy Data Management and Stata Users
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../bemb/" class="md-nav__link">
        Tutorial for Bayesian Embedding (BEMB)
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../bemb_obs2prior_simulation/" class="md-nav__link">
        Tutorial for BEMB obs2prior feature
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Tutorial for Inference with BEMB
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Tutorial for Inference with BEMB
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#generate-simulated-datasets" class="md-nav__link">
    Generate Simulated Datasets
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#construct-and-train-the-model" class="md-nav__link">
    Construct and Train the Model
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-forward-function" class="md-nav__link">
    The forward() Function
  </a>
  
    <nav class="md-nav" aria-label="The forward() Function">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#summary-table-for-forward-method" class="md-nav__link">
    Summary Table for forward() Method
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#predicting-item-index-pred_item-true" class="md-nav__link">
    Predicting Item Index (pred_item == True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#predicting-binary-labels-pred_item-false" class="md-nav__link">
    Predicting Binary Labels (pred_item == False)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-deterministic-option" class="md-nav__link">
    The deterministic Option
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#syntax-sugar-the-predict_proba-function" class="md-nav__link">
    Syntax Sugar: the predict_proba() Function
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../projects/" class="md-nav__link">
        Related Projects
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../test/" class="md-nav__link">
        Compatibility Tests
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../api_bemb/" class="md-nav__link">
        API Reference BEMB
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#generate-simulated-datasets" class="md-nav__link">
    Generate Simulated Datasets
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#construct-and-train-the-model" class="md-nav__link">
    Construct and Train the Model
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-forward-function" class="md-nav__link">
    The forward() Function
  </a>
  
    <nav class="md-nav" aria-label="The forward() Function">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#summary-table-for-forward-method" class="md-nav__link">
    Summary Table for forward() Method
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#predicting-item-index-pred_item-true" class="md-nav__link">
    Predicting Item Index (pred_item == True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#predicting-binary-labels-pred_item-false" class="md-nav__link">
    Predicting Binary Labels (pred_item == False)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-deterministic-option" class="md-nav__link">
    The deterministic Option
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#syntax-sugar-the-predict_proba-function" class="md-nav__link">
    Syntax Sugar: the predict_proba() Function
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="inference-using-the-bemb-model">Inference using the BEMB model</h1>
<p>This tutorial covers methods for post-estimation inference. After training the BEMB model, it is useful to have a more detailed look at predictions from the model. Functionalities covered in this tutorial allows you to make prediction on new datasets. Methods here (i.e., <code>forward()</code> and <code>predict_proba()</code>) are versatile and offering both predicted probabilities and predicted utilities.</p>
<p>Author: Tianyu Du</p>
<p>Date: Aug. 8, 2022</p>
<p>Update: Aug. 10, 2022</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">import</span> <span class="nn">sys</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Dict</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="kn">import</span> <span class="nn">torch</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="c1"># we use the dataset simulation method from unit tests.</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;../../tests&#39;</span><span class="p">)</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="kn">import</span> <span class="nn">simulate_choice_dataset</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="kn">import</span> <span class="nn">torch</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="kn">from</span> <span class="nn">bemb.model</span> <span class="kn">import</span> <span class="n">LitBEMBFlex</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a><span class="kn">from</span> <span class="nn">torch_choice.data</span> <span class="kn">import</span> <span class="n">ChoiceDataset</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>/Users/tianyudu/miniforge3/envs/ml/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f&quot;A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}&quot;
</code></pre></div>
<h2 id="generate-simulated-datasets">Generate Simulated Datasets</h2>
<p>We will use the simulated dataset in <a href="https://gsbdbi.github.io/bemb/bemb_obs2prior_simulation/">this tutorial</a> for demonstration purpose.</p>
<p>The simulated dataset is divided into train (80%), validation (10%), and test (10%) subsets automatically.</p>
<p>Moreover, the simulated dataset includes 50-dimensional user observables and item observables. <a href="https://gsbdbi.github.io/bemb/bemb_obs2prior_simulation/">This tutorial</a> mentioned definitions of these observables but you don't need to know them for the purpose of this tutorial.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="n">num_users</span> <span class="o">=</span> <span class="mi">1500</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="n">num_items</span> <span class="o">=</span> <span class="mi">50</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="n">data_size</span> <span class="o">=</span> <span class="mi">10000</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span class="c1"># split into three train, validation and test datasets.</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span class="n">dataset_list</span> <span class="o">=</span> <span class="n">simulate_choice_dataset</span><span class="o">.</span><span class="n">simulate_dataset</span><span class="p">(</span><span class="n">num_users</span><span class="o">=</span><span class="n">num_users</span><span class="p">,</span> <span class="n">num_items</span><span class="o">=</span><span class="n">num_items</span><span class="p">,</span> <span class="n">data_size</span><span class="o">=</span><span class="n">data_size</span><span class="p">)</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span class="n">dataset_list</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>No `session_index` is provided, assume each choice instance is in its own session.





[ChoiceDataset(label=[], item_index=[8000], user_index=[8000], session_index=[8000], item_availability=[], user_obs=[1500, 50], item_obs=[50, 50], device=cpu),
 ChoiceDataset(label=[], item_index=[1000], user_index=[1000], session_index=[1000], item_availability=[], user_obs=[1500, 50], item_obs=[50, 50], device=cpu),
 ChoiceDataset(label=[], item_index=[1000], user_index=[1000], session_index=[1000], item_availability=[], user_obs=[1500, 50], item_obs=[50, 50], device=cpu)]
</code></pre></div>
<h2 id="construct-and-train-the-model">Construct and Train the Model</h2>
<p>Here we will be using a rather simple model with two sets of parameters, an user latent <span class="arithmatex">\(\theta_u \in \mathbb{R}^{10}\)</span> for each of the 1,500 users, and an item latent <span class="arithmatex">\(\alpha_i \in \mathbb{R}^{10}\)</span> for each of the 50 items.</p>
<p><strong>Note</strong>: The behavior of <code>forward()</code> and <code>predict_proba()</code> methods depends on the model setup (i.e., whether <code>pred_item</code> or not).</p>
<p><strong>Note</strong>: The <code>LitBEMBFlex</code> object is a class wrapping the actual model with training loops, to access the core model encompassed, we use <code>bemb.model</code> (see the <code>return</code> line of <code>train_model()</code>).</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">pred_item</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>    <span class="n">bemb</span> <span class="o">=</span> <span class="n">LitBEMBFlex</span><span class="p">(</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span>  <span class="c1"># set the learning rate, feel free to play with different levels.</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>        <span class="n">pred_item</span><span class="o">=</span><span class="n">pred_item</span><span class="p">,</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>        <span class="n">num_seeds</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>  <span class="c1"># number of Monte Carlo samples for estimating the ELBO.</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>        <span class="n">utility_formula</span><span class="o">=</span><span class="s1">&#39;theta_user * alpha_item&#39;</span><span class="p">,</span>  <span class="c1"># the utility formula.</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>        <span class="c1"># tell the model some necessary information about the setup.</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>        <span class="n">num_users</span><span class="o">=</span><span class="n">num_users</span><span class="p">,</span>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>        <span class="n">num_items</span><span class="o">=</span><span class="n">num_items</span><span class="p">,</span>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>        <span class="n">num_user_obs</span><span class="o">=</span><span class="n">dataset_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">user_obs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>        <span class="n">num_item_obs</span><span class="o">=</span><span class="n">dataset_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item_obs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a>        <span class="c1"># whether to turn on obs2prior for each parameter.</span>
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a>        <span class="n">obs2prior_dict</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;theta_user&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;alpha_item&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a>        <span class="c1"># the dimension of latents, since the utility is an inner product of theta and alpha, they should have</span>
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a>        <span class="c1"># the same dimension.</span>
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#__codelineno-2-16"></a>        <span class="n">coef_dim_dict</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;theta_user&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;alpha_item&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">}</span>
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#__codelineno-2-17"></a>    <span class="p">)</span>
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#__codelineno-2-18"></a>
<a id="__codelineno-2-19" name="__codelineno-2-19" href="#__codelineno-2-19"></a>    <span class="c1"># use GPU if available.</span>
<a id="__codelineno-2-20" name="__codelineno-2-20" href="#__codelineno-2-20"></a>    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
<a id="__codelineno-2-21" name="__codelineno-2-21" href="#__codelineno-2-21"></a>        <span class="n">bemb</span> <span class="o">=</span> <span class="n">bemb</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<a id="__codelineno-2-22" name="__codelineno-2-22" href="#__codelineno-2-22"></a>
<a id="__codelineno-2-23" name="__codelineno-2-23" href="#__codelineno-2-23"></a>    <span class="c1"># use the provided run helper to train the model.</span>
<a id="__codelineno-2-24" name="__codelineno-2-24" href="#__codelineno-2-24"></a>    <span class="c1"># we set batch size to be 5% of the data size, and train the model for 10 epochs.</span>
<a id="__codelineno-2-25" name="__codelineno-2-25" href="#__codelineno-2-25"></a>    <span class="c1"># there would be 20*10=200 gradient update steps in total.</span>
<a id="__codelineno-2-26" name="__codelineno-2-26" href="#__codelineno-2-26"></a>    <span class="n">bemb</span> <span class="o">=</span> <span class="n">bemb</span><span class="o">.</span><span class="n">fit_model</span><span class="p">(</span><span class="n">dataset_list</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset_list</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">//</span> <span class="mi">20</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<a id="__codelineno-2-27" name="__codelineno-2-27" href="#__codelineno-2-27"></a>    <span class="c1"># The `LitBEMBFlex` object is a class wrapping the actual model with training loops, to access the core model encompassed, we use `bemb.model`.</span>
<a id="__codelineno-2-28" name="__codelineno-2-28" href="#__codelineno-2-28"></a>    <span class="k">return</span> <span class="n">bemb</span><span class="o">.</span><span class="n">model</span>
</code></pre></div>
<h2 id="the-forward-function">The <code>forward()</code> Function</h2>
<p>The <code>forward()</code> function is the main workhorse for inference, please see the doc-string of <code>forward()</code> function for definitions of its arguments.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">ChoiceDataset</span><span class="p">,</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>            <span class="n">return_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>            <span class="n">return_scope</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>            <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>            <span class="n">sample_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>            <span class="n">num_seeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>            <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a>    <span class="sd">&quot;&quot;&quot;A combined method for inference with the model.</span>
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a>
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a><span class="sd">    Args:</span>
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a><span class="sd">        batch (ChoiceDataset): batch data containing choice information.</span>
<a id="__codelineno-3-12" name="__codelineno-3-12" href="#__codelineno-3-12"></a><span class="sd">        return_type (str): either &#39;log_prob&#39; or &#39;utility&#39;.</span>
<a id="__codelineno-3-13" name="__codelineno-3-13" href="#__codelineno-3-13"></a><span class="sd">            &#39;log_prob&#39;: return the log-probability (by within-category log-softmax) for items</span>
<a id="__codelineno-3-14" name="__codelineno-3-14" href="#__codelineno-3-14"></a><span class="sd">            &#39;utility&#39;: return the utility value of items.</span>
<a id="__codelineno-3-15" name="__codelineno-3-15" href="#__codelineno-3-15"></a><span class="sd">        return_scope (str): either &#39;item_index&#39; or &#39;all_items&#39;.</span>
<a id="__codelineno-3-16" name="__codelineno-3-16" href="#__codelineno-3-16"></a><span class="sd">            &#39;item_index&#39;: for each observation i, return log-prob/utility for the chosen item batch.item_index[i] only.</span>
<a id="__codelineno-3-17" name="__codelineno-3-17" href="#__codelineno-3-17"></a><span class="sd">            &#39;all_items&#39;: for each observation i, return log-prob/utility for all items.</span>
<a id="__codelineno-3-18" name="__codelineno-3-18" href="#__codelineno-3-18"></a><span class="sd">        deterministic (bool, optional):</span>
<a id="__codelineno-3-19" name="__codelineno-3-19" href="#__codelineno-3-19"></a><span class="sd">            True: expectations of parameter variational distributions are used for inference.</span>
<a id="__codelineno-3-20" name="__codelineno-3-20" href="#__codelineno-3-20"></a><span class="sd">            False: the user needs to supply a dictionary of sampled parameters for inference.</span>
<a id="__codelineno-3-21" name="__codelineno-3-21" href="#__codelineno-3-21"></a><span class="sd">            Defaults to True.</span>
<a id="__codelineno-3-22" name="__codelineno-3-22" href="#__codelineno-3-22"></a><span class="sd">        sample_dict (Optional[Dict[str, torch.Tensor]], optional): sampled parameters for inference task.</span>
<a id="__codelineno-3-23" name="__codelineno-3-23" href="#__codelineno-3-23"></a><span class="sd">            This is not needed when `deterministic` is True.</span>
<a id="__codelineno-3-24" name="__codelineno-3-24" href="#__codelineno-3-24"></a><span class="sd">            When `deterministic` is False, the user can supply a `sample_dict`. If `sample_dict` is not provided,</span>
<a id="__codelineno-3-25" name="__codelineno-3-25" href="#__codelineno-3-25"></a><span class="sd">            this method will create `num_seeds` samples.</span>
<a id="__codelineno-3-26" name="__codelineno-3-26" href="#__codelineno-3-26"></a><span class="sd">            Defaults to None.</span>
<a id="__codelineno-3-27" name="__codelineno-3-27" href="#__codelineno-3-27"></a><span class="sd">        num_seeds (Optional[int]): the number of random samples of parameters to construct. This is only required</span>
<a id="__codelineno-3-28" name="__codelineno-3-28" href="#__codelineno-3-28"></a><span class="sd">            if `deterministic` is False (i.e., stochastic mode) and `sample_dict` is not provided.</span>
<a id="__codelineno-3-29" name="__codelineno-3-29" href="#__codelineno-3-29"></a><span class="sd">            Defaults to None.</span>
<a id="__codelineno-3-30" name="__codelineno-3-30" href="#__codelineno-3-30"></a><span class="sd">    Returns:</span>
<a id="__codelineno-3-31" name="__codelineno-3-31" href="#__codelineno-3-31"></a><span class="sd">        torch.Tensor: a tensor of log-probabilities or utilities, depending on `return_type`.</span>
<a id="__codelineno-3-32" name="__codelineno-3-32" href="#__codelineno-3-32"></a><span class="sd">            The shape of the returned tensor depends on `return_scope` and `deterministic`.</span>
<a id="__codelineno-3-33" name="__codelineno-3-33" href="#__codelineno-3-33"></a><span class="sd">            -------------------------------------------------------------------------</span>
<a id="__codelineno-3-34" name="__codelineno-3-34" href="#__codelineno-3-34"></a><span class="sd">            | `return_scope` | `deterministic` |         Output shape               |</span>
<a id="__codelineno-3-35" name="__codelineno-3-35" href="#__codelineno-3-35"></a><span class="sd">            -------------------------------------------------------------------------</span>
<a id="__codelineno-3-36" name="__codelineno-3-36" href="#__codelineno-3-36"></a><span class="sd">            |   &#39;item_index` |      True       | (len(batch),)                      |</span>
<a id="__codelineno-3-37" name="__codelineno-3-37" href="#__codelineno-3-37"></a><span class="sd">            -------------------------------------------------------------------------</span>
<a id="__codelineno-3-38" name="__codelineno-3-38" href="#__codelineno-3-38"></a><span class="sd">            |   &#39;all_items&#39;  |      True       | (len(batch), num_items)            |</span>
<a id="__codelineno-3-39" name="__codelineno-3-39" href="#__codelineno-3-39"></a><span class="sd">            -------------------------------------------------------------------------</span>
<a id="__codelineno-3-40" name="__codelineno-3-40" href="#__codelineno-3-40"></a><span class="sd">            |   &#39;item_index&#39; |      False      | (num_seeds, len(batch))            |</span>
<a id="__codelineno-3-41" name="__codelineno-3-41" href="#__codelineno-3-41"></a><span class="sd">            -------------------------------------------------------------------------</span>
<a id="__codelineno-3-42" name="__codelineno-3-42" href="#__codelineno-3-42"></a><span class="sd">            |   &#39;all_items&#39;  |      False      | (num_seeds, len(batch), num_items) |</span>
<a id="__codelineno-3-43" name="__codelineno-3-43" href="#__codelineno-3-43"></a><span class="sd">            -------------------------------------------------------------------------</span>
<a id="__codelineno-3-44" name="__codelineno-3-44" href="#__codelineno-3-44"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-3-45" name="__codelineno-3-45" href="#__codelineno-3-45"></a>    <span class="c1"># function body omitted.</span>
<a id="__codelineno-3-46" name="__codelineno-3-46" href="#__codelineno-3-46"></a>    <span class="k">return</span> <span class="kc">None</span>
</code></pre></div>
<p>With our simple model, for the <span class="arithmatex">\(k\)</span>-th purchasing record (observation) in the dataset, suppose <code>dataset.user_index[k] =</code> <span class="arithmatex">\(u(k)\)</span> and <code>dataset.user_index[k] =</code> <span class="arithmatex">\(i(k)\)</span>.
Suppose there are <span class="arithmatex">\(K\)</span> such observations in the dataset.</p>
<p>After training the model, the model now contain fitted values of <span class="arithmatex">\(\theta\)</span>'s and <span class="arithmatex">\(\alpha\)</span>'s, inference predictions will be based on these parameters. </p>
<p>Our simple model calculates <span class="arithmatex">\(<span class="arithmatex">\(U_{u(k) \ell} = \theta_{u(k)}^\top \alpha_\ell\)</span>\)</span> for every possible item <span class="arithmatex">\(\ell\)</span> including the chosen <span class="arithmatex">\(i(k)\)</span>. This is called user <span class="arithmatex">\(u(k)\)</span>'s <em>utility</em> from buying item <span class="arithmatex">\(\ell\)</span>.</p>
<p>As mentioned before, interpretations of the <code>forward()</code> function are slightly different depending on whether <code>pred_item == True</code>.</p>
<h3 id="summary-table-for-forward-method">Summary Table for <code>forward()</code> Method</h3>
<table>
<thead>
<tr>
<th align="center"><code>pred_item</code></th>
<th align="center"><code>return_scope</code></th>
<th align="center"><code>return_type</code></th>
<th align="center">Output Shape</th>
<th align="center">Output Tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><code>True</code></td>
<td align="center"><code>item_index</code></td>
<td align="center"><code>utility</code></td>
<td align="center"><code>(len(batch),)</code></td>
<td align="center"><span class="arithmatex">\(\left[\theta_{u(1)}^\top \alpha_{i(1)}, \theta_{u(2)}^\top \alpha_{i(2)}, \dots, \theta_{u(K)}^\top \alpha_{i(K)}\right]\)</span></td>
</tr>
<tr>
<td align="center"><code>True</code></td>
<td align="center"><code>item_index</code></td>
<td align="center"><code>log_prob</code></td>
<td align="center"><code>(len(batch),)</code></td>
<td align="center"><span class="arithmatex">\(\left[\log\left(\frac{\exp(\theta_{u(1)}^\top \alpha_{i(1)})}{\sum_{\ell \in \text{category of } i(1)} \exp(\theta_{u(1)}^\top \alpha_{\ell}) }\right), \log\left(\frac{\exp(\theta_{u(2)}^\top \alpha_{i(2)})}{\sum_{\ell \in \text{category of } i(2)} \exp(\theta_{u(2)}^\top \alpha_{\ell}) }\right), \dots, \log\left(\frac{\exp(\theta_{u(K)}^\top \alpha_{i(K)})}{\sum_{\ell \in \text{category of } i(K)} \exp(\theta_{u(K)}^\top \alpha_{\ell}) }\right)\right]\)</span></td>
</tr>
<tr>
<td align="center"><code>True</code></td>
<td align="center"><code>all_items</code></td>
<td align="center"><code>utility</code></td>
<td align="center"><code>(len(batch), num_items)</code></td>
<td align="center"><span class="arithmatex">\(\begin{bmatrix} \theta_{u(1)}^\top \alpha_{1}, \theta_{u(1)}^\top \alpha_{2}, \dots, \theta_{u(1)}^\top \alpha_{num\_items} \\ \theta_{u(2)}^\top \alpha_{1}, \theta_{u(2)}^\top \alpha_{2}, \dots, \theta_{u(2)}^\top \alpha_{num\_items} \\ \vdots \\ \theta_{u(K)}^\top \alpha_{1}, \theta_{u(K)}^\top \alpha_{2}, \dots, \theta_{u(K)}^\top \alpha_{num\_items} \end{bmatrix}\)</span></td>
</tr>
<tr>
<td align="center"><code>True</code></td>
<td align="center"><code>all_items</code></td>
<td align="center"><code>log_prob</code></td>
<td align="center"><code>(len(batch), num_items)</code></td>
<td align="center"><span class="arithmatex">\(\begin{bmatrix} \log\left(\frac{\exp(\theta_{u(1)}^\top \alpha_{1})}{\sum_{\ell \in \text{category of } 1} \exp(\theta_{u(1)}^\top \alpha_{\ell}) }\right), \log\left(\frac{\exp(\theta_{u(1)}^\top \alpha_{2})}{\sum_{\ell \in \text{category of } 2} \exp(\theta_{u(1)}^\top \alpha_{\ell}) }\right), \dots, \log\left(\frac{\exp(\theta_{u(1)}^\top \alpha_{num\_items})}{\sum_{\ell \in \text{category of } num\_items} \exp(\theta_{u(1)}^\top \alpha_{\ell}) }\right) \\ \log\left(\frac{\exp(\theta_{u(2)}^\top \alpha_{1})}{\sum_{\ell \in \text{category of } 1} \exp(\theta_{u(2)}^\top \alpha_{\ell}) }\right), \log\left(\frac{\exp(\theta_{u(2)}^\top \alpha_{2})}{\sum_{\ell \in \text{category of } 2} \exp(\theta_{u(2)}^\top \alpha_{\ell}) }\right), \dots, \log\left(\frac{\exp(\theta_{u(2)}^\top \alpha_{num\_items})}{\sum_{\ell \in \text{category of } num\_items} \exp(\theta_{u(2)}^\top \alpha_{\ell}) }\right) \\ \vdots \\ \log\left(\frac{\exp(\theta_{u(K)}^\top \alpha_1)}{\sum_{\ell \in \text{category of } 1} \exp(\theta_{u(K)}^\top \alpha_{\ell}) }\right), \log\left(\frac{\exp(\theta_{u(K)}^\top \alpha_2)}{\sum_{\ell \in \text{category of } 2} \exp(\theta_{u(K)}^\top \alpha_{\ell}) }\right), \dots, \log\left(\frac{\exp(\theta_{u(K)}^\top \alpha_{num\_items})}{\sum_{\ell \in \text{category of } num\_items} \exp(\theta_{u(K)}^\top \alpha_{\ell}) }\right) \end{bmatrix}\)</span></td>
</tr>
<tr>
<td align="center"><code>False</code></td>
<td align="center"><code>item_index</code></td>
<td align="center"><code>utility</code></td>
<td align="center"><code>(len(batch),)</code></td>
<td align="center"><span class="arithmatex">\(\left[\theta_{u(1)}^\top \alpha_{i(1)}, \theta_{u(2)}^\top \alpha_{i(2)}, \dots, \theta_{u(K)}^\top \alpha_{i(K)}\right]\)</span></td>
</tr>
<tr>
<td align="center"><code>False</code></td>
<td align="center"><code>item_index</code></td>
<td align="center"><code>log_prob</code></td>
<td align="center"><code>(len(batch),)</code></td>
<td align="center"><span class="arithmatex">\(\left[y_1 \log\left(\sigma\left(\theta_{u(1)}^\top \alpha_{i(1)}\right)\right) + (1-y_1) \log\left(1-\sigma\left(\theta_{u(1)}^\top \alpha_{i(1)}\right)\right), y_2 \log\left(\sigma\left(\theta_{u(2)}^\top \alpha_{i(2)}\right)\right) + (1-y_2) \log\left(1-\sigma\left(\theta_{u(2)}^\top \alpha_{i(2)}\right)\right), \dots, y_K \log\left(\sigma\left(\theta_{u(K)}^\top \alpha_{i(K)}\right)\right) + (1-y_K) \log\left(1-\sigma\left(\theta_{u(K)}^\top \alpha_{i(K)}\right)\right)\right]\)</span></td>
</tr>
<tr>
<td align="center"><code>False</code></td>
<td align="center"><code>all_items</code></td>
<td align="center"><code>utility</code></td>
<td align="center"><code>(len(batch), num_items)</code></td>
<td align="center"><span class="arithmatex">\(\begin{bmatrix} \theta_{u(1)}^\top \alpha_{1}, \theta_{u(1)}^\top \alpha_{2}, \dots, \theta_{u(1)}^\top \alpha_{num\_items} \\ \theta_{u(2)}^\top \alpha_{1}, \theta_{u(2)}^\top \alpha_{2}, \dots, \theta_{u(2)}^\top \alpha_{num\_items} \\ \vdots \\ \theta_{u(K)}^\top \alpha_{1}, \theta_{u(K)}^\top \alpha_{2}, \dots, \theta_{u(K)}^\top \alpha_{num\_items} \end{bmatrix}\)</span></td>
</tr>
<tr>
<td align="center"><code>False</code></td>
<td align="center"><code>all_items</code></td>
<td align="center"><code>log_prob</code></td>
<td align="center"><code>(len(batch), num_items)</code></td>
<td align="center"><span class="arithmatex">\(\begin{bmatrix} y_1 \log\left(\sigma\left(\theta_{u(1)}^\top \alpha_{1}\right)\right) + (1-y_1) \log\left(1-\sigma\left(\theta_{u(1)}^\top \alpha_{1}\right)\right), y_1 \log\left(\sigma\left(\theta_{u(1)}^\top \alpha_{2}\right)\right) + (1-y_1) \log\left(1-\sigma\left(\theta_{u(1)}^\top \alpha_{2}\right)\right) , \dots, y_1 \log\left(\sigma\left(\theta_{u(1)}^\top \alpha_{num\_items}\right)\right) + (1-y_1) \log\left(1-\sigma\left(\theta_{u(1)}^\top \alpha_{num\_items}\right)\right) \\ y_2 \log\left(\sigma\left(\theta_{u(2)}^\top \alpha_{1}\right)\right) + (1-y_2) \log\left(1-\sigma\left(\theta_{u(2)}^\top \alpha_{1}\right)\right), y_2 \log\left(\sigma\left(\theta_{u(2)}^\top \alpha_{2}\right)\right) + (1-y_2) \log\left(1-\sigma\left(\theta_{u(2)}^\top \alpha_{2}\right)\right) , \dots, y_2 \log\left(\sigma\left(\theta_{u(2)}^\top \alpha_{num\_items}\right)\right) + (1-y_2) \log\left(1-\sigma\left(\theta_{u(2)}^\top \alpha_{num\_items}\right)\right) \\ \vdots \\ y_K \log\left(\sigma\left(\theta_{u(K)}^\top \alpha_{1}\right)\right) + (1-y_K) \log\left(1-\sigma\left(\theta_{u(K)}^\top \alpha_{1}\right)\right), y_K \log\left(\sigma\left(\theta_{u(K)}^\top \alpha_{2}\right)\right) + (1-y_K) \log\left(1-\sigma\left(\theta_{u(K)}^\top \alpha_{2}\right)\right) , \dots, y_K \log\left(\sigma\left(\theta_{u(K)}^\top \alpha_{num\_items}\right)\right) + (1-y_K) \log\left(1-\sigma\left(\theta_{u(K)}^\top \alpha_{num\_items}\right)\right) \end{bmatrix}\)</span></td>
</tr>
</tbody>
</table>
<h3 id="predicting-item-index-pred_item-true">Predicting Item Index (<code>pred_item == True</code>)</h3>
<p>In this case, the model aims to predict <em>which item user <span class="arithmatex">\(u(k)\)</span> would purchase</em>.</p>
<p>Let's get a copy of the simple model.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="o">%%</span><span class="n">capture</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a><span class="c1"># use %%capture to hide cumbersome logs of training.</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="n">model</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">pred_item</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

  | Name  | Type     | Params
-----------------------------------
0 | model | BEMBFlex | 33.0 K
-----------------------------------
33.0 K    Trainable params
0         Non-trainable params
33.0 K    Total params
0.132     Total estimated model params size (MB)
</code></pre></div>
<p>Let's then make predicting using the test set (the last entry in <code>dataset_list</code>).</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="n">batch</span> <span class="o">=</span> <span class="n">dataset_list</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div>
<p>Recall that, for each observation <span class="arithmatex">\(k\)</span>, suppose <span class="arithmatex">\(u(k)\)</span> is the corresponding user and item <span class="arithmatex">\(i(k)\)</span> was chosen. With learned parameters <span class="arithmatex">\(\theta_u\)</span> and <span class="arithmatex">\(\alpha_i\)</span>, the model first calculates <span class="arithmatex">\(<span class="arithmatex">\(U_{u(k) \ell} = \theta_{u(k)}^\top \alpha_\ell\)</span>\)</span> for every possible item <span class="arithmatex">\(\ell\)</span> including the chosen <span class="arithmatex">\(i(k)\)</span>. This is called user <span class="arithmatex">\(u(k)\)</span>'s <em>utility</em> from buying item <span class="arithmatex">\(\ell\)</span>.</p>
<p>Then, the predicted probability for user <span class="arithmatex">\(u(k)\)</span> to purchase item <span class="arithmatex">\(i\)</span> is:
<span class="arithmatex">\(<span class="arithmatex">\(\frac{\exp(\theta_{u(1)}^\top \alpha_i)}{\sum_{\ell \in \text{category of } i} \exp(\theta_{u(1)}^\top \alpha_{\ell}) }\)</span>\)</span></p>
<p>The denominator was normalizing using all items belonging to the category of item <span class="arithmatex">\(i\)</span>, however, in this example, we don't consider items' categories (i.e., assuming all of them belong to the same category). The predicted probability becomes:
<span class="arithmatex">\(<span class="arithmatex">\(\frac{\exp(\theta_{u(1)}^\top \alpha_i)}{\sum_{\ell=1}^{num\_items} \exp(\theta_{u(1)}^\top \alpha_{\ell}) }\)</span>\)</span></p>
<p>The <code>forward()</code> function allows you to get predicted probabilities (actually, log-probability for numerical stability) for all items. You would need to specify 
* <code>return_scope='all_items'</code>
* <code>return_type='log_prob'</code></p>
<p>As we expected, the shape of <code>log_prob_all_items</code> is <span class="arithmatex">\(K\)</span> by num-items, so that <code>log_prob_all_items[k, i]</code> denotes the predicted log-probability for user <span class="arithmatex">\(u(k)\)</span> to choose item <span class="arithmatex">\(i\)</span> in the context of the <span class="arithmatex">\(k\)</span>-th observation.</p>
<p>Formally, the returned tensor is:</p>
<div class="arithmatex">\[\begin{bmatrix} \log\left(\frac{\exp(\theta_{u(1)}^\top \alpha_{1})}{\sum_{\ell \in \text{category of } 1} \exp(\theta_{u(1)}^\top \alpha_{\ell}) }\right), \log\left(\frac{\exp(\theta_{u(1)}^\top \alpha_{2})}{\sum_{\ell \in \text{category of } 2} \exp(\theta_{u(1)}^\top \alpha_{\ell}) }\right), \dots, \log\left(\frac{\exp(\theta_{u(1)}^\top \alpha_{num\_items})}{\sum_{\ell \in \text{category of } num\_items} \exp(\theta_{u(1)}^\top \alpha_{\ell}) }\right) \\ \log\left(\frac{\exp(\theta_{u(2)}^\top \alpha_{1})}{\sum_{\ell \in \text{category of } 1} \exp(\theta_{u(2)}^\top \alpha_{\ell}) }\right), \log\left(\frac{\exp(\theta_{u(2)}^\top \alpha_{2})}{\sum_{\ell \in \text{category of } 2} \exp(\theta_{u(2)}^\top \alpha_{\ell}) }\right), \dots, \log\left(\frac{\exp(\theta_{u(2)}^\top \alpha_{num\_items})}{\sum_{\ell \in \text{category of } num\_items} \exp(\theta_{u(2)}^\top \alpha_{\ell}) }\right) \\ \vdots \\ \log\left(\frac{\exp(\theta_{u(K)}^\top \alpha_1)}{\sum_{\ell \in \text{category of } 1} \exp(\theta_{u(K)}^\top \alpha_{\ell}) }\right), \log\left(\frac{\exp(\theta_{u(K)}^\top \alpha_2)}{\sum_{\ell \in \text{category of } 2} \exp(\theta_{u(K)}^\top \alpha_{\ell}) }\right), \dots, \log\left(\frac{\exp(\theta_{u(K)}^\top \alpha_{num\_items})}{\sum_{\ell \in \text{category of } num\_items} \exp(\theta_{u(K)}^\top \alpha_{\ell}) }\right) \end{bmatrix}\]</div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="n">log_prob_all_items</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;all_items&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;log_prob&#39;</span><span class="p">)</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">log_prob_all_items</span><span class="o">.</span><span class="n">shape</span><span class="si">=:}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>log_prob_all_items.shape=torch.Size([1000, 50])
</code></pre></div>
<p>The predicted probabilities in each row of <code>log_prob_all_items</code> should (approximately) sum to one.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="nb">print</span><span class="p">(</span><span class="n">log_prob_all_items</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">log_prob_all_items</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">=:}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">log_prob_all_items</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">=:}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>tensor([[0.0419, 0.0089, 0.0380,  ..., 0.0266, 0.0098, 0.0428],
        [0.0104, 0.0393, 0.0197,  ..., 0.0196, 0.0078, 0.0346],
        [0.0284, 0.0262, 0.0207,  ..., 0.0041, 0.0049, 0.0062],
        ...,
        [0.0043, 0.0255, 0.0065,  ..., 0.0147, 0.0056, 0.0263],
        [0.0101, 0.0215, 0.0091,  ..., 0.0168, 0.0462, 0.0052],
        [0.0191, 0.0283, 0.0014,  ..., 0.0049, 0.0254, 0.0192]],
       grad_fn=&lt;ExpBackward0&gt;)
log_prob_all_items.exp().sum(dim=1).max()=1.0000003576278687
log_prob_all_items.exp().sum(dim=1).min()=0.9999995827674866
</code></pre></div>
<p>Sometimes we want raw values of utilities of each user <span class="arithmatex">\(u(k)\)</span> from purchasing each item <span class="arithmatex">\(i\)</span>. To achieve this, we specify:
* <code>return_type='utility'</code></p>
<p>In this case, the returned tensor is:</p>
<div class="arithmatex">\[\begin{bmatrix} \theta_{u(1)}^\top \alpha_{1}, \theta_{u(1)}^\top \alpha_{2}, \dots, \theta_{u(1)}^\top \alpha_{num\_items} \\ \theta_{u(2)}^\top \alpha_{1}, \theta_{u(2)}^\top \alpha_{2}, \dots, \theta_{u(2)}^\top \alpha_{num\_items} \\ \vdots \\ \theta_{u(K)}^\top \alpha_{1}, \theta_{u(K)}^\top \alpha_{2}, \dots, \theta_{u(K)}^\top \alpha_{num\_items} \end{bmatrix}\]</div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="n">utility_all_items</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;all_items&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;utility&#39;</span><span class="p">)</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">utility_all_items</span><span class="o">.</span><span class="n">shape</span><span class="si">=:}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>utility_all_items.shape=torch.Size([1000, 50])
</code></pre></div>
<p>Note that there is no guarantee on the row-sum of this tensor.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="nb">print</span><span class="p">(</span><span class="n">utility_all_items</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>tensor([[ 0.7441, -0.8005,  0.6468,  ...,  0.2911, -0.7060,  0.7649],
        [-0.6786,  0.6514, -0.0381,  ..., -0.0436, -0.9664,  0.5246],
        [ 0.5187,  0.4397,  0.2012,  ..., -1.4056, -1.2460, -1.0009],
        ...,
        [-0.9111,  0.8633, -0.4985,  ...,  0.3130, -0.6555,  0.8928],
        [-0.2202,  0.5378, -0.3241,  ...,  0.2927,  1.3049, -0.8828],
        [ 0.4644,  0.8605, -2.1168,  ..., -0.8846,  0.7492,  0.4722]],
       grad_fn=&lt;SqueezeBackward1&gt;)
</code></pre></div>
<p>In some other cases, such as while computing the log-likelihood to assess the goodness-of-fit for the entire model, we only care about user <span class="arithmatex">\(u(k)\)</span>'s utility/log-probability for item <span class="arithmatex">\(i(k)\)</span> that she/he actually bought.</p>
<p>Specifying 
* <code>return_scope = 'item_index</code>
calculates these values much faster on large datasets and/or we have many categories of items compared to the <code>log_prob_all_items[torch.arange(len(batch)), batch.item_index]</code> operation.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="n">log_prob_item_index</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;item_index&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;log_prob&#39;</span><span class="p">)</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">log_prob_item_index</span><span class="o">.</span><span class="n">shape</span><span class="si">=:}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>log_prob_item_index.shape=torch.Size([1000])
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">log_prob_item_index</span> <span class="o">==</span> <span class="n">log_prob_all_items</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)),</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_index</span><span class="p">])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>tensor(True)
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="n">utility_item_index</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;item_index&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;utility&#39;</span><span class="p">)</span>
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">utility_item_index</span><span class="o">.</span><span class="n">shape</span><span class="si">=:}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">utility_item_index</span> <span class="o">==</span> <span class="n">utility_all_items</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)),</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_index</span><span class="p">])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>utility_item_index.shape=torch.Size([1000])





tensor(True)
</code></pre></div>
<h3 id="predicting-binary-labels-pred_item-false">Predicting Binary Labels (<code>pred_item == False</code>)</h3>
<p>We have a label <span class="arithmatex">\(y_k \in \{0, 1\}\)</span> for each observation <span class="arithmatex">\(k\)</span>, the model can either output the (1) raw utility or (2) the predicted <span class="arithmatex">\(\hat{P}(y_k = 1)\)</span> defined as <span class="arithmatex">\(\sigma(U)\)</span>, where <span class="arithmatex">\(\sigma(x) = \frac{1}{1 + \exp(-x)}\)</span>.</p>
<p><strong>Notes</strong>
1. the return shape does not depend on the <code>return_type</code>, please compare exact expressions to see the difference.
2. the <code>pred_item</code> variable was specified while initializing the model (see above), <code>return_scope</code> and <code>return_type</code> are supplied while calling <code>forward()</code>.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="k">for</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="n">dataset_list</span><span class="p">:</span>
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a>    <span class="c1"># assign some trivial labels.</span>
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a>    <span class="n">dataset</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">user_index</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">num_users</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="o">%%</span><span class="n">capture</span>
<a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a><span class="c1"># use %%capture to hide cumbersome logs of training.</span>
<a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a><span class="n">model</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">pred_item</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

  | Name  | Type     | Params
-----------------------------------
0 | model | BEMBFlex | 33.0 K
-----------------------------------
33.0 K    Trainable params
0         Non-trainable params
33.0 K    Total params
0.132     Total estimated model params size (MB)
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a><span class="n">batch</span> <span class="o">=</span> <span class="n">dataset_list</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div>
<p>With <code>return_scope='item_index', return_type='utility'</code>, the <code>forward()</code> method returns
<span class="arithmatex">\(<span class="arithmatex">\(\left[\theta_{u(1)}^\top \alpha_{i(1)}, \theta_{u(2)}^\top \alpha_{i(2)}, \dots, \theta_{u(K)}^\top \alpha_{i(K)}\right].\)</span>\)</span>
This is indeed the same as the case wth <code>pred_item = True</code> above.</p>
<p>Therefore, <code>utility_item_index[k]</code> below is the utility of user <span class="arithmatex">\(u(k)\)</span> from purchasing the item <span class="arithmatex">\(i(k)\)</span> in the <span class="arithmatex">\(k\)</span>-th observation.  </p>
<p>With <code>pred_item = False</code>, each observation <span class="arithmatex">\(k\)</span> is now associated with a label <span class="arithmatex">\(y_k \in \{0, 1\}\)</span>. With <code>return_type='log_prob'</code>, the <code>forward()</code> function returns the predicted probability of the label <span class="arithmatex">\(y_k\)</span>.</p>
<p>Specifically, the model will predict the probability of the positive class (i.e., <span class="arithmatex">\(y_k = 1\)</span>), condition on latent of user <span class="arithmatex">\(u(k)\)</span> and item <span class="arithmatex">\(i(k)\)</span> as
$$
P(y_k=1 | \theta_{u(k)}, \alpha_{i(k)}) = \sigma(\theta_{u(k)}^\top \alpha_{i(k)}) = \frac{1}{1 + \exp\left(-\theta_{u(K)}^\top \alpha_{i(K)}\right)} \in [0, 1]
$$
Therefore, the <span class="arithmatex">\(k\)</span>-th entry of the returned tensor (i.e., <code>log_prob_item_index[k]</code> below) is <span class="arithmatex">\(\log \sigma(\theta_{u(k)}^\top \alpha_{i(k)})\)</span> if <span class="arithmatex">\(y_k = 1\)</span> and <span class="arithmatex">\(\log \left(1 - \sigma(\theta_{u(k)}^\top \alpha_{i(k)})\right)\)</span> if <span class="arithmatex">\(y_k = 0\)</span>.
This can be equivalently written as
<span class="arithmatex">\(<span class="arithmatex">\(\left[y_1 \log\left(\sigma\left(\theta_{u(1)}^\top \alpha_{i(1)}\right)\right) + (1-y_1) \log\left(1-\sigma\left(\theta_{u(1)}^\top \alpha_{i(1)}\right)\right), y_2 \log\left(\sigma\left(\theta_{u(2)}^\top \alpha_{i(2)}\right)\right) + (1-y_2) \log\left(1-\sigma\left(\theta_{u(2)}^\top \alpha_{i(2)}\right)\right), \dots, y_K \log\left(\sigma\left(\theta_{u(K)}^\top \alpha_{i(K)}\right)\right) + (1-y_K) \log\left(1-\sigma\left(\theta_{u(K)}^\top \alpha_{i(K)}\right)\right)\right]\)</span>\)</span></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="n">utility_item_index</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;item_index&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;utility&#39;</span><span class="p">)</span>
<a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">utility_item_index</span><span class="o">.</span><span class="n">shape</span><span class="si">=:}</span><span class="s1">&#39;</span><span class="p">)</span>
<a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a><span class="n">log_prob_item_index</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;item_index&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;log_prob&#39;</span><span class="p">)</span>
<a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">log_prob_item_index</span><span class="o">.</span><span class="n">shape</span><span class="si">=:}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>utility_item_index.shape=torch.Size([1000])
log_prob_item_index.shape=torch.Size([1000])
</code></pre></div>
<p>But, what if I want <span class="arithmatex">\(P(y_k=1 | \theta_{u(k)}, \alpha_{i(k)})\)</span> for every observation <span class="arithmatex">\(k\)</span>? The solution is straightforward, we simply take the <span class="arithmatex">\(\sigma(\cdot)\)</span> transformation of utilities returned by <code>utility_all_items = model.forward(batch, return_scope='item_index', return_type='utility')</code>.</p>
<p>Please note that when <code>return_type = 'utility'</code>, the dataset (<code>batch</code>) doesn't need to have a <code>label</code> attribute! For example, you might have <code>label</code> on your training dataset, but you want to conduct inference on a new dataset without known labels. You can simply create a <code>ChoiceDataset</code> object without the <code>label</code> attribute and use the following method to draw inference on it (e.g., get predicted probabilities of positive classes).</p>
<p>Here is an example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a><span class="c1"># make a copy of the batch.</span>
<a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a><span class="n">batch_copy</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a><span class="c1"># manually delete the label attribute, with return_type = &#39;utility&#39;, we don&#39;t need this.</span>
<a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a><span class="k">del</span> <span class="n">batch_copy</span><span class="o">.</span><span class="n">label</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a><span class="n">A</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch_copy</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;item_index&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;utility&#39;</span><span class="p">)</span>
<a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a><span class="n">prob_positive_class</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">A</span><span class="p">))</span>
</code></pre></div>
<p>Recall that <code>log_prob_item_index = model.forward(batch, return_scope='item_index', return_type='log_prob')</code> reports the predicted log-probability of the actual label <span class="arithmatex">\(y_k\)</span>, namely <span class="arithmatex">\(\log P(y_k | \theta_{u(k)}, \alpha_{i(k)})\)</span>.
Therefore, the relationship between <code>log_prob_item_index</code> we computed before and the <code>prob_positive_class</code> tensor we just computed is
$$
\texttt{log_prob_item_index} = \log P(y_k | \theta_{u(k)}, \alpha_{i(k)}) = \begin{cases}
\log P(y_k=1 | \theta_{u(k)}, \alpha_{i(k)}) &amp;\text{ if } y_k = 1 \
\log \left(1 - P(y_k=1 | \theta_{u(k)}, \alpha_{i(k)})\right) &amp;\text{ if } y_k = 0 \
\end{cases} 
= \begin{cases}
\log \texttt{prob_positive_class} &amp;\text{ if } y_k = 1 \
\log \left(1 - \texttt{prob_positive_class}\right) &amp;\text{ if } y_k = 0 \
\end{cases}
$$</p>
<p>Let confirm the relationship now.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a><span class="n">y</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">label</span>
<a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a><span class="n">log_prob_item_index_from_alternative_method</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob_positive_class</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">prob_positive_class</span><span class="p">)</span>
<a id="__codelineno-19-3" name="__codelineno-19-3" href="#__codelineno-19-3"></a><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">log_prob_item_index</span> <span class="o">==</span> <span class="n">log_prob_item_index_from_alternative_method</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>tensor(True)
</code></pre></div>
<p>Since different items have different latent <span class="arithmatex">\(\alpha_\ell\)</span>, the predicted probability of <span class="arithmatex">\(y_k = 1\)</span> depends on the item chosen.
By setting <code>return_scope='all_items'</code>, the <code>forward()</code> method returns <span class="arithmatex">\(\theta_{u(k)}^\top \alpha_{\ell}\)</span> and <span class="arithmatex">\(\log P(y_k | \theta_{u(k)}, \alpha_{\ell})\)</span> for all items <span class="arithmatex">\(\ell \in \{1, 2, \dots, num\_items\}\)</span>.</p>
<p>Formally, the <code>utility_all_items</code> tensors contains:
$$
\begin{bmatrix} \theta_{u(1)}^\top \alpha_{1}, \theta_{u(1)}^\top \alpha_{2}, \dots, \theta_{u(1)}^\top \alpha_{num_items} \ \theta_{u(2)}^\top \alpha_{1}, \theta_{u(2)}^\top \alpha_{2}, \dots, \theta_{u(2)}^\top \alpha_{num_items} \ \vdots \ \theta_{u(K)}^\top \alpha_{1}, \theta_{u(K)}^\top \alpha_{2}, \dots, \theta_{u(K)}^\top \alpha_{num_items} \end{bmatrix}
$$</p>
<p>and the <code>log_prob_all_items</code> tensors contains:
<span class="arithmatex">\(<span class="arithmatex">\(\begin{bmatrix} y_1 \log\left(\sigma\left(\theta_{u(1)}^\top \alpha_{1}\right)\right) + (1-y_1) \log\left(1-\sigma\left(\theta_{u(1)}^\top \alpha_{1}\right)\right), y_1 \log\left(\sigma\left(\theta_{u(1)}^\top \alpha_{2}\right)\right) + (1-y_1) \log\left(1-\sigma\left(\theta_{u(1)}^\top \alpha_{2}\right)\right) , \dots, y_1 \log\left(\sigma\left(\theta_{u(1)}^\top \alpha_{num\_items}\right)\right) + (1-y_1) \log\left(1-\sigma\left(\theta_{u(1)}^\top \alpha_{num\_items}\right)\right) \\ y_2 \log\left(\sigma\left(\theta_{u(2)}^\top \alpha_{1}\right)\right) + (1-y_2) \log\left(1-\sigma\left(\theta_{u(2)}^\top \alpha_{1}\right)\right), y_2 \log\left(\sigma\left(\theta_{u(2)}^\top \alpha_{2}\right)\right) + (1-y_2) \log\left(1-\sigma\left(\theta_{u(2)}^\top \alpha_{2}\right)\right) , \dots, y_2 \log\left(\sigma\left(\theta_{u(2)}^\top \alpha_{num\_items}\right)\right) + (1-y_2) \log\left(1-\sigma\left(\theta_{u(2)}^\top \alpha_{num\_items}\right)\right) \\ \vdots \\ y_K \log\left(\sigma\left(\theta_{u(K)}^\top \alpha_{1}\right)\right) + (1-y_K) \log\left(1-\sigma\left(\theta_{u(K)}^\top \alpha_{1}\right)\right), y_K \log\left(\sigma\left(\theta_{u(K)}^\top \alpha_{2}\right)\right) + (1-y_K) \log\left(1-\sigma\left(\theta_{u(K)}^\top \alpha_{2}\right)\right) , \dots, y_K \log\left(\sigma\left(\theta_{u(K)}^\top \alpha_{num\_items}\right)\right) + (1-y_K) \log\left(1-\sigma\left(\theta_{u(K)}^\top \alpha_{num\_items}\right)\right) \end{bmatrix}\)</span>\)</span></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">pred_item</span><span class="si">=:}</span><span class="s1">&#39;</span><span class="p">)</span>
<a id="__codelineno-20-2" name="__codelineno-20-2" href="#__codelineno-20-2"></a><span class="n">utility_all_items</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;all_items&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;utility&#39;</span><span class="p">)</span>
<a id="__codelineno-20-3" name="__codelineno-20-3" href="#__codelineno-20-3"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">utility_all_items</span><span class="o">.</span><span class="n">shape</span><span class="si">=:}</span><span class="s1">&#39;</span><span class="p">)</span>
<a id="__codelineno-20-4" name="__codelineno-20-4" href="#__codelineno-20-4"></a><span class="n">log_prob_all_items</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;all_items&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;log_prob&#39;</span><span class="p">)</span>
<a id="__codelineno-20-5" name="__codelineno-20-5" href="#__codelineno-20-5"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">log_prob_all_items</span><span class="o">.</span><span class="n">shape</span><span class="si">=:}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>model.pred_item=False
utility_all_items.shape=torch.Size([1000, 50])
Using the new version...
log_prob_all_items.shape=torch.Size([1000, 50])
</code></pre></div>
<p>Let check these tensors are consistent with the <code>return_scope='item_index'</code> case:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">utility_all_items</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)),</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_index</span><span class="p">]</span> <span class="o">==</span> <span class="n">utility_item_index</span><span class="p">))</span>
<a id="__codelineno-21-2" name="__codelineno-21-2" href="#__codelineno-21-2"></a><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">log_prob_all_items</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)),</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_index</span><span class="p">]</span> <span class="o">==</span> <span class="n">log_prob_item_index</span><span class="p">))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>tensor(True)
tensor(True)
</code></pre></div>
<p>If you want the predicted probability for the positive class <span class="arithmatex">\(y_k = 1\)</span>, then you can simply apply sigmoid function <span class="arithmatex">\(\sigma()\)</span> to the <code>utility_all_items</code> tensor.</p>
<h3 id="the-deterministic-option">The <code>deterministic</code> Option</h3>
<p>By default, the <code>forward()</code> function has keyword argument <code>deterministic = True</code>. In this case, the model uses the means of fitted variational distributions of <span class="arithmatex">\(\theta\)</span> and <span class="arithmatex">\(\alpha\)</span> to compute utilities and log-probabilities.</p>
<p>One can specify <code>forward(deterministic=False, num_seeds=&lt;XXX&gt;)</code>, the model will firstly sample <code>num_seeds</code> copies of <span class="arithmatex">\(\theta\)</span> and <span class="arithmatex">\(\alpha\)</span> from their variational distributions. For each copy, the model calculated utility/log-probability as described above.</p>
<p>Therefore, with the same <code>pred_item</code>, <code>return_scope</code>, and <code>return_type</code>, the returned tensor has shape <code>(num_seeds, &lt;the shape described in the table above&gt;)</code>.</p>
<p>For example, for a model initialized with <code>pred_item=False</code>,</p>
<ul>
<li><code>forward(batch, return_scope='all_items', return_type='utility', deterministic=True)</code> returns shape <code>(len(batch), num_items)</code> as mentioned above.</li>
<li>However, <code>forward(batch, return_scope='all_items', return_type='utility', deterministic=False, num_seeds=32)</code> returns shape <code>(32, len(batch), num_items)</code>.</li>
</ul>
<p>Here is an actual example (with <code>pred_item = False</code>):</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a><span class="n">deterministic</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;item_index&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;utility&#39;</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-22-2" name="__codelineno-22-2" href="#__codelineno-22-2"></a><span class="n">random</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;item_index&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;utility&#39;</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_seeds</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<a id="__codelineno-22-3" name="__codelineno-22-3" href="#__codelineno-22-3"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">deterministic</span><span class="o">.</span><span class="n">shape</span><span class="si">=:}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-22-4" name="__codelineno-22-4" href="#__codelineno-22-4"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">random</span><span class="o">.</span><span class="n">shape</span><span class="si">=:}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-22-5" name="__codelineno-22-5" href="#__codelineno-22-5"></a><span class="c1"># the mean absolute difference between deterministic estimation and random estimation.</span>
<a id="__codelineno-22-6" name="__codelineno-22-6" href="#__codelineno-22-6"></a><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">deterministic</span> <span class="o">-</span> <span class="n">random</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>deterministic.shape=torch.Size([1000])
random.shape=torch.Size([128, 1000])





tensor(0.0433, grad_fn=&lt;MeanBackward0&gt;)
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a><span class="n">deterministic</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;all_items&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;utility&#39;</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-23-2" name="__codelineno-23-2" href="#__codelineno-23-2"></a><span class="n">random</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;all_items&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;utility&#39;</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_seeds</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<a id="__codelineno-23-3" name="__codelineno-23-3" href="#__codelineno-23-3"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">deterministic</span><span class="o">.</span><span class="n">shape</span><span class="si">=:}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-23-4" name="__codelineno-23-4" href="#__codelineno-23-4"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">random</span><span class="o">.</span><span class="n">shape</span><span class="si">=:}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-23-5" name="__codelineno-23-5" href="#__codelineno-23-5"></a><span class="c1"># the mean absolute difference between deterministic estimation and random estimation.</span>
<a id="__codelineno-23-6" name="__codelineno-23-6" href="#__codelineno-23-6"></a><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">deterministic</span> <span class="o">-</span> <span class="n">random</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>deterministic.shape=torch.Size([1000, 50])
random.shape=torch.Size([128, 1000, 50])





tensor(0.0443, grad_fn=&lt;MeanBackward0&gt;)
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-24-1" name="__codelineno-24-1" href="#__codelineno-24-1"></a><span class="n">deterministic</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;all_items&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;log_prob&#39;</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-24-2" name="__codelineno-24-2" href="#__codelineno-24-2"></a><span class="n">random</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;all_items&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;log_prob&#39;</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_seeds</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<a id="__codelineno-24-3" name="__codelineno-24-3" href="#__codelineno-24-3"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">deterministic</span><span class="o">.</span><span class="n">shape</span><span class="si">=:}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-24-4" name="__codelineno-24-4" href="#__codelineno-24-4"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">random</span><span class="o">.</span><span class="n">shape</span><span class="si">=:}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-24-5" name="__codelineno-24-5" href="#__codelineno-24-5"></a><span class="c1"># the mean absolute difference between deterministic estimation and random estimation.</span>
<a id="__codelineno-24-6" name="__codelineno-24-6" href="#__codelineno-24-6"></a><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">deterministic</span> <span class="o">-</span> <span class="n">random</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Using the new version...
Using the new version...
deterministic.shape=torch.Size([1000, 50])
random.shape=torch.Size([128, 1000, 50])





tensor(0.0483, grad_fn=&lt;MeanBackward0&gt;)
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-25-1" name="__codelineno-25-1" href="#__codelineno-25-1"></a><span class="n">deterministic</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;item_index&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;log_prob&#39;</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-25-2" name="__codelineno-25-2" href="#__codelineno-25-2"></a><span class="n">random</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;item_index&#39;</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;log_prob&#39;</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_seeds</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<a id="__codelineno-25-3" name="__codelineno-25-3" href="#__codelineno-25-3"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">deterministic</span><span class="o">.</span><span class="n">shape</span><span class="si">=:}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-25-4" name="__codelineno-25-4" href="#__codelineno-25-4"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">random</span><span class="o">.</span><span class="n">shape</span><span class="si">=:}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-25-5" name="__codelineno-25-5" href="#__codelineno-25-5"></a><span class="c1"># the mean absolute difference between deterministic estimation and random estimation.</span>
<a id="__codelineno-25-6" name="__codelineno-25-6" href="#__codelineno-25-6"></a><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">deterministic</span> <span class="o">-</span> <span class="n">random</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>deterministic.shape=torch.Size([1000])
random.shape=torch.Size([128, 1000])





tensor(0.0482, grad_fn=&lt;MeanBackward0&gt;)
</code></pre></div>
<h2 id="syntax-sugar-the-predict_proba-function">Syntax Sugar: the <code>predict_proba()</code> Function</h2>
<p>The <code>predict_proba()</code> Function mimics the method with the same name in scikit-learn library.</p>
<p><strong>Note</strong>: to avoid over-flow or under-flow issues, please use the <code>forward()</code> function, which provides log-probabilities whenever possible.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-26-1" name="__codelineno-26-1" href="#__codelineno-26-1"></a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-26-2" name="__codelineno-26-2" href="#__codelineno-26-2"></a><span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">ChoiceDataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-26-3" name="__codelineno-26-3" href="#__codelineno-26-3"></a>    <span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-26-4" name="__codelineno-26-4" href="#__codelineno-26-4"></a><span class="sd">    Draw prediction on a given batch of dataset.</span>
<a id="__codelineno-26-5" name="__codelineno-26-5" href="#__codelineno-26-5"></a>
<a id="__codelineno-26-6" name="__codelineno-26-6" href="#__codelineno-26-6"></a><span class="sd">    Args:</span>
<a id="__codelineno-26-7" name="__codelineno-26-7" href="#__codelineno-26-7"></a><span class="sd">    batch (ChoiceDataset): the dataset to draw inference on.</span>
<a id="__codelineno-26-8" name="__codelineno-26-8" href="#__codelineno-26-8"></a>
<a id="__codelineno-26-9" name="__codelineno-26-9" href="#__codelineno-26-9"></a><span class="sd">    Returns:</span>
<a id="__codelineno-26-10" name="__codelineno-26-10" href="#__codelineno-26-10"></a><span class="sd">    torch.Tensor: the predicted probabilities for each class, the behavior varies by self.pred_item.</span>
<a id="__codelineno-26-11" name="__codelineno-26-11" href="#__codelineno-26-11"></a><span class="sd">    (1: pred_item == True) While predicting items, the return tensor has shape (len(batch), num_items), out[i, j] is the predicted probability for choosing item j AMONG ALL ITEMS IN ITS CATEGORY in observation i. Please note that since probabilities are computed from within-category normalization, hence out.sum(dim=0) can be greater than 1 if there are multiple categories.</span>
<a id="__codelineno-26-12" name="__codelineno-26-12" href="#__codelineno-26-12"></a><span class="sd">    (2: pred_item == False) While predicting external labels for each observations, out[i, 0] is the predicted probability for label == 0 on the i-th observation, out[i, 1] is the predicted probability for label == 1 on the i-th observation. Generally, out[i, 0] + out[i, 1] = 1.0. However, this could be false if under-flowing/over-flowing issue is encountered.</span>
<a id="__codelineno-26-13" name="__codelineno-26-13" href="#__codelineno-26-13"></a><span class="sd">    We highly recommend users to use the forward function to get the log-prob instead.</span>
<a id="__codelineno-26-14" name="__codelineno-26-14" href="#__codelineno-26-14"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-26-15" name="__codelineno-26-15" href="#__codelineno-26-15"></a>    <span class="k">pass</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-27-1" name="__codelineno-27-1" href="#__codelineno-27-1"></a><span class="n">proba</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>/Users/tianyudu/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/functional.py:1909: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn(&quot;nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.&quot;)
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-28-1" name="__codelineno-28-1" href="#__codelineno-28-1"></a><span class="n">proba</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>tensor([[0.4757, 0.5243],
        [0.5466, 0.4534],
        [0.5449, 0.4551],
        ...,
        [0.4290, 0.5710],
        [0.5680, 0.4320],
        [0.5732, 0.4268]])
</code></pre></div>
<p>Let's verify that each row of <code>proba</code> sum to one:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-29-1" name="__codelineno-29-1" href="#__codelineno-29-1"></a><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">proba</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>tensor(True)
</code></pre></div>
<p>And the second column of <code>proba</code> should be the <code>prob_positive_class</code> we calculated above:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-30-1" name="__codelineno-30-1" href="#__codelineno-30-1"></a><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">prob_positive_class</span> <span class="o">==</span> <span class="n">proba</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>tensor(True)
</code></pre></div>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../bemb_obs2prior_simulation/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Tutorial for BEMB obs2prior feature" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Tutorial for BEMB obs2prior feature
            </div>
          </div>
        </a>
      
      
        
        <a href="../projects/" class="md-footer__link md-footer__link--next" aria-label="Next: Related Projects" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Related Projects
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.5bf1dace.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.37e9125f.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>