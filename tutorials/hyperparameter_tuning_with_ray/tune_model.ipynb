{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter Tuning with Ray-Tune Tutorial\n",
    "This tutorial helps you tune hyper-parameters (e.g., learning rate, batch size, number of latent dimensions, etc) in BEMB.\n",
    "\n",
    "We will be using the `ray` library, which enables parallelization, to tune hyper-parameters.\n",
    "\n",
    "For more details regarding using Ray to tune hyper-parameters of models (especially PyTorch lightning models), please refer to this [tutorial](https://docs.ray.io/en/latest/ray-core/examples/using-ray-with-pytorch-lightning.html).\n",
    "\n",
    "Author: Tianyu Du\n",
    "Date: July. 27, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_choice.data import ChoiceDataset\n",
    "from bemb.model import LitBEMBFlex\n",
    "from bemb.utils.run_helper import run\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bemb.model import LitBEMBFlex\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_choice.data.utils import create_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind tuning hyper-parameters is to find the best configuration of the model among a space of hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Datasets\n",
    "\n",
    "I use the simulated datasets from the *simulation* tutorial (refer to the `tutorials/simulation/simulation.ipynb` notebook).\n",
    "\n",
    "The `simulate_dataset` method returns a list of three `ChoiceDataset` corresponding to the train/validation/test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No `session_index` is provided, assume each choice instance is in its own session.\n"
     ]
    }
   ],
   "source": [
    "# def simulate_dataset() -> List[ChoiceDataset]:\n",
    "num_users = 1500\n",
    "num_items = 50\n",
    "data_size = 1000\n",
    "user_index = torch.LongTensor(np.random.choice(num_users, size=data_size))\n",
    "Us = np.arange(num_users)\n",
    "Is = np.sin(np.arange(num_users) / num_users * 4 * np.pi)\n",
    "Is = (Is + 1) / 2 * num_items\n",
    "Is = Is.astype(int)\n",
    "\n",
    "PREFERENCE = dict((u, i) for (u, i) in zip(Us, Is))\n",
    "\n",
    "item_index = torch.LongTensor(np.random.choice(num_items, size=data_size))\n",
    "\n",
    "for idx in range(data_size):\n",
    "    if np.random.rand() <= 0.5:\n",
    "        item_index[idx] = PREFERENCE[int(user_index[idx])]\n",
    "\n",
    "# df = pd.DataFrame(data={'item': item_index, 'user': user_index}).groupby(['item', 'user']).size().rename('size').reset_index()\n",
    "# df = df.pivot('item', 'user', 'size').fillna(0.0)\n",
    "\n",
    "user_obs = torch.zeros(num_users, num_items)\n",
    "user_obs[torch.arange(num_users), Is] = 1\n",
    "\n",
    "item_obs = torch.eye(num_items)\n",
    "\n",
    "dataset = ChoiceDataset(user_index=user_index, item_index=item_index, user_obs=user_obs, item_obs=item_obs)\n",
    "\n",
    "idx = np.random.permutation(len(dataset))\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "train_idx = idx[:train_size]\n",
    "val_idx = idx[train_size: train_size + val_size]\n",
    "test_idx = idx[train_size + val_size:]\n",
    "\n",
    "dataset_list = [dataset[train_idx], dataset[val_idx], dataset[test_idx]]\n",
    "    # return dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChoiceDataset(label=[], item_index=[800], user_index=[800], session_index=[800], item_availability=[], user_obs=[1500, 50], item_obs=[50, 50], device=cpu),\n",
       " ChoiceDataset(label=[], item_index=[100], user_index=[100], session_index=[100], item_availability=[], user_obs=[1500, 50], item_obs=[50, 50], device=cpu),\n",
       " ChoiceDataset(label=[], item_index=[100], user_index=[100], session_index=[100], item_availability=[], user_obs=[1500, 50], item_obs=[50, 50], device=cpu)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset_list = simulate_dataset()\n",
    "dataset_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 3\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_specific_params(hparams):\n",
    "    bemb = LitBEMBFlex(\n",
    "        learning_rate=hparams['learning_rate'],  # set the learning rate, feel free to play with different levels.\n",
    "        pred_item=True,  # let the model predict item_index, don't change this one.\n",
    "        num_seeds=32,  # number of Monte Carlo samples for estimating the ELBO.\n",
    "        utility_formula='theta_user * alpha_item',  # the utility formula.\n",
    "        num_users=num_users,\n",
    "        num_items=num_items,\n",
    "        num_user_obs=dataset.user_obs.shape[1],\n",
    "        num_item_obs=dataset.item_obs.shape[1],\n",
    "        # whether to turn on obs2prior for each parameter.\n",
    "        obs2prior_dict={'theta_user': hparams['obs2prior'], 'alpha_item': hparams['obs2prior']},\n",
    "        # the dimension of latents, since the utility is an inner product of theta and alpha, they should have\n",
    "        # the same dimension.\n",
    "        coef_dim_dict={'theta_user': hparams['latent_dim'], 'alpha_item': hparams['latent_dim']}\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10,\n",
    "        check_val_every_n_epoch=1,\n",
    "        log_every_n_steps=1,\n",
    "        gpus=0,\n",
    "        progress_bar_refresh_rate=0,\n",
    "        auto_lr_find=False,\n",
    "        logger=TensorBoardLogger(save_dir='./', name='', version='.'),\n",
    "        callbacks=[TuneReportCallback({'val_ll': 'val_ll', 'val_acc': 'val_acc'}, on='validation_end'),\n",
    "                   EarlyStopping(monitor='val_acc', patience=30, mode='max')])\n",
    "\n",
    "    # find an appropriate learning rate.\n",
    "    # trainer.tune(bemb,\n",
    "    #              train_dataloaders=DataLoader(dataset_list[0]),\n",
    "    #              val_dataloaders=DataLoader(dataset_list[1]))\n",
    "                #  test_dataloaders=dataset_list[2])\n",
    "    trainer.fit(bemb, train_dataloaders=create_data_loader(dataset_list[0]), val_dataloaders=create_data_loader(dataset_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'learning_rate': tune.choice([0.01, 0.03, 0.1, 0.3]),\n",
    "    'latent_dim': tune.choice([10, 20, 50, 100]),\n",
    "    'obs2prior': tune.choice([True, False])\n",
    "}\n",
    "\n",
    "# scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n",
    "scheduler = None\n",
    "\n",
    "reporter = CLIReporter(parameter_columns=list(config.keys()),\n",
    "                        metric_columns=['val_ll', 'val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 01:35:23,380\tWARNING tune.py:668 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, set tune.run(resources_per_trial={'gpu': 1}...) which allows Tune to expose 1 GPU to each trial. You can also override `Trainable.default_resource_request` if using the Trainable API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-07-27 01:35:23 (running for 00:00:00.17)\n",
      "Memory usage on this node: 5.7/125.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/76.98 GiB heap, 0.0/36.98 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/tianyudu/ray_results/train_with_specific_params_2022-07-27_01-35-23\n",
      "Number of trials: 3/3 (2 PENDING, 1 RUNNING)\n",
      "+----------------------------------------+----------+---------------------+-----------------+--------------+-------------+\n",
      "| Trial name                             | status   | loc                 |   learning_rate |   latent_dim | obs2prior   |\n",
      "|----------------------------------------+----------+---------------------+-----------------+--------------+-------------|\n",
      "| train_with_specific_params_0e85e_00000 | RUNNING  | 192.168.0.158:14653 |            0.1  |          100 | True        |\n",
      "| train_with_specific_params_0e85e_00001 | PENDING  |                     |            0.3  |           10 | True        |\n",
      "| train_with_specific_params_0e85e_00002 | PENDING  |                     |            0.01 |           10 | False       |\n",
      "+----------------------------------------+----------+---------------------+-----------------+--------------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m BEMB: utility formula parsed:\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m [{'coefficient': ['theta_user', 'alpha_item'], 'observable': None}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m   | Name  | Type     | Params\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m -----------------------------------\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m 0 | model | BEMBFlex | 330 K \n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m -----------------------------------\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m 330 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m 330 K     Total params\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m 1.320     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14653)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m BEMB: utility formula parsed:\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m [{'coefficient': ['theta_user', 'alpha_item'], 'observable': None}]\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m BEMB: utility formula parsed:\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m [{'coefficient': ['theta_user', 'alpha_item'], 'observable': None}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m   | Name  | Type     | Params\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m -----------------------------------\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m 0 | model | BEMBFlex | 33.0 K\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m -----------------------------------\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m 33.0 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m 33.0 K    Total params\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m 0.132     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14692)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m   | Name  | Type     | Params\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m -----------------------------------\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m 0 | model | BEMBFlex | 31.0 K\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m -----------------------------------\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m 31.0 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m 31.0 K    Total params\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m 0.124     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m /home/tianyudu/anaconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_with_specific_params pid=14694)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_with_specific_params_0e85e_00000:\n",
      "  date: 2022-07-27_01-35-27\n",
      "  done: false\n",
      "  experiment_id: 3d450a95dec44141853a33437c188b91\n",
      "  hostname: aurora\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.158\n",
      "  pid: 14653\n",
      "  time_since_restore: 2.4849677085876465\n",
      "  time_this_iter_s: 2.4849677085876465\n",
      "  time_total_s: 2.4849677085876465\n",
      "  timestamp: 1658910927\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 0e85e_00000\n",
      "  val_acc: 0.03\n",
      "  val_ll: -18.660343192787114\n",
      "  warmup_time: 0.001985788345336914\n",
      "  \n",
      "Result for train_with_specific_params_0e85e_00001:\n",
      "  date: 2022-07-27_01-35-27\n",
      "  done: false\n",
      "  experiment_id: 1cc669fd41a743e7b52da84c466a60ce\n",
      "  hostname: aurora\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.158\n",
      "  pid: 14692\n",
      "  time_since_restore: 0.8729078769683838\n",
      "  time_this_iter_s: 0.8729078769683838\n",
      "  time_total_s: 0.8729078769683838\n",
      "  timestamp: 1658910927\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 0e85e_00001\n",
      "  val_acc: 0.03\n",
      "  val_ll: -5.681110769510269\n",
      "  warmup_time: 0.002115964889526367\n",
      "  \n",
      "Result for train_with_specific_params_0e85e_00002:\n",
      "  date: 2022-07-27_01-35-27\n",
      "  done: false\n",
      "  experiment_id: 2dd8a42e233d483ebdb8c012d3170668\n",
      "  hostname: aurora\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.158\n",
      "  pid: 14694\n",
      "  time_since_restore: 0.9212470054626465\n",
      "  time_this_iter_s: 0.9212470054626465\n",
      "  time_total_s: 0.9212470054626465\n",
      "  timestamp: 1658910927\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 0e85e_00002\n",
      "  val_acc: 0.04\n",
      "  val_ll: -7.79076402451843\n",
      "  warmup_time: 0.0019600391387939453\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-07-27 01:35:28 (running for 00:00:05.18)\n",
      "Memory usage on this node: 8.7/125.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 12.0/16 CPUs, 0/1 GPUs, 0.0/76.98 GiB heap, 0.0/36.98 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/tianyudu/ray_results/train_with_specific_params_2022-07-27_01-35-23\n",
      "Number of trials: 3/3 (3 RUNNING)\n",
      "+----------------------------------------+----------+---------------------+-----------------+--------------+-------------+-----------+-----------+\n",
      "| Trial name                             | status   | loc                 |   learning_rate |   latent_dim | obs2prior   |    val_ll |   val_acc |\n",
      "|----------------------------------------+----------+---------------------+-----------------+--------------+-------------+-----------+-----------|\n",
      "| train_with_specific_params_0e85e_00000 | RUNNING  | 192.168.0.158:14653 |            0.1  |          100 | True        | -18.6603  |      0.03 |\n",
      "| train_with_specific_params_0e85e_00001 | RUNNING  | 192.168.0.158:14692 |            0.3  |           10 | True        |  -4.08529 |      0.1  |\n",
      "| train_with_specific_params_0e85e_00002 | RUNNING  | 192.168.0.158:14694 |            0.01 |           10 | False       |  -7.44732 |      0.04 |\n",
      "+----------------------------------------+----------+---------------------+-----------------+--------------+-------------+-----------+-----------+\n",
      "\n",
      "\n",
      "Result for train_with_specific_params_0e85e_00001:\n",
      "  date: 2022-07-27_01-35-29\n",
      "  done: true\n",
      "  experiment_id: 1cc669fd41a743e7b52da84c466a60ce\n",
      "  experiment_tag: 1_latent_dim=10,learning_rate=0.3000,obs2prior=True\n",
      "  hostname: aurora\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.158\n",
      "  pid: 14692\n",
      "  time_since_restore: 2.9798271656036377\n",
      "  time_this_iter_s: 0.22636651992797852\n",
      "  time_total_s: 2.9798271656036377\n",
      "  timestamp: 1658910929\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 0e85e_00001\n",
      "  val_acc: 0.17\n",
      "  val_ll: -3.9133335845172406\n",
      "  warmup_time: 0.002115964889526367\n",
      "  \n",
      "Result for train_with_specific_params_0e85e_00002:\n",
      "  date: 2022-07-27_01-35-29\n",
      "  done: true\n",
      "  experiment_id: 2dd8a42e233d483ebdb8c012d3170668\n",
      "  experiment_tag: 2_latent_dim=10,learning_rate=0.0100,obs2prior=False\n",
      "  hostname: aurora\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.158\n",
      "  pid: 14694\n",
      "  time_since_restore: 2.999868869781494\n",
      "  time_this_iter_s: 0.22551584243774414\n",
      "  time_total_s: 2.999868869781494\n",
      "  timestamp: 1658910929\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 0e85e_00002\n",
      "  val_acc: 0.04\n",
      "  val_ll: -7.051034507006407\n",
      "  warmup_time: 0.0019600391387939453\n",
      "  \n",
      "Result for train_with_specific_params_0e85e_00000:\n",
      "  date: 2022-07-27_01-35-32\n",
      "  done: false\n",
      "  experiment_id: 3d450a95dec44141853a33437c188b91\n",
      "  hostname: aurora\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.158\n",
      "  pid: 14653\n",
      "  time_since_restore: 7.89588475227356\n",
      "  time_this_iter_s: 0.9463896751403809\n",
      "  time_total_s: 7.89588475227356\n",
      "  timestamp: 1658910932\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 6\n",
      "  trial_id: 0e85e_00000\n",
      "  val_acc: 0.14\n",
      "  val_ll: -13.589529894989488\n",
      "  warmup_time: 0.001985788345336914\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-07-27 01:35:33 (running for 00:00:10.53)\n",
      "Memory usage on this node: 6.1/125.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/76.98 GiB heap, 0.0/36.98 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/tianyudu/ray_results/train_with_specific_params_2022-07-27_01-35-23\n",
      "Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)\n",
      "+----------------------------------------+------------+---------------------+-----------------+--------------+-------------+-----------+-----------+\n",
      "| Trial name                             | status     | loc                 |   learning_rate |   latent_dim | obs2prior   |    val_ll |   val_acc |\n",
      "|----------------------------------------+------------+---------------------+-----------------+--------------+-------------+-----------+-----------|\n",
      "| train_with_specific_params_0e85e_00000 | RUNNING    | 192.168.0.158:14653 |            0.1  |          100 | True        | -13.5871  |      0.14 |\n",
      "| train_with_specific_params_0e85e_00001 | TERMINATED | 192.168.0.158:14692 |            0.3  |           10 | True        |  -3.91333 |      0.17 |\n",
      "| train_with_specific_params_0e85e_00002 | TERMINATED | 192.168.0.158:14694 |            0.01 |           10 | False       |  -7.05103 |      0.04 |\n",
      "+----------------------------------------+------------+---------------------+-----------------+--------------+-------------+-----------+-----------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 01:35:36,854\tINFO tune.py:747 -- Total run time: 13.49 seconds (13.37 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_with_specific_params_0e85e_00000:\n",
      "  date: 2022-07-27_01-35-36\n",
      "  done: true\n",
      "  experiment_id: 3d450a95dec44141853a33437c188b91\n",
      "  experiment_tag: 0_latent_dim=100,learning_rate=0.1000,obs2prior=True\n",
      "  hostname: aurora\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.158\n",
      "  pid: 14653\n",
      "  time_since_restore: 11.668215990066528\n",
      "  time_this_iter_s: 0.9347765445709229\n",
      "  time_total_s: 11.668215990066528\n",
      "  timestamp: 1658910936\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 0e85e_00000\n",
      "  val_acc: 0.15\n",
      "  val_ll: -13.605984133323654\n",
      "  warmup_time: 0.001985788345336914\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-07-27 01:35:36 (running for 00:00:13.37)\n",
      "Memory usage on this node: 6.2/125.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/76.98 GiB heap, 0.0/36.98 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/tianyudu/ray_results/train_with_specific_params_2022-07-27_01-35-23\n",
      "Number of trials: 3/3 (3 TERMINATED)\n",
      "+----------------------------------------+------------+---------------------+-----------------+--------------+-------------+-----------+-----------+\n",
      "| Trial name                             | status     | loc                 |   learning_rate |   latent_dim | obs2prior   |    val_ll |   val_acc |\n",
      "|----------------------------------------+------------+---------------------+-----------------+--------------+-------------+-----------+-----------|\n",
      "| train_with_specific_params_0e85e_00000 | TERMINATED | 192.168.0.158:14653 |            0.1  |          100 | True        | -13.606   |      0.15 |\n",
      "| train_with_specific_params_0e85e_00001 | TERMINATED | 192.168.0.158:14692 |            0.3  |           10 | True        |  -3.91333 |      0.17 |\n",
      "| train_with_specific_params_0e85e_00002 | TERMINATED | 192.168.0.158:14694 |            0.01 |           10 | False       |  -7.05103 |      0.04 |\n",
      "+----------------------------------------+------------+---------------------+-----------------+--------------+-------------+-----------+-----------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(\n",
    "    tune.with_parameters(train_with_specific_params),\n",
    "    # metric='val_ll',\n",
    "    # mode='min',\n",
    "    # NOTE: I am fixing the GPU support now, set it to 0.\n",
    "    resources_per_trial={'cpu': 4, 'gpu': 0},\n",
    "    config=config,\n",
    "    num_samples=num_samples,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5859d33511df864b0b7226a715510a0165ef032ed4b83eb4ae2c092f0788759c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
