
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://example.com/api_bemb/">
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.13">
    
    
      
        <title>API Reference BEMB - Deep Choice</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e411adfe.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.cc9b2e1e.min.css">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#api-reference-bemb" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Deep Choice" class="md-header__button md-logo" aria-label="Deep Choice" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Deep Choice
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              API Reference BEMB
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Deep Choice" class="md-nav__button md-logo" aria-label="Deep Choice" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Deep Choice
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../intro/" class="md-nav__link">
        About
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../install/" class="md-nav__link">
        Get Started
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../data_management/" class="md-nav__link">
        Tutorial for  Data Management
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../conditional_logit_model_mode_canada/" class="md-nav__link">
        Tutorial for Conditional Logit Model
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../nested_logit_model_house_cooling/" class="md-nav__link">
        Tutorial for Nested Logit Model
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../bemb/" class="md-nav__link">
        Tutorial for Bayesian Embedding (BEMB)
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../projects/" class="md-nav__link">
        Related Projects
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../test/" class="md-nav__link">
        Compability Tests
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../api_torch_choice/" class="md-nav__link">
        API Reference Torch-Choice
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          API Reference BEMB
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        API Reference BEMB
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#bemb" class="md-nav__link">
    bemb
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bemb.model" class="md-nav__link">
    model
  </a>
  
    <nav class="md-nav" aria-label="model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient" class="md-nav__link">
    bayesian_coefficient
  </a>
  
    <nav class="md-nav" aria-label="bayesian_coefficient">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient" class="md-nav__link">
    BayesianCoefficient
  </a>
  
    <nav class="md-nav" aria-label="BayesianCoefficient">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.device" class="md-nav__link">
    device
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.variational_distribution" class="md-nav__link">
    variational_distribution
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.variational_mean" class="md-nav__link">
    variational_mean
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.log_prior" class="md-nav__link">
    log_prior()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.log_variational" class="md-nav__link">
    log_variational()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.rsample" class="md-nav__link">
    rsample()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.update_variational_mean_fixed" class="md-nav__link">
    update_variational_mean_fixed()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear" class="md-nav__link">
    bayesian_linear
  </a>
  
    <nav class="md-nav" aria-label="bayesian_linear">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear.BayesianLinear" class="md-nav__link">
    BayesianLinear
  </a>
  
    <nav class="md-nav" aria-label="BayesianLinear">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear.BayesianLinear.W_variational_distribution" class="md-nav__link">
    W_variational_distribution
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear.BayesianLinear.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear.BayesianLinear.dsample" class="md-nav__link">
    dsample()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear.BayesianLinear.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear.BayesianLinear.log_prior" class="md-nav__link">
    log_prior()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear.BayesianLinear.log_variational" class="md-nav__link">
    log_variational()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear.BayesianLinear.rsample" class="md-nav__link">
    rsample()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb" class="md-nav__link">
    bemb
  </a>
  
    <nav class="md-nav" aria-label="bemb">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex" class="md-nav__link">
    BEMBFlex
  </a>
  
    <nav class="md-nav" aria-label="BEMBFlex">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.elbo" class="md-nav__link">
    elbo()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.get_within_category_accuracy" class="md-nav__link">
    get_within_category_accuracy()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.log_likelihood_all_items" class="md-nav__link">
    log_likelihood_all_items()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.log_likelihood_item_index" class="md-nav__link">
    log_likelihood_item_index()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.log_prior" class="md-nav__link">
    log_prior()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.log_variational" class="md-nav__link">
    log_variational()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.sample_coefficient_dictionary" class="md-nav__link">
    sample_coefficient_dictionary()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.parse_utility" class="md-nav__link">
    parse_utility()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb_flex_lightning" class="md-nav__link">
    bemb_flex_lightning
  </a>
  
    <nav class="md-nav" aria-label="bemb_flex_lightning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb_flex_lightning.LitBEMBFlex" class="md-nav__link">
    LitBEMBFlex
  </a>
  
    <nav class="md-nav" aria-label="LitBEMBFlex">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb_flex_lightning.LitBEMBFlex.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb_flex_lightning.LitBEMBFlex.configure_optimizers" class="md-nav__link">
    configure_optimizers()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb_flex_lightning.LitBEMBFlex.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb_flex_lightning.LitBEMBFlex.test_step" class="md-nav__link">
    test_step()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb_flex_lightning.LitBEMBFlex.training_step" class="md-nav__link">
    training_step()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb_flex_lightning.LitBEMBFlex.validation_step" class="md-nav__link">
    validation_step()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#bemb" class="md-nav__link">
    bemb
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bemb.model" class="md-nav__link">
    model
  </a>
  
    <nav class="md-nav" aria-label="model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient" class="md-nav__link">
    bayesian_coefficient
  </a>
  
    <nav class="md-nav" aria-label="bayesian_coefficient">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient" class="md-nav__link">
    BayesianCoefficient
  </a>
  
    <nav class="md-nav" aria-label="BayesianCoefficient">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.device" class="md-nav__link">
    device
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.variational_distribution" class="md-nav__link">
    variational_distribution
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.variational_mean" class="md-nav__link">
    variational_mean
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.log_prior" class="md-nav__link">
    log_prior()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.log_variational" class="md-nav__link">
    log_variational()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.rsample" class="md-nav__link">
    rsample()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_coefficient.BayesianCoefficient.update_variational_mean_fixed" class="md-nav__link">
    update_variational_mean_fixed()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear" class="md-nav__link">
    bayesian_linear
  </a>
  
    <nav class="md-nav" aria-label="bayesian_linear">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear.BayesianLinear" class="md-nav__link">
    BayesianLinear
  </a>
  
    <nav class="md-nav" aria-label="BayesianLinear">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear.BayesianLinear.W_variational_distribution" class="md-nav__link">
    W_variational_distribution
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear.BayesianLinear.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear.BayesianLinear.dsample" class="md-nav__link">
    dsample()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear.BayesianLinear.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear.BayesianLinear.log_prior" class="md-nav__link">
    log_prior()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear.BayesianLinear.log_variational" class="md-nav__link">
    log_variational()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bayesian_linear.BayesianLinear.rsample" class="md-nav__link">
    rsample()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb" class="md-nav__link">
    bemb
  </a>
  
    <nav class="md-nav" aria-label="bemb">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex" class="md-nav__link">
    BEMBFlex
  </a>
  
    <nav class="md-nav" aria-label="BEMBFlex">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.elbo" class="md-nav__link">
    elbo()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.get_within_category_accuracy" class="md-nav__link">
    get_within_category_accuracy()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.log_likelihood_all_items" class="md-nav__link">
    log_likelihood_all_items()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.log_likelihood_item_index" class="md-nav__link">
    log_likelihood_item_index()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.log_prior" class="md-nav__link">
    log_prior()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.log_variational" class="md-nav__link">
    log_variational()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.BEMBFlex.sample_coefficient_dictionary" class="md-nav__link">
    sample_coefficient_dictionary()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb.parse_utility" class="md-nav__link">
    parse_utility()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb_flex_lightning" class="md-nav__link">
    bemb_flex_lightning
  </a>
  
    <nav class="md-nav" aria-label="bemb_flex_lightning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb_flex_lightning.LitBEMBFlex" class="md-nav__link">
    LitBEMBFlex
  </a>
  
    <nav class="md-nav" aria-label="LitBEMBFlex">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb_flex_lightning.LitBEMBFlex.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb_flex_lightning.LitBEMBFlex.configure_optimizers" class="md-nav__link">
    configure_optimizers()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb_flex_lightning.LitBEMBFlex.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb_flex_lightning.LitBEMBFlex.test_step" class="md-nav__link">
    test_step()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb_flex_lightning.LitBEMBFlex.training_step" class="md-nav__link">
    training_step()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bemb.model.bemb_flex_lightning.LitBEMBFlex.validation_step" class="md-nav__link">
    validation_step()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


<h1 id="api-reference-bemb">API Reference: BEMB</h1>


  <div class="doc doc-object doc-module">

<a id="bemb"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">










  <div class="doc doc-object doc-module">



<h2 id="bemb.model" class="doc doc-heading">
        <code>model</code>


  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h2>

    <div class="doc doc-contents ">




  <div class="doc doc-children">










  <div class="doc doc-object doc-module">



<h3 id="bemb.model.bayesian_coefficient" class="doc doc-heading">
        <code>bayesian_coefficient</code>



</h3>

    <div class="doc doc-contents ">

      <p>Bayesian Coefficient is the building block for the BEMB model.</p>
<p>Author: Tianyu Du
Update: Apr. 28, 2022</p>



  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h4 id="bemb.model.bayesian_coefficient.BayesianCoefficient" class="doc doc-heading">
        <code>
BayesianCoefficient            (<span title="torch.nn.modules.module.Module">Module</span>)
        </code>



</h4>

    <div class="doc doc-contents ">


        <details class="quote">
          <summary>Source code in <code>bemb/model/bayesian_coefficient.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BayesianCoefficient</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">variation</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                 <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">obs2prior</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                 <span class="n">num_obs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">prior_variance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;The Bayesian coefficient object represents a learnable tensor mu_i in R^k, where i is from a family (e.g., user, item)</span>
<span class="sd">            so there are num_classes * num_obs learnable weights in total.</span>
<span class="sd">            The prior distribution of mu_i is N(0, I) or N(H*X_obs(H shape=num_obs, X_obs shape=dim), Ix1).</span>
<span class="sd">            The posterior(i.e., variational) distribution of mu_i is a Gaussian distribution with learnable mean mu_i and unit covariance.</span>
<span class="sd">            The mean of the variational distribution consists of two parts:</span>
<span class="sd">                1. The fixed part, which is not learnable. This part is particularly useful when the researcher want to impose</span>
<span class="sd">                    some structure on the variational distribution. For example, the research might have some variational mean</span>
<span class="sd">                    learned from another model and wish to use BEMB to polish the learned mean.</span>
<span class="sd">                2. The flexible part, which is the main learnable part of the variational mean.</span>

<span class="sd">        Args:</span>
<span class="sd">            variation (str): the variation # TODO: this will be removed in the next version, after we have a complete</span>
<span class="sd">                test pipline.</span>
<span class="sd">            num_classes (int): number of classes in the coefficient. For example, if we have user-specific coefficients,</span>
<span class="sd">                `theta_user`, the `num_classes` should be the number of users. If we have item-specific coefficients,</span>
<span class="sd">                the the `num_classes` should be the number of items.</span>
<span class="sd">            obs2prior (bool): whether the mean of coefficient prior depends on the observable or not.</span>
<span class="sd">            num_obs (int, optional): the number of observables associated with each class. For example, if the coefficient</span>
<span class="sd">                if item-specific, and we have `obs2prior` set to True, the `num_obs` should be the number of observables</span>
<span class="sd">                for each item.</span>
<span class="sd">                Defaults to None.</span>
<span class="sd">            dim (int, optional): the dimension of the coefficient.</span>
<span class="sd">                Defaults to 1.</span>
<span class="sd">            prior_variance (float): the variance of the prior distribution of coefficient.</span>
<span class="sd">                Defaults to 1.0.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BayesianCoefficient</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># do we use this at all? TODO: drop self.variation.</span>
        <span class="k">assert</span> <span class="n">variation</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;item&#39;</span><span class="p">,</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;constant&#39;</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">variation</span> <span class="o">=</span> <span class="n">variation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">obs2prior</span> <span class="o">=</span> <span class="n">obs2prior</span>
        <span class="k">if</span> <span class="n">variation</span> <span class="o">==</span> <span class="s1">&#39;constant&#39;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="n">obs2prior</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_obs</span> <span class="o">=</span> <span class="n">num_obs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>  <span class="c1"># the dimension of greek letter parameter.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior_variance</span> <span class="o">=</span> <span class="n">prior_variance</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_variance</span> <span class="o">&gt;</span> <span class="mi">0</span>

        <span class="c1"># create prior distribution.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">obs2prior</span><span class="p">:</span>
            <span class="c1"># the mean of prior distribution depends on observables.</span>
            <span class="c1"># initiate a Bayesian Coefficient with shape (dim, num_obs) standard Gaussian.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_H</span> <span class="o">=</span> <span class="n">BayesianCoefficient</span><span class="p">(</span><span class="n">variation</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">obs2prior</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                               <span class="n">dim</span><span class="o">=</span><span class="n">num_obs</span><span class="p">,</span> <span class="n">prior_variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                <span class="s1">&#39;prior_zero_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>

        <span class="c1"># self.prior_cov_factor = nn.Parameter(torch.zeros(num_classes, dim, 1), requires_grad=False)</span>
        <span class="c1"># self.prior_cov_diag = nn.Parameter(torch.ones(num_classes, dim), requires_grad=False)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;prior_cov_factor&#39;</span><span class="p">,</span>
                             <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;prior_cov_diag&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
            <span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_variance</span><span class="p">)</span>

        <span class="c1"># create variational distribution.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">variational_mean_flexible</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">variational_logstd</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;variational_cov_factor&#39;</span><span class="p">,</span>
                             <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">variational_mean_fixed</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Constructs a string representation of the Bayesian coefficient object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: the string representation of the Bayesian coefficient object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">obs2prior</span><span class="p">:</span>
            <span class="n">prior_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;prior=N(H*X_obs(H shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_H</span><span class="o">.</span><span class="n">prior_zero_mean</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, X_obs shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_H</span><span class="o">.</span><span class="n">dim</span><span class="si">}</span><span class="s1">), Ix</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_variance</span><span class="si">}</span><span class="s1">)&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">prior_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;prior=N(0, I)&#39;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;BayesianCoefficient(num_classes=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="si">}</span><span class="s1">, dimension=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">prior_str</span><span class="si">}</span><span class="s1">)&#39;</span>

    <span class="k">def</span> <span class="nf">update_variational_mean_fixed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Updates the fixed part of the mean of the variational distribution.</span>

<span class="sd">        Args:</span>
<span class="sd">            new_value (torch.Tensor): the new value of the fixed part of the mean of the variational distribution.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">new_value</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">variational_mean_flexible</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">variational_mean_fixed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;variational_mean_fixed&#39;</span><span class="p">,</span> <span class="n">new_value</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">variational_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns the mean of the variational distribution.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: the current mean of the variational distribution with shape (num_classes, dim).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">variational_mean_fixed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">variational_mean_flexible</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">variational_mean_fixed</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">variational_mean_flexible</span>

    <span class="k">def</span> <span class="nf">log_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">sample</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">H_sample</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                  <span class="n">x_obs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the logP_{Prior}(Coefficient Sample) for provided samples of the coefficient. The prior will either be a</span>
<span class="sd">        zero-mean Gaussian (if `obs2prior` is False) or a Gaussian with a learnable mean (if `obs2prior` is True).</span>

<span class="sd">        Args:</span>
<span class="sd">            sample (torch.Tensor): Monte Carlo samples of the variable with shape (num_seeds, num_classes, dim), where</span>
<span class="sd">                sample[i, :, :] corresponds to one sample of the coefficient.</span>

<span class="sd">            # arguments required only if `obs2prior == True`:</span>
<span class="sd">            H_sample (Optional[torch.Tensor], optional): Monte Carlo samples of the weight in obs2prior term, with shape</span>
<span class="sd">                (num_seeds, dim, self.num_obs), this is required if and only if obs2prior == True.</span>
<span class="sd">                Defaults to None.</span>
<span class="sd">            x_obs (Optional[torch.Tensor], optional): observables for obs2prior with shape (num_classes, num_obs),</span>
<span class="sd">                only required if and only if obs2prior == True.</span>
<span class="sd">                Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: the log prior of the variable with shape (num_seeds, num_classes).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># p(sample)</span>
        <span class="n">num_seeds</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">sample</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># shape (num_seeds, num_classes)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">obs2prior</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">H_sample</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_obs</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">x_obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_obs</span><span class="p">)</span>
            <span class="n">x_obs</span> <span class="o">=</span> <span class="n">x_obs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_obs</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
                <span class="n">num_seeds</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">H_sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">H_sample</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">H_sample</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_obs</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
            <span class="n">mu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">x_obs</span><span class="p">,</span> <span class="n">H_sample</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">mu</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_zero_mean</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">LowRankMultivariateNormal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span>
                                        <span class="n">cov_factor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_cov_factor</span><span class="p">,</span>
                                        <span class="n">cov_diag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_cov_diag</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">log_variational</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Given a set of sampled values of coefficients, with shape (num_seeds, num_classes, dim), computes the</span>
<span class="sd">            the log probability of these sampled values of coefficients under the current variational distribution.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample (torch.Tensor): a tensor of shape (num_seeds, num_classes, dim) containing sampled values of coefficients,</span>
<span class="sd">                where sample[i, :, :] corresponds to one sample of the coefficient.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: a tensor of shape (num_seeds, num_classes) containing the log probability of provided samples</span>
<span class="sd">                under the variational distribution. The output is splitted by random seeds and classes, you can sum</span>
<span class="sd">                along the second axis (i.e., the num_classes axis) to get the total log probability.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_seeds</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">sample</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variational_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">rsample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_seeds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Samples values of the coefficient from the variational distribution using re-parameterization trick.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_seeds (int, optional): number of values to be sampled. Defaults to 1.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[torch.Tensor, Tuple[torch.Tensor]]: if `obs2prior` is disabled, returns a tensor of shape (num_seeds, num_classes, dim)</span>
<span class="sd">                where each output[i, :, :] corresponds to one sample of the coefficient.</span>
<span class="sd">                If `obs2prior` is enabled, returns a tuple of samples: (1) a tensor of shape (num_seeds, num_classes, dim) containing</span>
<span class="sd">                sampled values of coefficient, and (2) a tensor o shape (num_seeds, dim, num_obs) containing samples of the H weight</span>
<span class="sd">                in the prior distribution.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">value_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variational_distribution</span><span class="o">.</span><span class="n">rsample</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">num_seeds</span><span class="p">]))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">obs2prior</span><span class="p">:</span>
            <span class="c1"># sample obs2prior H as well.</span>
            <span class="n">H_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_H</span><span class="o">.</span><span class="n">rsample</span><span class="p">(</span><span class="n">num_seeds</span><span class="o">=</span><span class="n">num_seeds</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">value_sample</span><span class="p">,</span> <span class="n">H_sample</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">value_sample</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">variational_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LowRankMultivariateNormal</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Constructs the current variational distribution of the coefficient from current variational mean and covariance.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">LowRankMultivariateNormal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">variational_mean</span><span class="p">,</span>
                                         <span class="n">cov_factor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">variational_cov_factor</span><span class="p">,</span>
                                         <span class="n">cov_diag</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">variational_logstd</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns the device of tensors contained in this module.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">variational_mean</span><span class="o">.</span><span class="n">device</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h5 id="bemb.model.bayesian_coefficient.BayesianCoefficient.device" class="doc doc-heading">
<code class="highlight language-python"><span class="n">device</span><span class="p">:</span> <span class="n">device</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h5>

    <div class="doc doc-contents ">

      <p>Returns the device of tensors contained in this module.</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="bemb.model.bayesian_coefficient.BayesianCoefficient.variational_distribution" class="doc doc-heading">
<code class="highlight language-python"><span class="n">variational_distribution</span><span class="p">:</span> <span class="n">LowRankMultivariateNormal</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h5>

    <div class="doc doc-contents ">

      <p>Constructs the current variational distribution of the coefficient from current variational mean and covariance.</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="bemb.model.bayesian_coefficient.BayesianCoefficient.variational_mean" class="doc doc-heading">
<code class="highlight language-python"><span class="n">variational_mean</span><span class="p">:</span> <span class="n">Tensor</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h5>

    <div class="doc doc-contents ">

      <p>Returns the mean of the variational distribution.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.Tensor</code></td>
      <td><p>the current mean of the variational distribution with shape (num_classes, dim).</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bayesian_coefficient.BayesianCoefficient.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variation</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">obs2prior</span><span class="p">,</span> <span class="n">num_obs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">prior_variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h5>

    <div class="doc doc-contents ">

      <p>The Bayesian coefficient object represents a learnable tensor mu_i in R^k, where i is from a family (e.g., user, item)
    so there are num_classes * num_obs learnable weights in total.
    The prior distribution of mu_i is N(0, I) or N(H*X_obs(H shape=num_obs, X_obs shape=dim), Ix1).
    The posterior(i.e., variational) distribution of mu_i is a Gaussian distribution with learnable mean mu_i and unit covariance.
    The mean of the variational distribution consists of two parts:
        1. The fixed part, which is not learnable. This part is particularly useful when the researcher want to impose
            some structure on the variational distribution. For example, the research might have some variational mean
            learned from another model and wish to use BEMB to polish the learned mean.
        2. The flexible part, which is the main learnable part of the variational mean.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>variation</code></td>
        <td><code>str</code></td>
        <td><p>the variation # TODO: this will be removed in the next version, after we have a complete
test pipline.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_classes</code></td>
        <td><code>int</code></td>
        <td><p>number of classes in the coefficient. For example, if we have user-specific coefficients,
<code>theta_user</code>, the <code>num_classes</code> should be the number of users. If we have item-specific coefficients,
the the <code>num_classes</code> should be the number of items.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>obs2prior</code></td>
        <td><code>bool</code></td>
        <td><p>whether the mean of coefficient prior depends on the observable or not.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_obs</code></td>
        <td><code>int</code></td>
        <td><p>the number of observables associated with each class. For example, if the coefficient
if item-specific, and we have <code>obs2prior</code> set to True, the <code>num_obs</code> should be the number of observables
for each item.
Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>dim</code></td>
        <td><code>int</code></td>
        <td><p>the dimension of the coefficient.
Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>prior_variance</code></td>
        <td><code>float</code></td>
        <td><p>the variance of the prior distribution of coefficient.
Defaults to 1.0.</p></td>
        <td><code>1.0</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bayesian_coefficient.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">variation</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
             <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">obs2prior</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
             <span class="n">num_obs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
             <span class="n">prior_variance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
             <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;The Bayesian coefficient object represents a learnable tensor mu_i in R^k, where i is from a family (e.g., user, item)</span>
<span class="sd">        so there are num_classes * num_obs learnable weights in total.</span>
<span class="sd">        The prior distribution of mu_i is N(0, I) or N(H*X_obs(H shape=num_obs, X_obs shape=dim), Ix1).</span>
<span class="sd">        The posterior(i.e., variational) distribution of mu_i is a Gaussian distribution with learnable mean mu_i and unit covariance.</span>
<span class="sd">        The mean of the variational distribution consists of two parts:</span>
<span class="sd">            1. The fixed part, which is not learnable. This part is particularly useful when the researcher want to impose</span>
<span class="sd">                some structure on the variational distribution. For example, the research might have some variational mean</span>
<span class="sd">                learned from another model and wish to use BEMB to polish the learned mean.</span>
<span class="sd">            2. The flexible part, which is the main learnable part of the variational mean.</span>

<span class="sd">    Args:</span>
<span class="sd">        variation (str): the variation # TODO: this will be removed in the next version, after we have a complete</span>
<span class="sd">            test pipline.</span>
<span class="sd">        num_classes (int): number of classes in the coefficient. For example, if we have user-specific coefficients,</span>
<span class="sd">            `theta_user`, the `num_classes` should be the number of users. If we have item-specific coefficients,</span>
<span class="sd">            the the `num_classes` should be the number of items.</span>
<span class="sd">        obs2prior (bool): whether the mean of coefficient prior depends on the observable or not.</span>
<span class="sd">        num_obs (int, optional): the number of observables associated with each class. For example, if the coefficient</span>
<span class="sd">            if item-specific, and we have `obs2prior` set to True, the `num_obs` should be the number of observables</span>
<span class="sd">            for each item.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        dim (int, optional): the dimension of the coefficient.</span>
<span class="sd">            Defaults to 1.</span>
<span class="sd">        prior_variance (float): the variance of the prior distribution of coefficient.</span>
<span class="sd">            Defaults to 1.0.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BayesianCoefficient</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="c1"># do we use this at all? TODO: drop self.variation.</span>
    <span class="k">assert</span> <span class="n">variation</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;item&#39;</span><span class="p">,</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;constant&#39;</span><span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">variation</span> <span class="o">=</span> <span class="n">variation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">obs2prior</span> <span class="o">=</span> <span class="n">obs2prior</span>
    <span class="k">if</span> <span class="n">variation</span> <span class="o">==</span> <span class="s1">&#39;constant&#39;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="n">obs2prior</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_obs</span> <span class="o">=</span> <span class="n">num_obs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>  <span class="c1"># the dimension of greek letter parameter.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prior_variance</span> <span class="o">=</span> <span class="n">prior_variance</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_variance</span> <span class="o">&gt;</span> <span class="mi">0</span>

    <span class="c1"># create prior distribution.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">obs2prior</span><span class="p">:</span>
        <span class="c1"># the mean of prior distribution depends on observables.</span>
        <span class="c1"># initiate a Bayesian Coefficient with shape (dim, num_obs) standard Gaussian.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior_H</span> <span class="o">=</span> <span class="n">BayesianCoefficient</span><span class="p">(</span><span class="n">variation</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">obs2prior</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                           <span class="n">dim</span><span class="o">=</span><span class="n">num_obs</span><span class="p">,</span> <span class="n">prior_variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s1">&#39;prior_zero_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>

    <span class="c1"># self.prior_cov_factor = nn.Parameter(torch.zeros(num_classes, dim, 1), requires_grad=False)</span>
    <span class="c1"># self.prior_cov_diag = nn.Parameter(torch.ones(num_classes, dim), requires_grad=False)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;prior_cov_factor&#39;</span><span class="p">,</span>
                         <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;prior_cov_diag&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
        <span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_variance</span><span class="p">)</span>

    <span class="c1"># create variational distribution.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">variational_mean_flexible</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">variational_logstd</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;variational_cov_factor&#39;</span><span class="p">,</span>
                         <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">variational_mean_fixed</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bayesian_coefficient.BayesianCoefficient.__repr__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h5>

    <div class="doc doc-contents ">

      <p>Constructs a string representation of the Bayesian coefficient object.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>str</code></td>
      <td><p>the string representation of the Bayesian coefficient object.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bayesian_coefficient.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Constructs a string representation of the Bayesian coefficient object.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: the string representation of the Bayesian coefficient object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">obs2prior</span><span class="p">:</span>
        <span class="n">prior_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;prior=N(H*X_obs(H shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_H</span><span class="o">.</span><span class="n">prior_zero_mean</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, X_obs shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_H</span><span class="o">.</span><span class="n">dim</span><span class="si">}</span><span class="s1">), Ix</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_variance</span><span class="si">}</span><span class="s1">)&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">prior_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;prior=N(0, I)&#39;</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;BayesianCoefficient(num_classes=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="si">}</span><span class="s1">, dimension=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">prior_str</span><span class="si">}</span><span class="s1">)&#39;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bayesian_coefficient.BayesianCoefficient.log_prior" class="doc doc-heading">
<code class="highlight language-python"><span class="n">log_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">H_sample</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">x_obs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>Computes the logP_{Prior}(Coefficient Sample) for provided samples of the coefficient. The prior will either be a
zero-mean Gaussian (if <code>obs2prior</code> is False) or a Gaussian with a learnable mean (if <code>obs2prior</code> is True).</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>sample</code></td>
        <td><code>torch.Tensor</code></td>
        <td><p>Monte Carlo samples of the variable with shape (num_seeds, num_classes, dim), where
sample[i, :, :] corresponds to one sample of the coefficient.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>#</code></td>
        <td><code>arguments required only if `obs2prior == True`</code></td>
        <td></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>H_sample</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Monte Carlo samples of the weight in obs2prior term, with shape
(num_seeds, dim, self.num_obs), this is required if and only if obs2prior == True.
Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>x_obs</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>observables for obs2prior with shape (num_classes, num_obs),
only required if and only if obs2prior == True.
Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.Tensor</code></td>
      <td><p>the log prior of the variable with shape (num_seeds, num_classes).</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bayesian_coefficient.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">log_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
              <span class="n">sample</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
              <span class="n">H_sample</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
              <span class="n">x_obs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the logP_{Prior}(Coefficient Sample) for provided samples of the coefficient. The prior will either be a</span>
<span class="sd">    zero-mean Gaussian (if `obs2prior` is False) or a Gaussian with a learnable mean (if `obs2prior` is True).</span>

<span class="sd">    Args:</span>
<span class="sd">        sample (torch.Tensor): Monte Carlo samples of the variable with shape (num_seeds, num_classes, dim), where</span>
<span class="sd">            sample[i, :, :] corresponds to one sample of the coefficient.</span>

<span class="sd">        # arguments required only if `obs2prior == True`:</span>
<span class="sd">        H_sample (Optional[torch.Tensor], optional): Monte Carlo samples of the weight in obs2prior term, with shape</span>
<span class="sd">            (num_seeds, dim, self.num_obs), this is required if and only if obs2prior == True.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        x_obs (Optional[torch.Tensor], optional): observables for obs2prior with shape (num_classes, num_obs),</span>
<span class="sd">            only required if and only if obs2prior == True.</span>
<span class="sd">            Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: the log prior of the variable with shape (num_seeds, num_classes).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># p(sample)</span>
    <span class="n">num_seeds</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">sample</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># shape (num_seeds, num_classes)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">obs2prior</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">H_sample</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_obs</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">x_obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_obs</span><span class="p">)</span>
        <span class="n">x_obs</span> <span class="o">=</span> <span class="n">x_obs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_obs</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
            <span class="n">num_seeds</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">H_sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">H_sample</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">H_sample</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_obs</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">x_obs</span><span class="p">,</span> <span class="n">H_sample</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">mu</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_zero_mean</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">LowRankMultivariateNormal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span>
                                    <span class="n">cov_factor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_cov_factor</span><span class="p">,</span>
                                    <span class="n">cov_diag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_cov_diag</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bayesian_coefficient.BayesianCoefficient.log_variational" class="doc doc-heading">
<code class="highlight language-python"><span class="n">log_variational</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>Given a set of sampled values of coefficients, with shape (num_seeds, num_classes, dim), computes the
    the log probability of these sampled values of coefficients under the current variational distribution.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>sample</code></td>
        <td><code>torch.Tensor</code></td>
        <td><p>a tensor of shape (num_seeds, num_classes, dim) containing sampled values of coefficients,
where sample[i, :, :] corresponds to one sample of the coefficient.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.Tensor</code></td>
      <td><p>a tensor of shape (num_seeds, num_classes) containing the log probability of provided samples
    under the variational distribution. The output is splitted by random seeds and classes, you can sum
    along the second axis (i.e., the num_classes axis) to get the total log probability.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bayesian_coefficient.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">log_variational</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Given a set of sampled values of coefficients, with shape (num_seeds, num_classes, dim), computes the</span>
<span class="sd">        the log probability of these sampled values of coefficients under the current variational distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample (torch.Tensor): a tensor of shape (num_seeds, num_classes, dim) containing sampled values of coefficients,</span>
<span class="sd">            where sample[i, :, :] corresponds to one sample of the coefficient.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: a tensor of shape (num_seeds, num_classes) containing the log probability of provided samples</span>
<span class="sd">            under the variational distribution. The output is splitted by random seeds and classes, you can sum</span>
<span class="sd">            along the second axis (i.e., the num_classes axis) to get the total log probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_seeds</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">sample</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variational_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bayesian_coefficient.BayesianCoefficient.rsample" class="doc doc-heading">
<code class="highlight language-python"><span class="n">rsample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_seeds</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>Samples values of the coefficient from the variational distribution using re-parameterization trick.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>num_seeds</code></td>
        <td><code>int</code></td>
        <td><p>number of values to be sampled. Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[torch.Tensor, Tuple[torch.Tensor]]</code></td>
      <td><p>if <code>obs2prior</code> is disabled, returns a tensor of shape (num_seeds, num_classes, dim)
    where each output[i, :, :] corresponds to one sample of the coefficient.
    If <code>obs2prior</code> is enabled, returns a tuple of samples: (1) a tensor of shape (num_seeds, num_classes, dim) containing
    sampled values of coefficient, and (2) a tensor o shape (num_seeds, dim, num_obs) containing samples of the H weight
    in the prior distribution.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bayesian_coefficient.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">rsample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_seeds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Samples values of the coefficient from the variational distribution using re-parameterization trick.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_seeds (int, optional): number of values to be sampled. Defaults to 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Union[torch.Tensor, Tuple[torch.Tensor]]: if `obs2prior` is disabled, returns a tensor of shape (num_seeds, num_classes, dim)</span>
<span class="sd">            where each output[i, :, :] corresponds to one sample of the coefficient.</span>
<span class="sd">            If `obs2prior` is enabled, returns a tuple of samples: (1) a tensor of shape (num_seeds, num_classes, dim) containing</span>
<span class="sd">            sampled values of coefficient, and (2) a tensor o shape (num_seeds, dim, num_obs) containing samples of the H weight</span>
<span class="sd">            in the prior distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">value_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variational_distribution</span><span class="o">.</span><span class="n">rsample</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">num_seeds</span><span class="p">]))</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">obs2prior</span><span class="p">:</span>
        <span class="c1"># sample obs2prior H as well.</span>
        <span class="n">H_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_H</span><span class="o">.</span><span class="n">rsample</span><span class="p">(</span><span class="n">num_seeds</span><span class="o">=</span><span class="n">num_seeds</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">value_sample</span><span class="p">,</span> <span class="n">H_sample</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">value_sample</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bayesian_coefficient.BayesianCoefficient.update_variational_mean_fixed" class="doc doc-heading">
<code class="highlight language-python"><span class="n">update_variational_mean_fixed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_value</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>Updates the fixed part of the mean of the variational distribution.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>new_value</code></td>
        <td><code>torch.Tensor</code></td>
        <td><p>the new value of the fixed part of the mean of the variational distribution.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bayesian_coefficient.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">update_variational_mean_fixed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Updates the fixed part of the mean of the variational distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        new_value (torch.Tensor): the new value of the fixed part of the mean of the variational distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">new_value</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">variational_mean_flexible</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">variational_mean_fixed</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;variational_mean_fixed&#39;</span><span class="p">,</span> <span class="n">new_value</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">



<h3 id="bemb.model.bayesian_linear" class="doc doc-heading">
        <code>bayesian_linear</code>



</h3>

    <div class="doc doc-contents ">

      <p>Bayesian tensor object.</p>



  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h4 id="bemb.model.bayesian_linear.BayesianLinear" class="doc doc-heading">
        <code>
BayesianLinear            (<span title="torch.nn.modules.module.Module">Module</span>)
        </code>



</h4>

    <div class="doc doc-contents ">


        <details class="quote">
          <summary>Source code in <code>bemb/model/bayesian_linear.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BayesianLinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">W_variational_mean_fixed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">W_prior_variance</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">b_prior_variance</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span>
                 <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Linear layer where weight and bias are modelled as distributions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;dtype is not Supported yet.&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>  <span class="c1"># the same as number of classes before.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span>  <span class="c1"># the same as latent dimension before.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>

        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># prior distributions for mean and bias.</span>
        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># the prior of weights are gausssian distributions independent across in_feature dimensions.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;W_prior_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;W_prior_logstd&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">W_prior_variance</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;b_prior_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;b_prior_logstd&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">b_prior_variance</span><span class="p">))</span>

        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># variational distributions for weight and bias.</span>
        <span class="c1"># ==============================================================================================================</span>
        <span class="k">if</span> <span class="n">W_variational_mean_fixed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_mean_fixed</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">W_variational_mean_fixed</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">),</span> \
                <span class="sa">f</span><span class="s1">&#39;W_variational_mean_fixed tensor should have shape (in_features, out_features), got </span><span class="si">{</span><span class="n">W_variational_mean_fixed</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;W_variational_mean_fixed&#39;</span><span class="p">,</span> <span class="n">W_variational_mean_fixed</span><span class="p">)</span>

        <span class="c1"># TODO: optionally add customizable initialization here.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_mean_flexible</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_logstd</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b_variational_mean</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">out_features</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b_variational_logstd</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">out_features</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b_sample</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">W_variational_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_mean_fixed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_mean_flexible</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_mean_fixed</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_mean_flexible</span>

    <span class="k">def</span> <span class="nf">rsample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_seeds</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
        <span class="sd">&quot;&quot;&quot;sample all parameters using re-parameterization trick.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span> <span class="o">=</span> <span class="n">num_seeds</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_distribution</span><span class="o">.</span><span class="n">rsample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">num_seeds</span><span class="p">]))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_variational_distribution</span><span class="o">.</span><span class="n">rsample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">num_seeds</span><span class="p">]))</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_sample</span>

    <span class="k">def</span> <span class="nf">dsample</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Deterministic sample method, set (W, b) sample to the mean of variational distribution.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_mean</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_variational_mean</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_sample</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s1">&#39;multiply&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward with weight sampling. Forward does out = XW + b, for forward() method behaves like the embedding layer</span>
<span class="sd">        in PyTorch, use the lookup() method.</span>
<span class="sd">        To have determinstic results, call self.dsample() before executing.</span>
<span class="sd">        To have stochastic results, call self.rsample() before executing.</span>
<span class="sd">        mode in [&#39;multiply&#39;, &#39;lookup&#39;]</span>

<span class="sd">        output shape: (num_seeds, batch_size, out_features).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;run BayesianLinear.rsample() or dsample() first to sample weight and bias.&#39;</span>

        <span class="c1"># if determinstic, num_seeds is set to 1.</span>
        <span class="c1"># w: (num_seeds, in_features=num_classes, out_features)</span>
        <span class="c1"># b: (num_seeds, out_features)</span>
        <span class="c1"># x: (N, in_features) if multiply and (N,) if lookup.</span>
        <span class="c1"># output: (num_seeds, N, out_features)</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;multiply&#39;</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (num_seeds, N, in_features)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span><span class="p">)</span>  <span class="c1"># (num_seeds, N, out_features)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;lookup&#39;</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span><span class="p">[:,</span> <span class="n">x</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># (num_seeds, N, out_features)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;mode=</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s1"> is not allowed.&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_sample</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">)</span>

        <span class="c1"># (num_seeds, N, out_features)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">W_variational_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;the weight variational distribution.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">W_variational_mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_variational_logstd</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">b_variational_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">b_variational_mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_variational_logstd</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_mean</span><span class="o">.</span><span class="n">device</span>

    <span class="k">def</span> <span class="nf">log_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Evaluate the likelihood of the provided samples of parameter under the current prior distribution.&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;run BayesianLinear.rsample() or dsample() first to sample weight and bias.&#39;</span>
        <span class="n">num_seeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">total_log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># log P(W_sample). shape = (num_seeds,)</span>
        <span class="n">W_prior</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">W_prior_mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_prior_logstd</span><span class="p">))</span>
        <span class="n">total_log_prob</span> <span class="o">+=</span> <span class="n">W_prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

        <span class="c1"># log P(b_sample) if applicable.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
            <span class="n">b_prior</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">b_prior_mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_prior_logstd</span><span class="p">))</span>
            <span class="n">total_log_prob</span> <span class="o">+=</span> <span class="n">b_prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_sample</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">total_log_prob</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_seeds</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">total_log_prob</span>

    <span class="k">def</span> <span class="nf">log_variational</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Evaluate the likelihood of the provided samples of parameter under the current variational distribution.&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;run BayesianLinear.rsample() or dsample() first to sample weight and bias.&#39;</span>
        <span class="n">num_seeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">total_log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">total_log_prob</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
            <span class="n">total_log_prob</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_variational_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_sample</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">total_log_prob</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_seeds</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">total_log_prob</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">prior_info</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;W_prior ~ N(mu=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">W_prior_mean</span><span class="si">}</span><span class="s1">, logstd=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">W_prior_logstd</span><span class="si">}</span><span class="s1">)&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
            <span class="n">prior_info</span> <span class="o">+=</span> <span class="sa">f</span><span class="s1">&#39;b_prior ~ N(mu=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">b_prior_mean</span><span class="si">}</span><span class="s1">, logstd=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">b_prior_logstd</span><span class="si">}</span><span class="s1">)&#39;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;BayesianLinear(in_features=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">in_features</span><span class="si">}</span><span class="s2">, out_features=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="si">}</span><span class="s2">, bias=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">prior_info</span><span class="si">}</span><span class="s2">)&quot;</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h5 id="bemb.model.bayesian_linear.BayesianLinear.W_variational_distribution" class="doc doc-heading">
<code class="highlight language-python"><span class="n">W_variational_distribution</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h5>

    <div class="doc doc-contents ">

      <p>the weight variational distribution.</p>
    </div>

  </div>









  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bayesian_linear.BayesianLinear.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">W_variational_mean_fixed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">W_prior_variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">b_prior_variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h5>

    <div class="doc doc-contents ">

      <p>Linear layer where weight and bias are modelled as distributions.</p>

        <details class="quote">
          <summary>Source code in <code>bemb/model/bayesian_linear.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">W_variational_mean_fixed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">W_prior_variance</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
             <span class="n">b_prior_variance</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span>
             <span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Linear layer where weight and bias are modelled as distributions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;dtype is not Supported yet.&#39;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>  <span class="c1"># the same as number of classes before.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span>  <span class="c1"># the same as latent dimension before.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>

    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># prior distributions for mean and bias.</span>
    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># the prior of weights are gausssian distributions independent across in_feature dimensions.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;W_prior_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;W_prior_logstd&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">W_prior_variance</span><span class="p">))</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;b_prior_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;b_prior_logstd&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">b_prior_variance</span><span class="p">))</span>

    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># variational distributions for weight and bias.</span>
    <span class="c1"># ==============================================================================================================</span>
    <span class="k">if</span> <span class="n">W_variational_mean_fixed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_mean_fixed</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">W_variational_mean_fixed</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">),</span> \
            <span class="sa">f</span><span class="s1">&#39;W_variational_mean_fixed tensor should have shape (in_features, out_features), got </span><span class="si">{</span><span class="n">W_variational_mean_fixed</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;W_variational_mean_fixed&#39;</span><span class="p">,</span> <span class="n">W_variational_mean_fixed</span><span class="p">)</span>

    <span class="c1"># TODO: optionally add customizable initialization here.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_mean_flexible</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_logstd</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b_variational_mean</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">out_features</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b_variational_logstd</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">out_features</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b_sample</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div>
        </details>
    </div>

  </div>




  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bayesian_linear.BayesianLinear.dsample" class="doc doc-heading">
<code class="highlight language-python"><span class="n">dsample</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>Deterministic sample method, set (W, b) sample to the mean of variational distribution.</p>

        <details class="quote">
          <summary>Source code in <code>bemb/model/bayesian_linear.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">dsample</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deterministic sample method, set (W, b) sample to the mean of variational distribution.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_mean</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_variational_mean</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_sample</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bayesian_linear.BayesianLinear.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;multiply&#39;</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>Forward with weight sampling. Forward does out = XW + b, for forward() method behaves like the embedding layer
in PyTorch, use the lookup() method.
To have determinstic results, call self.dsample() before executing.
To have stochastic results, call self.rsample() before executing.
mode in ['multiply', 'lookup']</p>
<p>output shape: (num_seeds, batch_size, out_features).</p>

        <details class="quote">
          <summary>Source code in <code>bemb/model/bayesian_linear.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s1">&#39;multiply&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward with weight sampling. Forward does out = XW + b, for forward() method behaves like the embedding layer</span>
<span class="sd">    in PyTorch, use the lookup() method.</span>
<span class="sd">    To have determinstic results, call self.dsample() before executing.</span>
<span class="sd">    To have stochastic results, call self.rsample() before executing.</span>
<span class="sd">    mode in [&#39;multiply&#39;, &#39;lookup&#39;]</span>

<span class="sd">    output shape: (num_seeds, batch_size, out_features).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;run BayesianLinear.rsample() or dsample() first to sample weight and bias.&#39;</span>

    <span class="c1"># if determinstic, num_seeds is set to 1.</span>
    <span class="c1"># w: (num_seeds, in_features=num_classes, out_features)</span>
    <span class="c1"># b: (num_seeds, out_features)</span>
    <span class="c1"># x: (N, in_features) if multiply and (N,) if lookup.</span>
    <span class="c1"># output: (num_seeds, N, out_features)</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;multiply&#39;</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (num_seeds, N, in_features)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span><span class="p">)</span>  <span class="c1"># (num_seeds, N, out_features)</span>
    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;lookup&#39;</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span><span class="p">[:,</span> <span class="n">x</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># (num_seeds, N, out_features)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;mode=</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s1"> is not allowed.&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_sample</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">)</span>

    <span class="c1"># (num_seeds, N, out_features)</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bayesian_linear.BayesianLinear.log_prior" class="doc doc-heading">
<code class="highlight language-python"><span class="n">log_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>Evaluate the likelihood of the provided samples of parameter under the current prior distribution.</p>

        <details class="quote">
          <summary>Source code in <code>bemb/model/bayesian_linear.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">log_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Evaluate the likelihood of the provided samples of parameter under the current prior distribution.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;run BayesianLinear.rsample() or dsample() first to sample weight and bias.&#39;</span>
    <span class="n">num_seeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">total_log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># log P(W_sample). shape = (num_seeds,)</span>
    <span class="n">W_prior</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">W_prior_mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_prior_logstd</span><span class="p">))</span>
    <span class="n">total_log_prob</span> <span class="o">+=</span> <span class="n">W_prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

    <span class="c1"># log P(b_sample) if applicable.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
        <span class="n">b_prior</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">b_prior_mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_prior_logstd</span><span class="p">))</span>
        <span class="n">total_log_prob</span> <span class="o">+=</span> <span class="n">b_prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_sample</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">total_log_prob</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_seeds</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">total_log_prob</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bayesian_linear.BayesianLinear.log_variational" class="doc doc-heading">
<code class="highlight language-python"><span class="n">log_variational</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>Evaluate the likelihood of the provided samples of parameter under the current variational distribution.</p>

        <details class="quote">
          <summary>Source code in <code>bemb/model/bayesian_linear.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">log_variational</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Evaluate the likelihood of the provided samples of parameter under the current variational distribution.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;run BayesianLinear.rsample() or dsample() first to sample weight and bias.&#39;</span>
    <span class="n">num_seeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">total_log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">total_log_prob</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
        <span class="n">total_log_prob</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_variational_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_sample</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">total_log_prob</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_seeds</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">total_log_prob</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bayesian_linear.BayesianLinear.rsample" class="doc doc-heading">
<code class="highlight language-python"><span class="n">rsample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_seeds</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>sample all parameters using re-parameterization trick.</p>

        <details class="quote">
          <summary>Source code in <code>bemb/model/bayesian_linear.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">rsample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_seeds</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
    <span class="sd">&quot;&quot;&quot;sample all parameters using re-parameterization trick.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span> <span class="o">=</span> <span class="n">num_seeds</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_variational_distribution</span><span class="o">.</span><span class="n">rsample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">num_seeds</span><span class="p">]))</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_variational_distribution</span><span class="o">.</span><span class="n">rsample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">num_seeds</span><span class="p">]))</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_sample</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_sample</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">



<h3 id="bemb.model.bemb" class="doc doc-heading">
        <code>bemb</code>



</h3>

    <div class="doc doc-contents ">

      <p>The core class of the Bayesian EMBedding (BEMB) model.</p>
<p>Author: Tianyu Du
Update: Apr. 28, 2022</p>



  <div class="doc doc-children">








  <div class="doc doc-object doc-class">



<h4 id="bemb.model.bemb.BEMBFlex" class="doc doc-heading">
        <code>
BEMBFlex            (<span title="torch.nn.modules.module.Module">Module</span>)
        </code>



</h4>

    <div class="doc doc-contents ">


        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BEMBFlex</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1"># ==================================================================================================================</span>
    <span class="c1"># core function as a PyTorch module.</span>
    <span class="c1"># ==================================================================================================================</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">utility_formula</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                 <span class="n">obs2prior_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">bool</span><span class="p">],</span>
                 <span class="n">coef_dim_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
                 <span class="n">num_items</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">pred_item</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                 <span class="n">prior_variance</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">num_users</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_sessions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">trace_log_q</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">category_to_item</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="c1"># number of observables.</span>
                 <span class="n">num_user_obs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_item_obs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_session_obs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_price_obs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_taste_obs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="c1"># additional modules.</span>
                 <span class="n">additional_modules</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            utility_formula (str): a string representing the utility function U[user, item, session].</span>
<span class="sd">                See documentation for more details in the documentation for the format of formula.</span>
<span class="sd">                Examples:</span>
<span class="sd">                    lambda_item</span>
<span class="sd">                    lambda_item + theta_user * alpha_item + zeta_user * item_obs</span>
<span class="sd">                    lambda_item + theta_user * alpha_item + gamma_user * beta_item * price_obs</span>
<span class="sd">                See the doc-string of parse_utility for an example.</span>

<span class="sd">            obs2prior_dict (Dict[str, bool]): a dictionary maps coefficient name (e.g., &#39;lambda_item&#39;)</span>
<span class="sd">                to a boolean indicating if observable (e.g., item_obs) enters the prior of the coefficient.</span>

<span class="sd">            coef_dim_dict (Dict[str, int]): a dictionary maps coefficient name (e.g., &#39;lambda_item&#39;)</span>
<span class="sd">                to an integer indicating the dimension of coefficient.</span>
<span class="sd">                For standalone coefficients like U = lambda_item, the dim should be 1.</span>
<span class="sd">                For factorized coefficients like U = theta_user * alpha_item, the dim should be the</span>
<span class="sd">                    latent dimension of theta and alpha.</span>
<span class="sd">                For coefficients multiplied with observables like U = zeta_user * item_obs, the dim</span>
<span class="sd">                    should be the number of observables in item_obs.</span>
<span class="sd">                For factorized coefficient multiplied with observables like U = gamma_user * beta_item * price_obs,</span>
<span class="sd">                    the dim should be the latent dim multiplied by number of observables in price_obs.</span>

<span class="sd">            num_items (int): number of items.</span>

<span class="sd">            pred_item (bool): there are two use cases of this model, suppose we have `user_index[i]` and `item_index[i]`</span>
<span class="sd">                for the i-th observation in the dataset.</span>
<span class="sd">                Case 1: which item among all items user `user_index[i]` is going to purchase, the prediction label</span>
<span class="sd">                    is therefore `item_index[i]`. Equivalently, we can ask what&#39;s the likelihood for user `user_index[i]`</span>
<span class="sd">                    to purchase `item_index[i]`.</span>
<span class="sd">                Case 2: what rating would user `user_index[i]` assign to item `item_index[i]`? In this case, the dataset</span>
<span class="sd">                    object needs to contain a separate label.</span>
<span class="sd">                    NOTE: for now, we only support binary labels.</span>

<span class="sd">            prior_variance (Union[float, Dict[str, float]]): the variance of prior distribution for</span>
<span class="sd">                coefficients. If a float is provided, all priors will be diagonal matrix with</span>
<span class="sd">                prior_variance along the diagonal. If a dictionary is provided, keys of prior_variance</span>
<span class="sd">                should be coefficient names, and the variance of prior of coef_name would be a diagonal</span>
<span class="sd">                matrix with prior_variance[coef_name] along the diagonal.</span>
<span class="sd">                Defaults to 1.0, which means all prior have identity matrix as the covariance matrix.</span>

<span class="sd">            num_users (int, optional): number of users, required only if coefficient or observable</span>
<span class="sd">                depending on user is in utility. Defaults to None.</span>
<span class="sd">            num_sessions (int, optional): number of sessions, required only if coefficient or</span>
<span class="sd">                observable depending on session is in utility. Defaults to None.</span>

<span class="sd">            trace_log_q (bool, optional): whether to trace the derivative of variational likelihood logQ</span>
<span class="sd">                with respect to variational parameters in the ELBO while conducting gradient update.</span>
<span class="sd">                Defaults to False.</span>

<span class="sd">            category_to_item (Dict[str, List[int]], optional): a dictionary with category id or name</span>
<span class="sd">                as keys, and category_to_item[C] contains the list of item ids belonging to category C.</span>
<span class="sd">                If None is provided, all items are assumed to be in the same category.</span>
<span class="sd">                Defaults to None.</span>

<span class="sd">            num_{user, item, session, price, taste}_obs (int, optional): number of observables of</span>
<span class="sd">                each type of features, only required if observable enters prior.</span>
<span class="sd">                NOTE: currently we only allow coefficient to depend on either user or item, thus only</span>
<span class="sd">                user and item observables can enter the prior of coefficient. Hence session, price,</span>
<span class="sd">                and taste observables are never required, we include it here for completeness.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BEMBFlex</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">utility_formula</span> <span class="o">=</span> <span class="n">utility_formula</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">obs2prior_dict</span> <span class="o">=</span> <span class="n">obs2prior_dict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_dim_dict</span> <span class="o">=</span> <span class="n">coef_dim_dict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior_variance</span> <span class="o">=</span> <span class="n">prior_variance</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pred_item</span> <span class="o">=</span> <span class="n">pred_item</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_items</span> <span class="o">=</span> <span class="n">num_items</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_users</span> <span class="o">=</span> <span class="n">num_users</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_sessions</span> <span class="o">=</span> <span class="n">num_sessions</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">trace_log_q</span> <span class="o">=</span> <span class="n">trace_log_q</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">category_to_item</span> <span class="o">=</span> <span class="n">category_to_item</span>

        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># Category ID to Item ID mapping.</span>
        <span class="c1"># Category ID to Category Size mapping.</span>
        <span class="c1"># Item ID to Category ID mapping.</span>
        <span class="c1"># ==============================================================================================================</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">category_to_item</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_item</span><span class="p">:</span>
                <span class="c1"># assign all items to the same category if predicting items.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">category_to_item</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_items</span><span class="p">))}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># otherwise, for the j-th observation in the dataset, the label[j]</span>
                <span class="c1"># only depends on user_index[j] and item_index[j], so we put each</span>
                <span class="c1"># item to its own category.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">category_to_item</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_items</span><span class="p">)}</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">category_to_item</span><span class="p">)</span>

        <span class="n">max_category_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">category_to_item</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="n">category_to_item_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span><span class="p">,</span> <span class="n">max_category_size</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">category_to_size_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">item_in_c</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">category_to_item</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">category_to_item_tensor</span><span class="p">[</span><span class="n">c</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span>
                <span class="n">item_in_c</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">item_in_c</span><span class="p">)</span>
            <span class="n">category_to_size_tensor</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">scalar_tensor</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">item_in_c</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;category_to_item_tensor&#39;</span><span class="p">,</span>
                             <span class="n">category_to_item_tensor</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;category_to_size_tensor&#39;</span><span class="p">,</span>
                             <span class="n">category_to_size_tensor</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>

        <span class="n">item_to_category_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_items</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">items_in_c</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">category_to_item</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">item_to_category_tensor</span><span class="p">[</span><span class="n">items_in_c</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;item_to_category_tensor&#39;</span><span class="p">,</span>
                             <span class="n">item_to_category_tensor</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>

        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># Create Bayesian Coefficient Objects</span>
        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># model configuration.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">formula</span> <span class="o">=</span> <span class="n">parse_utility</span><span class="p">(</span><span class="n">utility_formula</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;BEMB: utility formula parsed:&#39;</span><span class="p">)</span>
        <span class="n">pprint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">formula</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">raw_formula</span> <span class="o">=</span> <span class="n">utility_formula</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">obs2prior_dict</span> <span class="o">=</span> <span class="n">obs2prior_dict</span>

        <span class="c1"># dimension of each observable, this one is used only for obs2prior.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_obs_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;user&#39;</span><span class="p">:</span> <span class="n">num_user_obs</span><span class="p">,</span>
            <span class="s1">&#39;item&#39;</span><span class="p">:</span> <span class="n">num_item_obs</span><span class="p">,</span>
            <span class="s1">&#39;session&#39;</span><span class="p">:</span> <span class="n">num_session_obs</span><span class="p">,</span>
            <span class="s1">&#39;price&#39;</span><span class="p">:</span> <span class="n">num_price_obs</span><span class="p">,</span>
            <span class="s1">&#39;taste&#39;</span><span class="p">:</span> <span class="n">num_taste_obs</span><span class="p">,</span>
            <span class="s1">&#39;constant&#39;</span><span class="p">:</span> <span class="mi">1</span>  <span class="c1"># not really used, for dummy variables.</span>
        <span class="p">}</span>

        <span class="c1"># how many classes for the variational distribution.</span>
        <span class="c1"># for example, beta_item would be `num_items` 10-dimensional gaussian if latent dim = 10.</span>
        <span class="n">variation_to_num_classes</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;user&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_users</span><span class="p">,</span>
            <span class="s1">&#39;item&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_items</span><span class="p">,</span>
            <span class="s1">&#39;constant&#39;</span><span class="p">:</span> <span class="mi">1</span>
        <span class="p">}</span>

        <span class="n">coef_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">additive_term</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">formula</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">coef_name</span> <span class="ow">in</span> <span class="n">additive_term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">]:</span>
                <span class="n">variation</span> <span class="o">=</span> <span class="n">coef_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">s2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_variance</span><span class="p">[</span><span class="n">coef_name</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prior_variance</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_variance</span>
                <span class="n">coef_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">BayesianCoefficient</span><span class="p">(</span><span class="n">variation</span><span class="o">=</span><span class="n">variation</span><span class="p">,</span>
                                                           <span class="n">num_classes</span><span class="o">=</span><span class="n">variation_to_num_classes</span><span class="p">[</span><span class="n">variation</span><span class="p">],</span>
                                                           <span class="n">obs2prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">obs2prior_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">],</span>
                                                           <span class="n">num_obs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_obs_dict</span><span class="p">[</span><span class="n">variation</span><span class="p">],</span>
                                                           <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_dim_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">],</span>
                                                           <span class="n">prior_variance</span><span class="o">=</span><span class="n">s2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_dict</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">(</span><span class="n">coef_dict</span><span class="p">)</span>

        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># Optional: register additional modules.</span>
        <span class="c1"># ==============================================================================================================</span>
        <span class="k">if</span> <span class="n">additional_modules</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">additional_modules</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s1">&#39;Additional modules are temporarily disabled for further development.&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">additional_modules</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">additional_modules</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;Bayesian EMBedding Model with U[user, item, session] = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">raw_formula</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span> \
               <span class="o">+</span> <span class="sa">f</span><span class="s1">&#39;Total number of parameters: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_params</span><span class="si">}</span><span class="s1">.</span><span class="se">\n</span><span class="s1">&#39;</span> \
               <span class="o">+</span> <span class="s1">&#39;With the following coefficients:</span><span class="se">\n</span><span class="s1">&#39;</span> \
               <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_dict</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span> \
               <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">additional_modules</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">ChoiceDataset</span><span class="p">,</span>
                <span class="n">return_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                <span class="n">return_scope</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                <span class="n">sample_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">num_seeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;A combined method for inference with the model.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch (ChoiceDataset): batch data containing choice information.</span>
<span class="sd">            return_type (str): either &#39;log_prob&#39; or &#39;utility&#39;.</span>
<span class="sd">                &#39;log_prob&#39;: return the log-probability (by within-category log-softmax) for items</span>
<span class="sd">                &#39;utility&#39;: return the utility value of items.</span>
<span class="sd">            return_scope (str): either &#39;item_index&#39; or &#39;all_items&#39;.</span>
<span class="sd">                &#39;item_index&#39;: for each observation i, return log-prob/utility for the chosen item batch.item_index[i] only.</span>
<span class="sd">                &#39;all_items&#39;: for each observation i, return log-prob/utility for all items.</span>
<span class="sd">            deterministic (bool, optional):</span>
<span class="sd">                True: expectations of parameter variational distributions are used for inference.</span>
<span class="sd">                False: the user needs to supply a dictionary of sampled parameters for inference.</span>
<span class="sd">                Defaults to True.</span>
<span class="sd">            sample_dict (Optional[Dict[str, torch.Tensor]], optional): sampled parameters for inference task.</span>
<span class="sd">                This is not needed when `deterministic` is True.</span>
<span class="sd">                When `deterministic` is False, the user can supply a `sample_dict`. If `sample_dict` is not provided,</span>
<span class="sd">                this method will create `num_seeds` samples.</span>
<span class="sd">                Defaults to None.</span>
<span class="sd">            num_seeds (Optional[int]): the number of random samples of parameters to construct. This is only required</span>
<span class="sd">                if `deterministic` is False (i.e., stochastic mode) and `sample_dict` is not provided.</span>
<span class="sd">                Defaults to None.</span>
<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: a tensor of log-probabilities or utilities, depending on `return_type`.</span>
<span class="sd">                The shape of the returned tensor depends on `return_scope` and `deterministic`.</span>
<span class="sd">                -------------------------------------------------------------------------</span>
<span class="sd">                | `return_scope` | `deterministic` |         Output shape               |</span>
<span class="sd">                -------------------------------------------------------------------------</span>
<span class="sd">                |   &#39;item_index` |      True       | (len(batch),)                      |</span>
<span class="sd">                -------------------------------------------------------------------------</span>
<span class="sd">                |   &#39;all_items&#39;  |      True       | (len(batch), num_items)            |</span>
<span class="sd">                -------------------------------------------------------------------------</span>
<span class="sd">                |   &#39;item_index&#39; |      False      | (num_seeds, len(batch))            |</span>
<span class="sd">                -------------------------------------------------------------------------</span>
<span class="sd">                |   &#39;all_items&#39;  |      False      | (num_seeds, len(batch), num_items) |</span>
<span class="sd">                -------------------------------------------------------------------------</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># check arguments.</span>
        <span class="c1"># ==============================================================================================================</span>
        <span class="k">assert</span> <span class="n">return_type</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="s1">&#39;log_prob&#39;</span><span class="p">,</span> <span class="s1">&#39;utility&#39;</span><span class="p">],</span> <span class="s2">&quot;return_type must be either &#39;log_prob&#39; or &#39;utility&#39;.&quot;</span>
        <span class="k">assert</span> <span class="n">return_scope</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="s1">&#39;item_index&#39;</span><span class="p">,</span> <span class="s1">&#39;all_items&#39;</span><span class="p">],</span> <span class="s2">&quot;return_scope must be either &#39;item_index&#39; or &#39;all_items&#39;.&quot;</span>
        <span class="k">assert</span> <span class="n">deterministic</span> <span class="ow">in</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">deterministic</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">sample_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">num_seeds</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;A positive interger `num_seeds` is required if `deterministic` is False and no `sample_dict` is provided.&quot;</span>

        <span class="c1"># when pred_item is true, the model is predicting which item is bought (specified by item_index).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_item</span><span class="p">:</span>
            <span class="n">batch</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_index</span>

        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># get sample_dict ready.</span>
        <span class="c1"># ==============================================================================================================</span>
        <span class="k">if</span> <span class="n">deterministic</span><span class="p">:</span>
            <span class="n">num_seeds</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="c1"># Use the means of variational distributions as the sole deterministic MC sample.</span>
            <span class="c1"># NOTE: here we don&#39;t need to sample the obs2prior weight H since we only compute the log-likelihood.</span>
            <span class="c1"># TODO: is this correct?</span>
            <span class="n">sample_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">coef_name</span><span class="p">,</span> <span class="n">coef</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">coef</span><span class="o">.</span><span class="n">variational_distribution</span><span class="o">.</span><span class="n">mean</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
                    <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (1, num_*, dim)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">sample_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># sample stochastic parameters.</span>
                <span class="n">sample_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_coefficient_dictionary</span><span class="p">(</span><span class="n">num_seeds</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># use the provided sample_dict.</span>
                <span class="n">num_seeds</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">sample_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># call the sampling method of additional modules.</span>
        <span class="c1"># ==============================================================================================================</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_modules</span><span class="p">:</span>
            <span class="c1"># deterministic sample.</span>
            <span class="k">if</span> <span class="n">deterministic</span><span class="p">:</span>
                <span class="n">module</span><span class="o">.</span><span class="n">dsample</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">module</span><span class="o">.</span><span class="n">rsample</span><span class="p">(</span><span class="n">num_seeds</span><span class="o">=</span><span class="n">num_seeds</span><span class="p">)</span>

        <span class="c1"># if utility is requested, don&#39;t run log-softmax, simply return logit.</span>
        <span class="n">return_logit</span> <span class="o">=</span> <span class="p">(</span><span class="n">return_type</span> <span class="o">==</span> <span class="s1">&#39;utility&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_scope</span> <span class="o">==</span> <span class="s1">&#39;all_items&#39;</span><span class="p">:</span>
            <span class="c1"># (num_seeds, len(batch), num_items)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_likelihood_all_items</span><span class="p">(</span>
                <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span> <span class="n">sample_dict</span><span class="o">=</span><span class="n">sample_dict</span><span class="p">,</span> <span class="n">return_logit</span><span class="o">=</span><span class="n">return_logit</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">return_scope</span> <span class="o">==</span> <span class="s1">&#39;item_index&#39;</span><span class="p">:</span>
            <span class="c1"># (num_seeds, len(batch))</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_likelihood_item_index</span><span class="p">(</span>
                <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span> <span class="n">sample_dict</span><span class="o">=</span><span class="n">sample_dict</span><span class="p">,</span> <span class="n">return_logit</span><span class="o">=</span><span class="n">return_logit</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">deterministic</span><span class="p">:</span>
            <span class="c1"># drop the first dimension, which has size of `num_seeds` (equals 1 in the deterministic case).</span>
            <span class="c1"># (len(batch), num_items) or (len(batch),)</span>
            <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">num_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()])</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">coef</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_dict</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">coef</span><span class="o">.</span><span class="n">device</span>

    <span class="c1"># ==================================================================================================================</span>
    <span class="c1"># helper functions.</span>
    <span class="c1"># ==================================================================================================================</span>
    <span class="k">def</span> <span class="nf">sample_coefficient_dictionary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_seeds</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;A helper function to sample parameters from coefficients.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_seeds (int): number of random samples.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, torch.Tensor]: a dictionary maps coefficient names to tensor of sampled coefficient parameters,</span>
<span class="sd">                where the first dimension of the sampled tensor has size `num_seeds`.</span>
<span class="sd">                Each sample tensor has shape (num_seeds, num_classes, dim).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sample_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">coef_name</span><span class="p">,</span> <span class="n">coef</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">coef</span><span class="o">.</span><span class="n">rsample</span><span class="p">(</span><span class="n">num_seeds</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">coef</span><span class="o">.</span><span class="n">obs2prior</span><span class="p">:</span>
                <span class="c1"># sample both obs2prior weight and realization of variable.</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span> <span class="o">+</span> <span class="s1">&#39;.H&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># only sample the realization of variable.</span>
                <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>
        <span class="k">return</span> <span class="n">sample_dict</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">get_within_category_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_p_all_items</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">label</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;A helper function for computing prediction accuracy (i.e., all non-differential metrics)</span>
<span class="sd">        within category.</span>
<span class="sd">        In particular, this method calculates the accuracy, precision, recall and F1 score.</span>


<span class="sd">        This method has the same functionality as the following peusodcode:</span>
<span class="sd">        for C in categories:</span>
<span class="sd">            # get sessions in which item in category C was purchased.</span>
<span class="sd">            T &lt;- (t for t in {0,1,..., len(label)-1} if label[t] is in C)</span>
<span class="sd">            Y &lt;- label[T]</span>

<span class="sd">            predictions = list()</span>
<span class="sd">            for t in T:</span>
<span class="sd">                # get the prediction within category for this session.</span>
<span class="sd">                y_pred = argmax_{items in C} log prob computed before.</span>
<span class="sd">                predictions.append(y_pred)</span>

<span class="sd">            accuracy = mean(Y == predictions)</span>

<span class="sd">        Similarly, this function computes precision, recall and f1score as well.</span>

<span class="sd">        Args:</span>
<span class="sd">            log_p_all_items (torch.Tensor): shape (num_sessions, num_items) the log probability of</span>
<span class="sd">                choosing each item in each session.</span>
<span class="sd">            label (torch.LongTensor): shape (num_sessions,), the IDs of items purchased in each session.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [Dict[str, float]]: A dictionary containing performance metrics.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># argmax: (num_sessions, num_categories), within category argmax.</span>
        <span class="c1"># item IDs are consecutive, thus argmax is the same as IDs of the item with highest P.</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">argmax_by_category</span> <span class="o">=</span> <span class="n">scatter_max</span><span class="p">(</span>
            <span class="n">log_p_all_items</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">item_to_category_tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># category_purchased[t] = the category of item label[t].</span>
        <span class="c1"># (num_sessions,)</span>
        <span class="n">category_purchased</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">item_to_category_tensor</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>

        <span class="c1"># pred[t] = the item with highest utility from the category item label[t] belongs to.</span>
        <span class="c1"># (num_sessions,)</span>
        <span class="n">pred_from_category</span> <span class="o">=</span> <span class="n">argmax_by_category</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">)),</span> <span class="n">category_purchased</span><span class="p">]</span>

        <span class="n">within_category_accuracy</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">pred_from_category</span> <span class="o">==</span> <span class="n">label</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># precision</span>
        <span class="n">precision</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

        <span class="n">recall</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_items</span><span class="p">):</span>
            <span class="n">correct_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">pred_from_category</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="o">==</span> <span class="n">i</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
            <span class="n">precision_i</span> <span class="o">=</span> <span class="n">correct_i</span> <span class="o">/</span> \
                <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">pred_from_category</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
            <span class="n">recall_i</span> <span class="o">=</span> <span class="n">correct_i</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">label</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>

            <span class="c1"># do not add if divided by zero.</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">pred_from_category</span> <span class="o">==</span> <span class="n">i</span><span class="p">):</span>
                <span class="n">precision</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">precision_i</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">label</span> <span class="o">==</span> <span class="n">i</span><span class="p">):</span>
                <span class="n">recall</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">recall_i</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="n">precision</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">precision</span><span class="p">))</span>
        <span class="n">recall</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">recall</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">precision</span> <span class="o">==</span> <span class="n">recall</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">f1</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">f1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="n">within_category_accuracy</span><span class="p">,</span>
                <span class="s1">&#39;precision&#39;</span><span class="p">:</span> <span class="n">precision</span><span class="p">,</span>
                <span class="s1">&#39;recall&#39;</span><span class="p">:</span> <span class="n">recall</span><span class="p">,</span>
                <span class="s1">&#39;f1score&#39;</span><span class="p">:</span> <span class="n">f1</span><span class="p">}</span>

    <span class="c1"># ==================================================================================================================</span>
    <span class="c1"># Methods for terms in the ELBO: prior, likelihood, and variational.</span>
    <span class="c1"># ==================================================================================================================</span>
    <span class="k">def</span> <span class="nf">log_likelihood_all_items</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">ChoiceDataset</span><span class="p">,</span> <span class="n">return_logit</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">sample_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        NOTE to developers:</span>
<span class="sd">        This method computes utilities for all items available, which is a relatively slow operation. For</span>
<span class="sd">        training the model, you only need the utility/log-prob for the chosen/relevant item (i.e., item_index[i] for each i-th observation).</span>
<span class="sd">        Use this method for inference only.</span>
<span class="sd">        Use self.log_likelihood_item_index() for training instead.</span>

<span class="sd">        Computes the log probability of choosing `each` item in each session based on current model parameters.</span>
<span class="sd">        This method allows for specifying {user, item}_latent_value for Monte Carlo estimation in ELBO.</span>
<span class="sd">        For actual prediction tasks, use the forward() function, which will use means of variational</span>
<span class="sd">        distributions for user and item latents.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch (ChoiceDataset): a ChoiceDataset object containing relevant information.</span>
<span class="sd">            return_logit(bool): if set to True, return the log-probability, otherwise return the logit/utility.</span>
<span class="sd">            sample_dict(Dict[str, torch.Tensor]): Monte Carlo samples for model coefficients</span>
<span class="sd">                (i.e., those Greek letters).</span>
<span class="sd">                sample_dict.keys() should be the same as keys of self.obs2prior_dict, i.e., those</span>
<span class="sd">                greek letters actually enter the functional form of utility.</span>
<span class="sd">                The value of sample_dict should be tensors of shape (num_seeds, num_classes, dim)</span>
<span class="sd">                where num_classes in {num_users, num_items, 1}</span>
<span class="sd">                and dim in {latent_dim(K), num_item_obs, num_user_obs, 1}.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: a tensor of shape (num_seeds, len(batch), self.num_items), where</span>
<span class="sd">                out[x, y, z] is the probability of choosing item z in session y conditioned on</span>
<span class="sd">                latents to be the x-th Monte Carlo sample.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_seeds</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">sample_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># avoid repeated work when user purchased several items in the same session.</span>
        <span class="n">user_session_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
            <span class="p">[</span><span class="n">batch</span><span class="o">.</span><span class="n">user_index</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">session_index</span><span class="p">])</span>
        <span class="k">assert</span> <span class="n">user_session_index</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
        <span class="n">unique_user_sess</span><span class="p">,</span> <span class="n">inverse_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span>
            <span class="n">user_session_index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">user_index</span> <span class="o">=</span> <span class="n">unique_user_sess</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">session_index</span> <span class="o">=</span> <span class="n">unique_user_sess</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">user_index</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">session_index</span><span class="p">)</span>

        <span class="c1"># short-hands for easier shape check.</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">num_seeds</span>
        <span class="c1"># P = len(batch)  # num_purchases.</span>
        <span class="n">P</span> <span class="o">=</span> <span class="n">unique_user_sess</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">S</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_sessions</span>
        <span class="n">U</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_users</span>
        <span class="n">I</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_items</span>

        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># Helper Functions for Reshaping.</span>
        <span class="c1"># ==============================================================================================================</span>
        <span class="k">def</span> <span class="nf">reshape_user_coef_sample</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
            <span class="c1"># input shape (R, U, *)</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (R, U, I, *)</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">user_index</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
            <span class="k">assert</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">C</span>

        <span class="k">def</span> <span class="nf">reshape_item_coef_sample</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
            <span class="c1"># input shape (R, I, *)</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">C</span>

        <span class="k">def</span> <span class="nf">reshape_constant_coef_sample</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
            <span class="c1"># input shape (R, *)</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">C</span>

        <span class="k">def</span> <span class="nf">reshape_coef_sample</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
            <span class="c1"># reshape the monte carlo sample of coefficients to (R, P, I, *).</span>
            <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_user&#39;</span><span class="p">):</span>
                <span class="c1"># (R, U, *) --&gt; (R, P, I, *)</span>
                <span class="k">return</span> <span class="n">reshape_user_coef_sample</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_item&#39;</span><span class="p">):</span>
                <span class="c1"># (R, I, *) --&gt; (R, P, I, *)</span>
                <span class="k">return</span> <span class="n">reshape_item_coef_sample</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_constant&#39;</span><span class="p">):</span>
                <span class="c1"># (R, *) --&gt; (R, P, I, *)</span>
                <span class="k">return</span> <span class="n">reshape_constant_coef_sample</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span>

        <span class="k">def</span> <span class="nf">reshape_observable</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
            <span class="c1"># reshape observable to (R, P, I, *) so that it can be multiplied with monte carlo</span>
            <span class="c1"># samples of coefficients.</span>
            <span class="n">O</span> <span class="o">=</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of observables.</span>
            <span class="k">assert</span> <span class="n">O</span> <span class="o">==</span> <span class="n">positive_integer</span>
            <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;item_&#39;</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;user_&#39;</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">user_index</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># (P, O)</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;session_&#39;</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">session_index</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># (P, O)</span>
                <span class="k">return</span> <span class="n">obs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;price_&#39;</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">session_index</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># (P, I, O)</span>
                <span class="k">return</span> <span class="n">obs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;taste_&#39;</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">user_index</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># (P, I, O)</span>
                <span class="k">return</span> <span class="n">obs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span>
            <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">obs</span>

        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># Copmute the Utility Term by Term.</span>
        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># P is the number of unique (user, session) pairs.</span>
        <span class="c1"># (random_seeds, P, num_items).</span>
        <span class="n">utility</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># loop over additive term to utility</span>
        <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">formula</span><span class="p">:</span>
            <span class="c1"># Type I: single coefficient, e.g., lambda_item or lambda_user.</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># E.g., lambda_item or lambda_user</span>
                <span class="n">coef_name</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">coef_sample</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                    <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">],</span> <span class="n">coef_name</span><span class="p">)</span>
                <span class="k">assert</span> <span class="n">coef_sample</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">additive_term</span> <span class="o">=</span> <span class="n">coef_sample</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">)</span>

            <span class="c1"># Type II: factorized coefficient, e.g., &lt;theta_user, lambda_item&gt;.</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">coef_name_0</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">coef_name_1</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

                <span class="n">coef_sample_0</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                    <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name_0</span><span class="p">],</span> <span class="n">coef_name_0</span><span class="p">)</span>
                <span class="n">coef_sample_1</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                    <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name_1</span><span class="p">],</span> <span class="n">coef_name_1</span><span class="p">)</span>

                <span class="k">assert</span> <span class="n">coef_sample_0</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">coef_sample_1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span>
                    <span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>

                <span class="n">additive_term</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef_sample_0</span> <span class="o">*</span> <span class="n">coef_sample_1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Type III: single coefficient multiplied by observable, e.g., theta_user * x_obs_item.</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">coef_name</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">coef_sample</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                    <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">],</span> <span class="n">coef_name</span><span class="p">)</span>
                <span class="k">assert</span> <span class="n">coef_sample</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>

                <span class="n">obs_name</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">reshape_observable</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">obs_name</span><span class="p">),</span> <span class="n">obs_name</span><span class="p">)</span>
                <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>

                <span class="n">additive_term</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef_sample</span> <span class="o">*</span> <span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Type IV: factorized coefficient multiplied by observable.</span>
            <span class="c1"># e.g., gamma_user * beta_item * price_obs.</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">coef_name_0</span><span class="p">,</span> <span class="n">coef_name_1</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

                <span class="n">coef_sample_0</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                    <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name_0</span><span class="p">],</span> <span class="n">coef_name_0</span><span class="p">)</span>
                <span class="n">coef_sample_1</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                    <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name_1</span><span class="p">],</span> <span class="n">coef_name_1</span><span class="p">)</span>
                <span class="k">assert</span> <span class="n">coef_sample_0</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">coef_sample_1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span>
                    <span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>
                <span class="n">num_obs_times_latent_dim</span> <span class="o">=</span> <span class="n">coef_sample_0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

                <span class="n">obs_name</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">reshape_observable</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">obs_name</span><span class="p">),</span> <span class="n">obs_name</span><span class="p">)</span>
                <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>
                <span class="n">num_obs</span> <span class="o">=</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of observables.</span>

                <span class="k">assert</span> <span class="p">(</span><span class="n">num_obs_times_latent_dim</span> <span class="o">%</span> <span class="n">num_obs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="n">latent_dim</span> <span class="o">=</span> <span class="n">num_obs_times_latent_dim</span> <span class="o">//</span> <span class="n">num_obs</span>

                <span class="n">coef_sample_0</span> <span class="o">=</span> <span class="n">coef_sample_0</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                    <span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">num_obs</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
                <span class="n">coef_sample_1</span> <span class="o">=</span> <span class="n">coef_sample_1</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                    <span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">num_obs</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
                <span class="c1"># compute the factorized coefficient with shape (R, P, I, O).</span>
                <span class="n">coef</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef_sample_0</span> <span class="o">*</span> <span class="n">coef_sample_1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

                <span class="n">additive_term</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef</span> <span class="o">*</span> <span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Undefined term type: </span><span class="si">{</span><span class="n">term</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

            <span class="k">assert</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">)</span>
            <span class="n">utility</span> <span class="o">+=</span> <span class="n">additive_term</span>

        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># Mask Out Unavailable Items in Each Session.</span>
        <span class="c1"># ==============================================================================================================</span>

        <span class="k">if</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_availability</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># expand to the Monte Carlo sample dimension.</span>
            <span class="c1"># (S, I) -&gt; (P, I) -&gt; (1, P, I) -&gt; (R, P, I)</span>
            <span class="n">A</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_availability</span><span class="p">[</span><span class="n">session_index</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
                <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">utility</span><span class="p">[</span><span class="o">~</span><span class="n">A</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">utility</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">utility</span> <span class="o">=</span> <span class="n">utility</span><span class="p">[:,</span> <span class="n">inverse_indices</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">assert</span> <span class="n">utility</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="n">I</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_modules</span><span class="p">:</span>
            <span class="n">additive_term</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">utility</span> <span class="o">+=</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">I</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_logit</span><span class="p">:</span>
            <span class="c1"># output shape: (num_seeds, len(batch), num_items)</span>
            <span class="k">return</span> <span class="n">utility</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># compute log likelihood log p(choosing item i | user, item latents)</span>
            <span class="c1"># compute log softmax separately within each category.</span>
            <span class="n">log_p</span> <span class="o">=</span> <span class="n">scatter_log_softmax</span><span class="p">(</span>
                <span class="n">utility</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">item_to_category_tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># output shape: (num_seeds, len(batch), num_items)</span>
            <span class="k">return</span> <span class="n">log_p</span>

    <span class="k">def</span> <span class="nf">log_likelihood_item_index</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">ChoiceDataset</span><span class="p">,</span> <span class="n">return_logit</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">sample_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        NOTE for developers:</span>
<span class="sd">        This method is more efficient and only computes log-likelihood/logit(utility) for item in item_index[i] for each</span>
<span class="sd">        i-th observation.</span>
<span class="sd">        Developers should use use `log_likelihood_all_items` for inference purpose and to computes log-likelihoods/utilities</span>
<span class="sd">        for ALL items for the i-th observation.</span>

<span class="sd">        Computes the log probability of choosing item_index[i] in each session based on current model parameters.</span>
<span class="sd">        This method allows for specifying {user, item}_latent_value for Monte Carlo estimation in ELBO.</span>
<span class="sd">        For actual prediction tasks, use the forward() function, which will use means of variational</span>
<span class="sd">        distributions for user and item latents.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch (ChoiceDataset): a ChoiceDataset object containing relevant information.</span>
<span class="sd">            return_logit(bool): if set to True, return the log-probability, otherwise return the logit/utility.</span>
<span class="sd">            sample_dict(Dict[str, torch.Tensor]): Monte Carlo samples for model coefficients</span>
<span class="sd">                (i.e., those Greek letters).</span>
<span class="sd">                sample_dict.keys() should be the same as keys of self.obs2prior_dict, i.e., those</span>
<span class="sd">                greek letters actually enter the functional form of utility.</span>
<span class="sd">                The value of sample_dict should be tensors of shape (num_seeds, num_classes, dim)</span>
<span class="sd">                where num_classes in {num_users, num_items, 1}</span>
<span class="sd">                and dim in {latent_dim(K), num_item_obs, num_user_obs, 1}.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: a tensor of shape (num_seeds, len(batch)), where</span>
<span class="sd">                out[x, y] is the probabilities of choosing item batch.item[y] in session y</span>
<span class="sd">                conditioned on latents to be the x-th Monte Carlo sample.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_seeds</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">sample_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># get category id of the item bought in each row of batch.</span>
        <span class="n">cate_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">item_to_category_tensor</span><span class="p">[</span><span class="n">batch</span><span class="o">.</span><span class="n">item_index</span><span class="p">]</span>

        <span class="c1"># get item ids of all items from the same category of each item bought.</span>
        <span class="n">relevant_item_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">category_to_item_tensor</span><span class="p">[</span><span class="n">cate_index</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">relevant_item_index</span> <span class="o">=</span> <span class="n">relevant_item_index</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>
        <span class="c1"># index were padded with -1&#39;s, drop those dummy entries.</span>
        <span class="n">relevant_item_index</span> <span class="o">=</span> <span class="n">relevant_item_index</span><span class="p">[</span><span class="n">relevant_item_index</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># the first repeats[0] entries in relevant_item_index are for the category of item_index[0]</span>
        <span class="n">repeats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">category_to_size_tensor</span><span class="p">[</span><span class="n">cate_index</span><span class="p">]</span>
        <span class="c1"># argwhere(reverse_indices == k) are positions in relevant_item_index for the category of item_index[k].</span>
        <span class="n">reverse_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">repeats</span><span class="p">)</span>
        <span class="c1"># expand the user_index and session_index.</span>
        <span class="n">user_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">user_index</span><span class="p">,</span> <span class="n">repeats</span><span class="p">)</span>
        <span class="n">session_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">session_index</span><span class="p">,</span> <span class="n">repeats</span><span class="p">)</span>
        <span class="c1"># duplicate the item focused to match.</span>
        <span class="n">item_index_expanded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span>
            <span class="n">batch</span><span class="o">.</span><span class="n">item_index</span><span class="p">,</span> <span class="n">repeats</span><span class="p">)</span>

        <span class="c1"># short-hands for easier shape check.</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">num_seeds</span>
        <span class="c1"># total number of relevant items.</span>
        <span class="n">total_computation</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">session_index</span><span class="p">)</span>
        <span class="n">S</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_sessions</span>
        <span class="n">U</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_users</span>
        <span class="n">I</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_items</span>
        <span class="c1"># ==========================================================================================</span>
        <span class="c1"># Helper Functions for Reshaping.</span>
        <span class="c1"># ==========================================================================================</span>

        <span class="k">def</span> <span class="nf">reshape_coef_sample</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
            <span class="c1"># reshape the monte carlo sample of coefficients to (R, P, I, *).</span>
            <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_user&#39;</span><span class="p">):</span>
                <span class="c1"># (R, U, *) --&gt; (R, total_computation, *)</span>
                <span class="k">return</span> <span class="n">sample</span><span class="p">[:,</span> <span class="n">user_index</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_item&#39;</span><span class="p">):</span>
                <span class="c1"># (R, I, *) --&gt; (R, total_computation, *)</span>
                <span class="k">return</span> <span class="n">sample</span><span class="p">[:,</span> <span class="n">relevant_item_index</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_constant&#39;</span><span class="p">):</span>
                <span class="c1"># (R, *) --&gt; (R, total_computation, *)</span>
                <span class="k">return</span> <span class="n">sample</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span>

        <span class="k">def</span> <span class="nf">reshape_observable</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
            <span class="c1"># reshape observable to (R, P, I, *) so that it can be multiplied with monte carlo</span>
            <span class="c1"># samples of coefficients.</span>
            <span class="n">O</span> <span class="o">=</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of observables.</span>
            <span class="k">assert</span> <span class="n">O</span> <span class="o">==</span> <span class="n">positive_integer</span>
            <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;item_&#39;</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">relevant_item_index</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;user_&#39;</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">user_index</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;session_&#39;</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">session_index</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;price_&#39;</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">session_index</span><span class="p">,</span> <span class="n">relevant_item_index</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;taste_&#39;</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">user_index</span><span class="p">,</span> <span class="n">relevant_item_index</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span>
            <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">total_computation</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">obs</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># ==========================================================================================</span>
        <span class="c1"># Compute Components related to users and items only.</span>
        <span class="c1"># ==========================================================================================</span>
        <span class="n">utility</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># loop over additive term to utility</span>
        <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">formula</span><span class="p">:</span>
            <span class="c1"># Type I: single coefficient, e.g., lambda_item or lambda_user.</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># E.g., lambda_item or lambda_user</span>
                <span class="n">coef_name</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">coef_sample</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                    <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">],</span> <span class="n">coef_name</span><span class="p">)</span>
                <span class="k">assert</span> <span class="n">coef_sample</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">additive_term</span> <span class="o">=</span> <span class="n">coef_sample</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">)</span>

            <span class="c1"># Type II: factorized coefficient, e.g., &lt;theta_user, lambda_item&gt;.</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">coef_name_0</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">coef_name_1</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

                <span class="n">coef_sample_0</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                    <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name_0</span><span class="p">],</span> <span class="n">coef_name_0</span><span class="p">)</span>
                <span class="n">coef_sample_1</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                    <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name_1</span><span class="p">],</span> <span class="n">coef_name_1</span><span class="p">)</span>

                <span class="k">assert</span> <span class="n">coef_sample_0</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">coef_sample_1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span>
                    <span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>

                <span class="n">additive_term</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef_sample_0</span> <span class="o">*</span> <span class="n">coef_sample_1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Type III: single coefficient multiplied by observable, e.g., theta_user * x_obs_item.</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">coef_name</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">coef_sample</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                    <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">],</span> <span class="n">coef_name</span><span class="p">)</span>
                <span class="k">assert</span> <span class="n">coef_sample</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span>
                    <span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>

                <span class="n">obs_name</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">reshape_observable</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">obs_name</span><span class="p">),</span> <span class="n">obs_name</span><span class="p">)</span>
                <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>

                <span class="n">additive_term</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef_sample</span> <span class="o">*</span> <span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Type IV: factorized coefficient multiplied by observable.</span>
            <span class="c1"># e.g., gamma_user * beta_item * price_obs.</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">coef_name_0</span><span class="p">,</span> <span class="n">coef_name_1</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">coef_sample_0</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                    <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name_0</span><span class="p">],</span> <span class="n">coef_name_0</span><span class="p">)</span>
                <span class="n">coef_sample_1</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                    <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name_1</span><span class="p">],</span> <span class="n">coef_name_1</span><span class="p">)</span>
                <span class="k">assert</span> <span class="n">coef_sample_0</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">coef_sample_1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span>
                    <span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>
                <span class="n">num_obs_times_latent_dim</span> <span class="o">=</span> <span class="n">coef_sample_0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

                <span class="n">obs_name</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">reshape_observable</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">obs_name</span><span class="p">),</span> <span class="n">obs_name</span><span class="p">)</span>
                <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>
                <span class="n">num_obs</span> <span class="o">=</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of observables.</span>

                <span class="k">assert</span> <span class="p">(</span><span class="n">num_obs_times_latent_dim</span> <span class="o">%</span> <span class="n">num_obs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="n">latent_dim</span> <span class="o">=</span> <span class="n">num_obs_times_latent_dim</span> <span class="o">//</span> <span class="n">num_obs</span>

                <span class="n">coef_sample_0</span> <span class="o">=</span> <span class="n">coef_sample_0</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                    <span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="n">num_obs</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
                <span class="n">coef_sample_1</span> <span class="o">=</span> <span class="n">coef_sample_1</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                    <span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="n">num_obs</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
                <span class="c1"># compute the factorized coefficient with shape (R, P, I, O).</span>
                <span class="n">coef</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef_sample_0</span> <span class="o">*</span> <span class="n">coef_sample_1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

                <span class="n">additive_term</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef</span> <span class="o">*</span> <span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Undefined term type: </span><span class="si">{</span><span class="n">term</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

            <span class="k">assert</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">)</span>
            <span class="n">utility</span> <span class="o">+=</span> <span class="n">additive_term</span>

        <span class="c1"># ==========================================================================================</span>
        <span class="c1"># Mask Out Unavailable Items in Each Session.</span>
        <span class="c1"># ==========================================================================================</span>

        <span class="k">if</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_availability</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># expand to the Monte Carlo sample dimension.</span>
            <span class="n">A</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_availability</span><span class="p">[</span><span class="n">session_index</span><span class="p">,</span> <span class="n">relevant_item_index</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
                <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">utility</span><span class="p">[</span><span class="o">~</span><span class="n">A</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">utility</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_modules</span><span class="p">:</span>
            <span class="c1"># current utility shape: (R, total_computation)</span>
            <span class="n">additive_term</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span>
                <span class="n">R</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span> <span class="ow">or</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="mi">1</span><span class="p">):</span>
                <span class="c1"># TODO: need to make this consistent with log_likelihood_all.</span>
                <span class="c1"># be tolerant for some customized module with BayesianLinear that returns (R, len(batch), 1).</span>
                <span class="n">additive_term</span> <span class="o">=</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
            <span class="c1"># expand to total number of computation, query by reverse_indices.</span>
            <span class="c1"># reverse_indices has length total_computation, and reverse_indices[i] correspond to the row-id that this</span>
            <span class="c1"># computation is responsible for.</span>
            <span class="n">additive_term</span> <span class="o">=</span> <span class="n">additive_term</span><span class="p">[:,</span> <span class="n">reverse_indices</span><span class="p">]</span>
            <span class="k">assert</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">)</span>

        <span class="c1"># compute log likelihood log p(choosing item i | user, item latents)</span>
        <span class="k">if</span> <span class="n">return_logit</span><span class="p">:</span>
            <span class="n">log_p</span> <span class="o">=</span> <span class="n">utility</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># compute the log probability from logits/utilities.</span>
            <span class="n">log_p</span> <span class="o">=</span> <span class="n">scatter_log_softmax</span><span class="p">(</span><span class="n">utility</span><span class="p">,</span> <span class="n">reverse_indices</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># select the log-P of the item actually bought.</span>
        <span class="n">log_p</span> <span class="o">=</span> <span class="n">log_p</span><span class="p">[:,</span> <span class="n">item_index_expanded</span> <span class="o">==</span> <span class="n">relevant_item_index</span><span class="p">]</span>
        <span class="c1"># output shape: (num_seeds, num_purchases, num_items)</span>
        <span class="k">return</span> <span class="n">log_p</span>

    <span class="k">def</span> <span class="nf">log_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">ChoiceDataset</span><span class="p">,</span> <span class="n">sample_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Calculates the log-likelihood of Monte Carlo samples of Bayesian coefficients under their</span>
<span class="sd">        prior distribution. This method assume coefficients are statistically independent.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch (ChoiceDataset): a dataset object contains observables for computing the prior distribution</span>
<span class="sd">                if obs2prior is True.</span>
<span class="sd">            sample_dict (Dict[str, torch.Tensor]): a dictionary coefficient names to Monte Carlo samples.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: [description]</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.scalar_tensor: a tensor with shape (num_seeds,) of [ log P_{prior_distribution}(param[i]) ],</span>
<span class="sd">                where param[i] is the i-th Monte Carlo sample.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># assert sample_dict.keys() == self.coef_dict.keys()</span>
        <span class="n">num_seeds</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">sample_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">total</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">coef_name</span><span class="p">,</span> <span class="n">coef</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">obs2prior_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">coef_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_item&#39;</span><span class="p">):</span>
                    <span class="n">x_obs</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_obs</span>
                <span class="k">elif</span> <span class="n">coef_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_user&#39;</span><span class="p">):</span>
                    <span class="n">x_obs</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">user_obs</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s1">&#39;No observable found to support obs2prior for </span><span class="si">{</span><span class="n">coef_name</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>

                <span class="n">total</span> <span class="o">+=</span> <span class="n">coef</span><span class="o">.</span><span class="n">log_prior</span><span class="p">(</span><span class="n">sample</span><span class="o">=</span><span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">],</span>
                                        <span class="n">H_sample</span><span class="o">=</span><span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span> <span class="o">+</span> <span class="s1">&#39;.H&#39;</span><span class="p">],</span>
                                        <span class="n">x_obs</span><span class="o">=</span><span class="n">x_obs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># log_prob outputs (num_seeds, num_{items, users}), sum to (num_seeds).</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">coef</span><span class="o">.</span><span class="n">log_prior</span><span class="p">(</span>
                    <span class="n">sample</span><span class="o">=</span><span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">],</span> <span class="n">H_sample</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">x_obs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_modules</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">module</span><span class="o">.</span><span class="n">log_prior</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">total</span>

    <span class="k">def</span> <span class="nf">log_variational</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Calculate the log-likelihood of samples in sample_dict under the current variational</span>
<span class="sd">        distribution.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_dict (Dict[str, torch.Tensor]):  a dictionary coefficient names to Monte Carlo</span>
<span class="sd">                samples.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: a tensor of shape (num_seeds) of [ log P_{variational_distribution}(param[i]) ],</span>
<span class="sd">                where param[i] is the i-th Monte Carlo sample.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_seeds</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">sample_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">total</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">coef_name</span><span class="p">,</span> <span class="n">coef</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># log_prob outputs (num_seeds, num_{items, users}), sum to (num_seeds).</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">coef</span><span class="o">.</span><span class="n">log_variational</span><span class="p">(</span><span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_modules</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
            <span class="c1"># with shape (num_seeds,)</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">module</span><span class="o">.</span><span class="n">log_variational</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">total</span>

    <span class="k">def</span> <span class="nf">elbo</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">ChoiceDataset</span><span class="p">,</span> <span class="n">num_seeds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;A combined method to computes the current ELBO given a batch, this method is used for training the model.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch (ChoiceDataset): a ChoiceDataset containing necessary information.</span>
<span class="sd">            num_seeds (int, optional): the number of Monte Carlo samples from variational distributions</span>
<span class="sd">                to evaluate the expectation in ELBO.</span>
<span class="sd">                Defaults to 1.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: a scalar tensor of the ELBO estimated from num_seeds Monte Carlo samples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># 1. sample latent variables from their variational distributions.</span>
        <span class="c1"># ==============================================================================================================</span>
        <span class="n">sample_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_coefficient_dictionary</span><span class="p">(</span><span class="n">num_seeds</span><span class="p">)</span>

        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># 2. compute log p(latent) prior.</span>
        <span class="c1"># (num_seeds,) --mean--&gt; scalar.</span>
        <span class="n">elbo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_prior</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">sample_dict</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># ==============================================================================================================</span>

        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># 3. compute the log likelihood log p(obs|latent).</span>
        <span class="c1"># sum over independent purchase decision for individual observations, mean over MC seeds.</span>
        <span class="c1"># the forward() function calls module.rsample(num_seeds) for module in self.additional_modules.</span>
        <span class="c1"># ==============================================================================================================</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_item</span><span class="p">:</span>
            <span class="c1"># the prediction target is item_index.</span>
            <span class="n">elbo</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span>
                                 <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;log_prob&#39;</span><span class="p">,</span>
                                 <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;item_index&#39;</span><span class="p">,</span>
                                 <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                 <span class="n">sample_dict</span><span class="o">=</span><span class="n">sample_dict</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (num_seeds, len(batch)) --&gt; scalar.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># the prediction target is binary.</span>
            <span class="c1"># TODO: update the prediction function.</span>
            <span class="n">utility</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span>
                                   <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;utility&#39;</span><span class="p">,</span>
                                   <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;item_index&#39;</span><span class="p">,</span>
                                   <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                   <span class="n">sample_dict</span><span class="o">=</span><span class="n">sample_dict</span><span class="p">)</span>  <span class="c1"># (num_seeds, len(batch))</span>

            <span class="c1"># compute the log-likelihood for binary label.</span>
            <span class="c1"># (num_seeds, len(batch))</span>
            <span class="n">y_stacked</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">batch</span><span class="o">.</span><span class="n">label</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_seeds</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="k">assert</span> <span class="n">y_stacked</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">utility</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">bce</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
            <span class="c1"># scalar.</span>
            <span class="n">ll</span> <span class="o">=</span> <span class="o">-</span> <span class="n">bce</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">utility</span><span class="p">),</span>
                       <span class="n">y_stacked</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">elbo</span> <span class="o">+=</span> <span class="n">ll</span>

        <span class="c1"># ==============================================================================================================</span>
        <span class="c1"># 4. optionally add log likelihood under variational distributions q(latent).</span>
        <span class="c1"># ==============================================================================================================</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trace_log_q</span><span class="p">:</span>
            <span class="n">elbo</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_variational</span><span class="p">(</span><span class="n">sample_dict</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">elbo</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">











  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bemb.BEMBFlex.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">utility_formula</span><span class="p">,</span> <span class="n">obs2prior_dict</span><span class="p">,</span> <span class="n">coef_dim_dict</span><span class="p">,</span> <span class="n">num_items</span><span class="p">,</span> <span class="n">pred_item</span><span class="p">,</span> <span class="n">prior_variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">num_users</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_sessions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">trace_log_q</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">category_to_item</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_user_obs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_item_obs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_session_obs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_price_obs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_taste_obs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">additional_modules</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h5>

    <div class="doc doc-contents ">


<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>utility_formula</code></td>
        <td><code>str</code></td>
        <td><p>a string representing the utility function U[user, item, session].
See documentation for more details in the documentation for the format of formula.
Examples:
    lambda_item
    lambda_item + theta_user * alpha_item + zeta_user * item_obs
    lambda_item + theta_user * alpha_item + gamma_user * beta_item * price_obs
See the doc-string of parse_utility for an example.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>obs2prior_dict</code></td>
        <td><code>Dict[str, bool]</code></td>
        <td><p>a dictionary maps coefficient name (e.g., 'lambda_item')
to a boolean indicating if observable (e.g., item_obs) enters the prior of the coefficient.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>coef_dim_dict</code></td>
        <td><code>Dict[str, int]</code></td>
        <td><p>a dictionary maps coefficient name (e.g., 'lambda_item')
to an integer indicating the dimension of coefficient.
For standalone coefficients like U = lambda_item, the dim should be 1.
For factorized coefficients like U = theta_user * alpha_item, the dim should be the
    latent dimension of theta and alpha.
For coefficients multiplied with observables like U = zeta_user * item_obs, the dim
    should be the number of observables in item_obs.
For factorized coefficient multiplied with observables like U = gamma_user * beta_item * price_obs,
    the dim should be the latent dim multiplied by number of observables in price_obs.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_items</code></td>
        <td><code>int</code></td>
        <td><p>number of items.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>pred_item</code></td>
        <td><code>bool</code></td>
        <td><p>there are two use cases of this model, suppose we have <code>user_index[i]</code> and <code>item_index[i]</code>
for the i-th observation in the dataset.
Case 1: which item among all items user <code>user_index[i]</code> is going to purchase, the prediction label
    is therefore <code>item_index[i]</code>. Equivalently, we can ask what's the likelihood for user <code>user_index[i]</code>
    to purchase <code>item_index[i]</code>.
Case 2: what rating would user <code>user_index[i]</code> assign to item <code>item_index[i]</code>? In this case, the dataset
    object needs to contain a separate label.
    NOTE: for now, we only support binary labels.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>prior_variance</code></td>
        <td><code>Union[float, Dict[str, float]]</code></td>
        <td><p>the variance of prior distribution for
coefficients. If a float is provided, all priors will be diagonal matrix with
prior_variance along the diagonal. If a dictionary is provided, keys of prior_variance
should be coefficient names, and the variance of prior of coef_name would be a diagonal
matrix with prior_variance[coef_name] along the diagonal.
Defaults to 1.0, which means all prior have identity matrix as the covariance matrix.</p></td>
        <td><code>1.0</code></td>
      </tr>
      <tr>
        <td><code>num_users</code></td>
        <td><code>int</code></td>
        <td><p>number of users, required only if coefficient or observable
depending on user is in utility. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>num_sessions</code></td>
        <td><code>int</code></td>
        <td><p>number of sessions, required only if coefficient or
observable depending on session is in utility. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>trace_log_q</code></td>
        <td><code>bool</code></td>
        <td><p>whether to trace the derivative of variational likelihood logQ
with respect to variational parameters in the ELBO while conducting gradient update.
Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>category_to_item</code></td>
        <td><code>Dict[str, List[int]]</code></td>
        <td><p>a dictionary with category id or name
as keys, and category_to_item[C] contains the list of item ids belonging to category C.
If None is provided, all items are assumed to be in the same category.
Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>num_{user,</code></td>
        <td><code>item, session, price, taste}_obs (int</code></td>
        <td><p>number of observables of
each type of features, only required if observable enters prior.
NOTE: currently we only allow coefficient to depend on either user or item, thus only
user and item observables can enter the prior of coefficient. Hence session, price,
and taste observables are never required, we include it here for completeness.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">utility_formula</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
             <span class="n">obs2prior_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">bool</span><span class="p">],</span>
             <span class="n">coef_dim_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
             <span class="n">num_items</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">pred_item</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
             <span class="n">prior_variance</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
             <span class="n">num_users</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">num_sessions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">trace_log_q</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
             <span class="n">category_to_item</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="c1"># number of observables.</span>
             <span class="n">num_user_obs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">num_item_obs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">num_session_obs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">num_price_obs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">num_taste_obs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="c1"># additional modules.</span>
             <span class="n">additional_modules</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
             <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        utility_formula (str): a string representing the utility function U[user, item, session].</span>
<span class="sd">            See documentation for more details in the documentation for the format of formula.</span>
<span class="sd">            Examples:</span>
<span class="sd">                lambda_item</span>
<span class="sd">                lambda_item + theta_user * alpha_item + zeta_user * item_obs</span>
<span class="sd">                lambda_item + theta_user * alpha_item + gamma_user * beta_item * price_obs</span>
<span class="sd">            See the doc-string of parse_utility for an example.</span>

<span class="sd">        obs2prior_dict (Dict[str, bool]): a dictionary maps coefficient name (e.g., &#39;lambda_item&#39;)</span>
<span class="sd">            to a boolean indicating if observable (e.g., item_obs) enters the prior of the coefficient.</span>

<span class="sd">        coef_dim_dict (Dict[str, int]): a dictionary maps coefficient name (e.g., &#39;lambda_item&#39;)</span>
<span class="sd">            to an integer indicating the dimension of coefficient.</span>
<span class="sd">            For standalone coefficients like U = lambda_item, the dim should be 1.</span>
<span class="sd">            For factorized coefficients like U = theta_user * alpha_item, the dim should be the</span>
<span class="sd">                latent dimension of theta and alpha.</span>
<span class="sd">            For coefficients multiplied with observables like U = zeta_user * item_obs, the dim</span>
<span class="sd">                should be the number of observables in item_obs.</span>
<span class="sd">            For factorized coefficient multiplied with observables like U = gamma_user * beta_item * price_obs,</span>
<span class="sd">                the dim should be the latent dim multiplied by number of observables in price_obs.</span>

<span class="sd">        num_items (int): number of items.</span>

<span class="sd">        pred_item (bool): there are two use cases of this model, suppose we have `user_index[i]` and `item_index[i]`</span>
<span class="sd">            for the i-th observation in the dataset.</span>
<span class="sd">            Case 1: which item among all items user `user_index[i]` is going to purchase, the prediction label</span>
<span class="sd">                is therefore `item_index[i]`. Equivalently, we can ask what&#39;s the likelihood for user `user_index[i]`</span>
<span class="sd">                to purchase `item_index[i]`.</span>
<span class="sd">            Case 2: what rating would user `user_index[i]` assign to item `item_index[i]`? In this case, the dataset</span>
<span class="sd">                object needs to contain a separate label.</span>
<span class="sd">                NOTE: for now, we only support binary labels.</span>

<span class="sd">        prior_variance (Union[float, Dict[str, float]]): the variance of prior distribution for</span>
<span class="sd">            coefficients. If a float is provided, all priors will be diagonal matrix with</span>
<span class="sd">            prior_variance along the diagonal. If a dictionary is provided, keys of prior_variance</span>
<span class="sd">            should be coefficient names, and the variance of prior of coef_name would be a diagonal</span>
<span class="sd">            matrix with prior_variance[coef_name] along the diagonal.</span>
<span class="sd">            Defaults to 1.0, which means all prior have identity matrix as the covariance matrix.</span>

<span class="sd">        num_users (int, optional): number of users, required only if coefficient or observable</span>
<span class="sd">            depending on user is in utility. Defaults to None.</span>
<span class="sd">        num_sessions (int, optional): number of sessions, required only if coefficient or</span>
<span class="sd">            observable depending on session is in utility. Defaults to None.</span>

<span class="sd">        trace_log_q (bool, optional): whether to trace the derivative of variational likelihood logQ</span>
<span class="sd">            with respect to variational parameters in the ELBO while conducting gradient update.</span>
<span class="sd">            Defaults to False.</span>

<span class="sd">        category_to_item (Dict[str, List[int]], optional): a dictionary with category id or name</span>
<span class="sd">            as keys, and category_to_item[C] contains the list of item ids belonging to category C.</span>
<span class="sd">            If None is provided, all items are assumed to be in the same category.</span>
<span class="sd">            Defaults to None.</span>

<span class="sd">        num_{user, item, session, price, taste}_obs (int, optional): number of observables of</span>
<span class="sd">            each type of features, only required if observable enters prior.</span>
<span class="sd">            NOTE: currently we only allow coefficient to depend on either user or item, thus only</span>
<span class="sd">            user and item observables can enter the prior of coefficient. Hence session, price,</span>
<span class="sd">            and taste observables are never required, we include it here for completeness.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BEMBFlex</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">utility_formula</span> <span class="o">=</span> <span class="n">utility_formula</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">obs2prior_dict</span> <span class="o">=</span> <span class="n">obs2prior_dict</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">coef_dim_dict</span> <span class="o">=</span> <span class="n">coef_dim_dict</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prior_variance</span> <span class="o">=</span> <span class="n">prior_variance</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">pred_item</span> <span class="o">=</span> <span class="n">pred_item</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">num_items</span> <span class="o">=</span> <span class="n">num_items</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_users</span> <span class="o">=</span> <span class="n">num_users</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_sessions</span> <span class="o">=</span> <span class="n">num_sessions</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">trace_log_q</span> <span class="o">=</span> <span class="n">trace_log_q</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">category_to_item</span> <span class="o">=</span> <span class="n">category_to_item</span>

    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># Category ID to Item ID mapping.</span>
    <span class="c1"># Category ID to Category Size mapping.</span>
    <span class="c1"># Item ID to Category ID mapping.</span>
    <span class="c1"># ==============================================================================================================</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">category_to_item</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_item</span><span class="p">:</span>
            <span class="c1"># assign all items to the same category if predicting items.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">category_to_item</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_items</span><span class="p">))}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># otherwise, for the j-th observation in the dataset, the label[j]</span>
            <span class="c1"># only depends on user_index[j] and item_index[j], so we put each</span>
            <span class="c1"># item to its own category.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">category_to_item</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_items</span><span class="p">)}</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">category_to_item</span><span class="p">)</span>

    <span class="n">max_category_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">category_to_item</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="n">category_to_item_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span><span class="p">,</span> <span class="n">max_category_size</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">category_to_size_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">item_in_c</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">category_to_item</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">category_to_item_tensor</span><span class="p">[</span><span class="n">c</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span>
            <span class="n">item_in_c</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">item_in_c</span><span class="p">)</span>
        <span class="n">category_to_size_tensor</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">scalar_tensor</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">item_in_c</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;category_to_item_tensor&#39;</span><span class="p">,</span>
                         <span class="n">category_to_item_tensor</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;category_to_size_tensor&#39;</span><span class="p">,</span>
                         <span class="n">category_to_size_tensor</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>

    <span class="n">item_to_category_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_items</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">items_in_c</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">category_to_item</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">item_to_category_tensor</span><span class="p">[</span><span class="n">items_in_c</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;item_to_category_tensor&#39;</span><span class="p">,</span>
                         <span class="n">item_to_category_tensor</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>

    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># Create Bayesian Coefficient Objects</span>
    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># model configuration.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">formula</span> <span class="o">=</span> <span class="n">parse_utility</span><span class="p">(</span><span class="n">utility_formula</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;BEMB: utility formula parsed:&#39;</span><span class="p">)</span>
    <span class="n">pprint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">formula</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">raw_formula</span> <span class="o">=</span> <span class="n">utility_formula</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">obs2prior_dict</span> <span class="o">=</span> <span class="n">obs2prior_dict</span>

    <span class="c1"># dimension of each observable, this one is used only for obs2prior.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_obs_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;user&#39;</span><span class="p">:</span> <span class="n">num_user_obs</span><span class="p">,</span>
        <span class="s1">&#39;item&#39;</span><span class="p">:</span> <span class="n">num_item_obs</span><span class="p">,</span>
        <span class="s1">&#39;session&#39;</span><span class="p">:</span> <span class="n">num_session_obs</span><span class="p">,</span>
        <span class="s1">&#39;price&#39;</span><span class="p">:</span> <span class="n">num_price_obs</span><span class="p">,</span>
        <span class="s1">&#39;taste&#39;</span><span class="p">:</span> <span class="n">num_taste_obs</span><span class="p">,</span>
        <span class="s1">&#39;constant&#39;</span><span class="p">:</span> <span class="mi">1</span>  <span class="c1"># not really used, for dummy variables.</span>
    <span class="p">}</span>

    <span class="c1"># how many classes for the variational distribution.</span>
    <span class="c1"># for example, beta_item would be `num_items` 10-dimensional gaussian if latent dim = 10.</span>
    <span class="n">variation_to_num_classes</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;user&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_users</span><span class="p">,</span>
        <span class="s1">&#39;item&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_items</span><span class="p">,</span>
        <span class="s1">&#39;constant&#39;</span><span class="p">:</span> <span class="mi">1</span>
    <span class="p">}</span>

    <span class="n">coef_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">additive_term</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">formula</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">coef_name</span> <span class="ow">in</span> <span class="n">additive_term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">]:</span>
            <span class="n">variation</span> <span class="o">=</span> <span class="n">coef_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">s2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_variance</span><span class="p">[</span><span class="n">coef_name</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">prior_variance</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_variance</span>
            <span class="n">coef_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">BayesianCoefficient</span><span class="p">(</span><span class="n">variation</span><span class="o">=</span><span class="n">variation</span><span class="p">,</span>
                                                       <span class="n">num_classes</span><span class="o">=</span><span class="n">variation_to_num_classes</span><span class="p">[</span><span class="n">variation</span><span class="p">],</span>
                                                       <span class="n">obs2prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">obs2prior_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">],</span>
                                                       <span class="n">num_obs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_obs_dict</span><span class="p">[</span><span class="n">variation</span><span class="p">],</span>
                                                       <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_dim_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">],</span>
                                                       <span class="n">prior_variance</span><span class="o">=</span><span class="n">s2</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">coef_dict</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">(</span><span class="n">coef_dict</span><span class="p">)</span>

    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># Optional: register additional modules.</span>
    <span class="c1"># ==============================================================================================================</span>
    <span class="k">if</span> <span class="n">additional_modules</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">additional_modules</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s1">&#39;Additional modules are temporarily disabled for further development.&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">additional_modules</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">additional_modules</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>




  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bemb.BEMBFlex.elbo" class="doc doc-heading">
<code class="highlight language-python"><span class="n">elbo</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">num_seeds</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>A combined method to computes the current ELBO given a batch, this method is used for training the model.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>ChoiceDataset</code></td>
        <td><p>a ChoiceDataset containing necessary information.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_seeds</code></td>
        <td><code>int</code></td>
        <td><p>the number of Monte Carlo samples from variational distributions
to evaluate the expectation in ELBO.
Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.Tensor</code></td>
      <td><p>a scalar tensor of the ELBO estimated from num_seeds Monte Carlo samples.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">elbo</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">ChoiceDataset</span><span class="p">,</span> <span class="n">num_seeds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;A combined method to computes the current ELBO given a batch, this method is used for training the model.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (ChoiceDataset): a ChoiceDataset containing necessary information.</span>
<span class="sd">        num_seeds (int, optional): the number of Monte Carlo samples from variational distributions</span>
<span class="sd">            to evaluate the expectation in ELBO.</span>
<span class="sd">            Defaults to 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: a scalar tensor of the ELBO estimated from num_seeds Monte Carlo samples.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># 1. sample latent variables from their variational distributions.</span>
    <span class="c1"># ==============================================================================================================</span>
    <span class="n">sample_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_coefficient_dictionary</span><span class="p">(</span><span class="n">num_seeds</span><span class="p">)</span>

    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># 2. compute log p(latent) prior.</span>
    <span class="c1"># (num_seeds,) --mean--&gt; scalar.</span>
    <span class="n">elbo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_prior</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">sample_dict</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># ==============================================================================================================</span>

    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># 3. compute the log likelihood log p(obs|latent).</span>
    <span class="c1"># sum over independent purchase decision for individual observations, mean over MC seeds.</span>
    <span class="c1"># the forward() function calls module.rsample(num_seeds) for module in self.additional_modules.</span>
    <span class="c1"># ==============================================================================================================</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_item</span><span class="p">:</span>
        <span class="c1"># the prediction target is item_index.</span>
        <span class="n">elbo</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span>
                             <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;log_prob&#39;</span><span class="p">,</span>
                             <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;item_index&#39;</span><span class="p">,</span>
                             <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                             <span class="n">sample_dict</span><span class="o">=</span><span class="n">sample_dict</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (num_seeds, len(batch)) --&gt; scalar.</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># the prediction target is binary.</span>
        <span class="c1"># TODO: update the prediction function.</span>
        <span class="n">utility</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span>
                               <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;utility&#39;</span><span class="p">,</span>
                               <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;item_index&#39;</span><span class="p">,</span>
                               <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                               <span class="n">sample_dict</span><span class="o">=</span><span class="n">sample_dict</span><span class="p">)</span>  <span class="c1"># (num_seeds, len(batch))</span>

        <span class="c1"># compute the log-likelihood for binary label.</span>
        <span class="c1"># (num_seeds, len(batch))</span>
        <span class="n">y_stacked</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">batch</span><span class="o">.</span><span class="n">label</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_seeds</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">y_stacked</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">utility</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">bce</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
        <span class="c1"># scalar.</span>
        <span class="n">ll</span> <span class="o">=</span> <span class="o">-</span> <span class="n">bce</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">utility</span><span class="p">),</span>
                   <span class="n">y_stacked</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">elbo</span> <span class="o">+=</span> <span class="n">ll</span>

    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># 4. optionally add log likelihood under variational distributions q(latent).</span>
    <span class="c1"># ==============================================================================================================</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trace_log_q</span><span class="p">:</span>
        <span class="n">elbo</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_variational</span><span class="p">(</span><span class="n">sample_dict</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">elbo</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bemb.BEMBFlex.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">return_type</span><span class="p">,</span> <span class="n">return_scope</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sample_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_seeds</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>A combined method for inference with the model.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>ChoiceDataset</code></td>
        <td><p>batch data containing choice information.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>return_type</code></td>
        <td><code>str</code></td>
        <td><p>either 'log_prob' or 'utility'.
'log_prob': return the log-probability (by within-category log-softmax) for items
'utility': return the utility value of items.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>return_scope</code></td>
        <td><code>str</code></td>
        <td><p>either 'item_index' or 'all_items'.
'item_index': for each observation i, return log-prob/utility for the chosen item batch.item_index[i] only.
'all_items': for each observation i, return log-prob/utility for all items.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>deterministic</code></td>
        <td><code>bool</code></td>
        <td><p>True: expectations of parameter variational distributions are used for inference.
False: the user needs to supply a dictionary of sampled parameters for inference.
Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>sample_dict</code></td>
        <td><code>Optional[Dict[str, torch.Tensor]]</code></td>
        <td><p>sampled parameters for inference task.
This is not needed when <code>deterministic</code> is True.
When <code>deterministic</code> is False, the user can supply a <code>sample_dict</code>. If <code>sample_dict</code> is not provided,
this method will create <code>num_seeds</code> samples.
Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>num_seeds</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>the number of random samples of parameters to construct. This is only required
if <code>deterministic</code> is False (i.e., stochastic mode) and <code>sample_dict</code> is not provided.
Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.Tensor</code></td>
      <td><p>a tensor of log-probabilities or utilities, depending on <code>return_type</code>.
    The shape of the returned tensor depends on <code>return_scope</code> and <code>deterministic</code>.
    -------------------------------------------------------------------------
    | <code>return_scope</code> | <code>deterministic</code> |         Output shape               |
    -------------------------------------------------------------------------
    |   'item_index` |      True       | (len(batch),)                      |
    -------------------------------------------------------------------------
    |   'all_items'  |      True       | (len(batch), num_items)            |
    -------------------------------------------------------------------------
    |   'item_index' |      False      | (num_seeds, len(batch))            |
    -------------------------------------------------------------------------
    |   'all_items'  |      False      | (num_seeds, len(batch), num_items) |
    -------------------------------------------------------------------------</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">ChoiceDataset</span><span class="p">,</span>
            <span class="n">return_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
            <span class="n">return_scope</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
            <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">sample_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">num_seeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;A combined method for inference with the model.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (ChoiceDataset): batch data containing choice information.</span>
<span class="sd">        return_type (str): either &#39;log_prob&#39; or &#39;utility&#39;.</span>
<span class="sd">            &#39;log_prob&#39;: return the log-probability (by within-category log-softmax) for items</span>
<span class="sd">            &#39;utility&#39;: return the utility value of items.</span>
<span class="sd">        return_scope (str): either &#39;item_index&#39; or &#39;all_items&#39;.</span>
<span class="sd">            &#39;item_index&#39;: for each observation i, return log-prob/utility for the chosen item batch.item_index[i] only.</span>
<span class="sd">            &#39;all_items&#39;: for each observation i, return log-prob/utility for all items.</span>
<span class="sd">        deterministic (bool, optional):</span>
<span class="sd">            True: expectations of parameter variational distributions are used for inference.</span>
<span class="sd">            False: the user needs to supply a dictionary of sampled parameters for inference.</span>
<span class="sd">            Defaults to True.</span>
<span class="sd">        sample_dict (Optional[Dict[str, torch.Tensor]], optional): sampled parameters for inference task.</span>
<span class="sd">            This is not needed when `deterministic` is True.</span>
<span class="sd">            When `deterministic` is False, the user can supply a `sample_dict`. If `sample_dict` is not provided,</span>
<span class="sd">            this method will create `num_seeds` samples.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        num_seeds (Optional[int]): the number of random samples of parameters to construct. This is only required</span>
<span class="sd">            if `deterministic` is False (i.e., stochastic mode) and `sample_dict` is not provided.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: a tensor of log-probabilities or utilities, depending on `return_type`.</span>
<span class="sd">            The shape of the returned tensor depends on `return_scope` and `deterministic`.</span>
<span class="sd">            -------------------------------------------------------------------------</span>
<span class="sd">            | `return_scope` | `deterministic` |         Output shape               |</span>
<span class="sd">            -------------------------------------------------------------------------</span>
<span class="sd">            |   &#39;item_index` |      True       | (len(batch),)                      |</span>
<span class="sd">            -------------------------------------------------------------------------</span>
<span class="sd">            |   &#39;all_items&#39;  |      True       | (len(batch), num_items)            |</span>
<span class="sd">            -------------------------------------------------------------------------</span>
<span class="sd">            |   &#39;item_index&#39; |      False      | (num_seeds, len(batch))            |</span>
<span class="sd">            -------------------------------------------------------------------------</span>
<span class="sd">            |   &#39;all_items&#39;  |      False      | (num_seeds, len(batch), num_items) |</span>
<span class="sd">            -------------------------------------------------------------------------</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># check arguments.</span>
    <span class="c1"># ==============================================================================================================</span>
    <span class="k">assert</span> <span class="n">return_type</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="s1">&#39;log_prob&#39;</span><span class="p">,</span> <span class="s1">&#39;utility&#39;</span><span class="p">],</span> <span class="s2">&quot;return_type must be either &#39;log_prob&#39; or &#39;utility&#39;.&quot;</span>
    <span class="k">assert</span> <span class="n">return_scope</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="s1">&#39;item_index&#39;</span><span class="p">,</span> <span class="s1">&#39;all_items&#39;</span><span class="p">],</span> <span class="s2">&quot;return_scope must be either &#39;item_index&#39; or &#39;all_items&#39;.&quot;</span>
    <span class="k">assert</span> <span class="n">deterministic</span> <span class="ow">in</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]</span>
    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">deterministic</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">sample_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">num_seeds</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;A positive interger `num_seeds` is required if `deterministic` is False and no `sample_dict` is provided.&quot;</span>

    <span class="c1"># when pred_item is true, the model is predicting which item is bought (specified by item_index).</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_item</span><span class="p">:</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_index</span>

    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># get sample_dict ready.</span>
    <span class="c1"># ==============================================================================================================</span>
    <span class="k">if</span> <span class="n">deterministic</span><span class="p">:</span>
        <span class="n">num_seeds</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="c1"># Use the means of variational distributions as the sole deterministic MC sample.</span>
        <span class="c1"># NOTE: here we don&#39;t need to sample the obs2prior weight H since we only compute the log-likelihood.</span>
        <span class="c1"># TODO: is this correct?</span>
        <span class="n">sample_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">coef_name</span><span class="p">,</span> <span class="n">coef</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">coef</span><span class="o">.</span><span class="n">variational_distribution</span><span class="o">.</span><span class="n">mean</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
                <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (1, num_*, dim)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">sample_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># sample stochastic parameters.</span>
            <span class="n">sample_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_coefficient_dictionary</span><span class="p">(</span><span class="n">num_seeds</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># use the provided sample_dict.</span>
            <span class="n">num_seeds</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">sample_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># call the sampling method of additional modules.</span>
    <span class="c1"># ==============================================================================================================</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_modules</span><span class="p">:</span>
        <span class="c1"># deterministic sample.</span>
        <span class="k">if</span> <span class="n">deterministic</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">dsample</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">rsample</span><span class="p">(</span><span class="n">num_seeds</span><span class="o">=</span><span class="n">num_seeds</span><span class="p">)</span>

    <span class="c1"># if utility is requested, don&#39;t run log-softmax, simply return logit.</span>
    <span class="n">return_logit</span> <span class="o">=</span> <span class="p">(</span><span class="n">return_type</span> <span class="o">==</span> <span class="s1">&#39;utility&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_scope</span> <span class="o">==</span> <span class="s1">&#39;all_items&#39;</span><span class="p">:</span>
        <span class="c1"># (num_seeds, len(batch), num_items)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_likelihood_all_items</span><span class="p">(</span>
            <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span> <span class="n">sample_dict</span><span class="o">=</span><span class="n">sample_dict</span><span class="p">,</span> <span class="n">return_logit</span><span class="o">=</span><span class="n">return_logit</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">return_scope</span> <span class="o">==</span> <span class="s1">&#39;item_index&#39;</span><span class="p">:</span>
        <span class="c1"># (num_seeds, len(batch))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_likelihood_item_index</span><span class="p">(</span>
            <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span> <span class="n">sample_dict</span><span class="o">=</span><span class="n">sample_dict</span><span class="p">,</span> <span class="n">return_logit</span><span class="o">=</span><span class="n">return_logit</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">deterministic</span><span class="p">:</span>
        <span class="c1"># drop the first dimension, which has size of `num_seeds` (equals 1 in the deterministic case).</span>
        <span class="c1"># (len(batch), num_items) or (len(batch),)</span>
        <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bemb.BEMBFlex.get_within_category_accuracy" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_within_category_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_p_all_items</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>A helper function for computing prediction accuracy (i.e., all non-differential metrics)
within category.
In particular, this method calculates the accuracy, precision, recall and F1 score.</p>
<p>This method has the same functionality as the following peusodcode:
for C in categories:
    # get sessions in which item in category C was purchased.
    T &lt;- (t for t in {0,1,..., len(label)-1} if label[t] is in C)
    Y &lt;- label[T]</p>
<pre><code>predictions = list()
for t in T:
    # get the prediction within category for this session.
    y_pred = argmax_{items in C} log prob computed before.
    predictions.append(y_pred)

accuracy = mean(Y == predictions)
</code></pre>
<p>Similarly, this function computes precision, recall and f1score as well.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>log_p_all_items</code></td>
        <td><code>torch.Tensor</code></td>
        <td><p>shape (num_sessions, num_items) the log probability of
choosing each item in each session.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>label</code></td>
        <td><code>torch.LongTensor</code></td>
        <td><p>shape (num_sessions,), the IDs of items purchased in each session.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>[Dict[str, float]]</code></td>
      <td><p>A dictionary containing performance metrics.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">get_within_category_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_p_all_items</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">label</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;A helper function for computing prediction accuracy (i.e., all non-differential metrics)</span>
<span class="sd">    within category.</span>
<span class="sd">    In particular, this method calculates the accuracy, precision, recall and F1 score.</span>


<span class="sd">    This method has the same functionality as the following peusodcode:</span>
<span class="sd">    for C in categories:</span>
<span class="sd">        # get sessions in which item in category C was purchased.</span>
<span class="sd">        T &lt;- (t for t in {0,1,..., len(label)-1} if label[t] is in C)</span>
<span class="sd">        Y &lt;- label[T]</span>

<span class="sd">        predictions = list()</span>
<span class="sd">        for t in T:</span>
<span class="sd">            # get the prediction within category for this session.</span>
<span class="sd">            y_pred = argmax_{items in C} log prob computed before.</span>
<span class="sd">            predictions.append(y_pred)</span>

<span class="sd">        accuracy = mean(Y == predictions)</span>

<span class="sd">    Similarly, this function computes precision, recall and f1score as well.</span>

<span class="sd">    Args:</span>
<span class="sd">        log_p_all_items (torch.Tensor): shape (num_sessions, num_items) the log probability of</span>
<span class="sd">            choosing each item in each session.</span>
<span class="sd">        label (torch.LongTensor): shape (num_sessions,), the IDs of items purchased in each session.</span>

<span class="sd">    Returns:</span>
<span class="sd">        [Dict[str, float]]: A dictionary containing performance metrics.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># argmax: (num_sessions, num_categories), within category argmax.</span>
    <span class="c1"># item IDs are consecutive, thus argmax is the same as IDs of the item with highest P.</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">argmax_by_category</span> <span class="o">=</span> <span class="n">scatter_max</span><span class="p">(</span>
        <span class="n">log_p_all_items</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">item_to_category_tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># category_purchased[t] = the category of item label[t].</span>
    <span class="c1"># (num_sessions,)</span>
    <span class="n">category_purchased</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">item_to_category_tensor</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>

    <span class="c1"># pred[t] = the item with highest utility from the category item label[t] belongs to.</span>
    <span class="c1"># (num_sessions,)</span>
    <span class="n">pred_from_category</span> <span class="o">=</span> <span class="n">argmax_by_category</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
        <span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">)),</span> <span class="n">category_purchased</span><span class="p">]</span>

    <span class="n">within_category_accuracy</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">pred_from_category</span> <span class="o">==</span> <span class="n">label</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="c1"># precision</span>
    <span class="n">precision</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

    <span class="n">recall</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_items</span><span class="p">):</span>
        <span class="n">correct_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
            <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">pred_from_category</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="o">==</span> <span class="n">i</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
        <span class="n">precision_i</span> <span class="o">=</span> <span class="n">correct_i</span> <span class="o">/</span> \
            <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">pred_from_category</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
        <span class="n">recall_i</span> <span class="o">=</span> <span class="n">correct_i</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">label</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>

        <span class="c1"># do not add if divided by zero.</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">pred_from_category</span> <span class="o">==</span> <span class="n">i</span><span class="p">):</span>
            <span class="n">precision</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">precision_i</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">label</span> <span class="o">==</span> <span class="n">i</span><span class="p">):</span>
            <span class="n">recall</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">recall_i</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="n">precision</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">precision</span><span class="p">))</span>
    <span class="n">recall</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">recall</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">precision</span> <span class="o">==</span> <span class="n">recall</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">f1</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">f1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="n">within_category_accuracy</span><span class="p">,</span>
            <span class="s1">&#39;precision&#39;</span><span class="p">:</span> <span class="n">precision</span><span class="p">,</span>
            <span class="s1">&#39;recall&#39;</span><span class="p">:</span> <span class="n">recall</span><span class="p">,</span>
            <span class="s1">&#39;f1score&#39;</span><span class="p">:</span> <span class="n">f1</span><span class="p">}</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bemb.BEMBFlex.log_likelihood_all_items" class="doc doc-heading">
<code class="highlight language-python"><span class="n">log_likelihood_all_items</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">return_logit</span><span class="p">,</span> <span class="n">sample_dict</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>NOTE to developers:
This method computes utilities for all items available, which is a relatively slow operation. For
training the model, you only need the utility/log-prob for the chosen/relevant item (i.e., item_index[i] for each i-th observation).
Use this method for inference only.
Use self.log_likelihood_item_index() for training instead.</p>
<p>Computes the log probability of choosing <code>each</code> item in each session based on current model parameters.
This method allows for specifying {user, item}_latent_value for Monte Carlo estimation in ELBO.
For actual prediction tasks, use the forward() function, which will use means of variational
distributions for user and item latents.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>ChoiceDataset</code></td>
        <td><p>a ChoiceDataset object containing relevant information.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>return_logit(bool)</code></td>
        <td></td>
        <td><p>if set to True, return the log-probability, otherwise return the logit/utility.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>sample_dict(Dict[str,</code></td>
        <td><code>torch.Tensor]</code></td>
        <td><p>Monte Carlo samples for model coefficients
(i.e., those Greek letters).
sample_dict.keys() should be the same as keys of self.obs2prior_dict, i.e., those
greek letters actually enter the functional form of utility.
The value of sample_dict should be tensors of shape (num_seeds, num_classes, dim)
where num_classes in {num_users, num_items, 1}
and dim in {latent_dim(K), num_item_obs, num_user_obs, 1}.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.Tensor</code></td>
      <td><p>a tensor of shape (num_seeds, len(batch), self.num_items), where
    out[x, y, z] is the probability of choosing item z in session y conditioned on
    latents to be the x-th Monte Carlo sample.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">log_likelihood_all_items</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">ChoiceDataset</span><span class="p">,</span> <span class="n">return_logit</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">sample_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    NOTE to developers:</span>
<span class="sd">    This method computes utilities for all items available, which is a relatively slow operation. For</span>
<span class="sd">    training the model, you only need the utility/log-prob for the chosen/relevant item (i.e., item_index[i] for each i-th observation).</span>
<span class="sd">    Use this method for inference only.</span>
<span class="sd">    Use self.log_likelihood_item_index() for training instead.</span>

<span class="sd">    Computes the log probability of choosing `each` item in each session based on current model parameters.</span>
<span class="sd">    This method allows for specifying {user, item}_latent_value for Monte Carlo estimation in ELBO.</span>
<span class="sd">    For actual prediction tasks, use the forward() function, which will use means of variational</span>
<span class="sd">    distributions for user and item latents.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (ChoiceDataset): a ChoiceDataset object containing relevant information.</span>
<span class="sd">        return_logit(bool): if set to True, return the log-probability, otherwise return the logit/utility.</span>
<span class="sd">        sample_dict(Dict[str, torch.Tensor]): Monte Carlo samples for model coefficients</span>
<span class="sd">            (i.e., those Greek letters).</span>
<span class="sd">            sample_dict.keys() should be the same as keys of self.obs2prior_dict, i.e., those</span>
<span class="sd">            greek letters actually enter the functional form of utility.</span>
<span class="sd">            The value of sample_dict should be tensors of shape (num_seeds, num_classes, dim)</span>
<span class="sd">            where num_classes in {num_users, num_items, 1}</span>
<span class="sd">            and dim in {latent_dim(K), num_item_obs, num_user_obs, 1}.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: a tensor of shape (num_seeds, len(batch), self.num_items), where</span>
<span class="sd">            out[x, y, z] is the probability of choosing item z in session y conditioned on</span>
<span class="sd">            latents to be the x-th Monte Carlo sample.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_seeds</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">sample_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># avoid repeated work when user purchased several items in the same session.</span>
    <span class="n">user_session_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span><span class="n">batch</span><span class="o">.</span><span class="n">user_index</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">session_index</span><span class="p">])</span>
    <span class="k">assert</span> <span class="n">user_session_index</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
    <span class="n">unique_user_sess</span><span class="p">,</span> <span class="n">inverse_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span>
        <span class="n">user_session_index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">user_index</span> <span class="o">=</span> <span class="n">unique_user_sess</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">session_index</span> <span class="o">=</span> <span class="n">unique_user_sess</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">user_index</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">session_index</span><span class="p">)</span>

    <span class="c1"># short-hands for easier shape check.</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">num_seeds</span>
    <span class="c1"># P = len(batch)  # num_purchases.</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">unique_user_sess</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">S</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_sessions</span>
    <span class="n">U</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_users</span>
    <span class="n">I</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_items</span>

    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># Helper Functions for Reshaping.</span>
    <span class="c1"># ==============================================================================================================</span>
    <span class="k">def</span> <span class="nf">reshape_user_coef_sample</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
        <span class="c1"># input shape (R, U, *)</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (R, U, I, *)</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">user_index</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
        <span class="k">assert</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">C</span>

    <span class="k">def</span> <span class="nf">reshape_item_coef_sample</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
        <span class="c1"># input shape (R, I, *)</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">C</span>

    <span class="k">def</span> <span class="nf">reshape_constant_coef_sample</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
        <span class="c1"># input shape (R, *)</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">C</span>

    <span class="k">def</span> <span class="nf">reshape_coef_sample</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="c1"># reshape the monte carlo sample of coefficients to (R, P, I, *).</span>
        <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_user&#39;</span><span class="p">):</span>
            <span class="c1"># (R, U, *) --&gt; (R, P, I, *)</span>
            <span class="k">return</span> <span class="n">reshape_user_coef_sample</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_item&#39;</span><span class="p">):</span>
            <span class="c1"># (R, I, *) --&gt; (R, P, I, *)</span>
            <span class="k">return</span> <span class="n">reshape_item_coef_sample</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_constant&#39;</span><span class="p">):</span>
            <span class="c1"># (R, *) --&gt; (R, P, I, *)</span>
            <span class="k">return</span> <span class="n">reshape_constant_coef_sample</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span>

    <span class="k">def</span> <span class="nf">reshape_observable</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="c1"># reshape observable to (R, P, I, *) so that it can be multiplied with monte carlo</span>
        <span class="c1"># samples of coefficients.</span>
        <span class="n">O</span> <span class="o">=</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of observables.</span>
        <span class="k">assert</span> <span class="n">O</span> <span class="o">==</span> <span class="n">positive_integer</span>
        <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;item_&#39;</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;user_&#39;</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">user_index</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># (P, O)</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;session_&#39;</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">session_index</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># (P, O)</span>
            <span class="k">return</span> <span class="n">obs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;price_&#39;</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">session_index</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># (P, I, O)</span>
            <span class="k">return</span> <span class="n">obs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;taste_&#39;</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">user_index</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># (P, I, O)</span>
            <span class="k">return</span> <span class="n">obs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span>
        <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">obs</span>

    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># Copmute the Utility Term by Term.</span>
    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># P is the number of unique (user, session) pairs.</span>
    <span class="c1"># (random_seeds, P, num_items).</span>
    <span class="n">utility</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># loop over additive term to utility</span>
    <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">formula</span><span class="p">:</span>
        <span class="c1"># Type I: single coefficient, e.g., lambda_item or lambda_user.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># E.g., lambda_item or lambda_user</span>
            <span class="n">coef_name</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">coef_sample</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">],</span> <span class="n">coef_name</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">coef_sample</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">additive_term</span> <span class="o">=</span> <span class="n">coef_sample</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">)</span>

        <span class="c1"># Type II: factorized coefficient, e.g., &lt;theta_user, lambda_item&gt;.</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">coef_name_0</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">coef_name_1</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">coef_sample_0</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name_0</span><span class="p">],</span> <span class="n">coef_name_0</span><span class="p">)</span>
            <span class="n">coef_sample_1</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name_1</span><span class="p">],</span> <span class="n">coef_name_1</span><span class="p">)</span>

            <span class="k">assert</span> <span class="n">coef_sample_0</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">coef_sample_1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span>
                <span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>

            <span class="n">additive_term</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef_sample_0</span> <span class="o">*</span> <span class="n">coef_sample_1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Type III: single coefficient multiplied by observable, e.g., theta_user * x_obs_item.</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">coef_name</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">coef_sample</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">],</span> <span class="n">coef_name</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">coef_sample</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>

            <span class="n">obs_name</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">reshape_observable</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">obs_name</span><span class="p">),</span> <span class="n">obs_name</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>

            <span class="n">additive_term</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef_sample</span> <span class="o">*</span> <span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Type IV: factorized coefficient multiplied by observable.</span>
        <span class="c1"># e.g., gamma_user * beta_item * price_obs.</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">coef_name_0</span><span class="p">,</span> <span class="n">coef_name_1</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">coef_sample_0</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name_0</span><span class="p">],</span> <span class="n">coef_name_0</span><span class="p">)</span>
            <span class="n">coef_sample_1</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name_1</span><span class="p">],</span> <span class="n">coef_name_1</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">coef_sample_0</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">coef_sample_1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span>
                <span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>
            <span class="n">num_obs_times_latent_dim</span> <span class="o">=</span> <span class="n">coef_sample_0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">obs_name</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">reshape_observable</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">obs_name</span><span class="p">),</span> <span class="n">obs_name</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>
            <span class="n">num_obs</span> <span class="o">=</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of observables.</span>

            <span class="k">assert</span> <span class="p">(</span><span class="n">num_obs_times_latent_dim</span> <span class="o">%</span> <span class="n">num_obs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="n">latent_dim</span> <span class="o">=</span> <span class="n">num_obs_times_latent_dim</span> <span class="o">//</span> <span class="n">num_obs</span>

            <span class="n">coef_sample_0</span> <span class="o">=</span> <span class="n">coef_sample_0</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">num_obs</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
            <span class="n">coef_sample_1</span> <span class="o">=</span> <span class="n">coef_sample_1</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">num_obs</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
            <span class="c1"># compute the factorized coefficient with shape (R, P, I, O).</span>
            <span class="n">coef</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef_sample_0</span> <span class="o">*</span> <span class="n">coef_sample_1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">additive_term</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef</span> <span class="o">*</span> <span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Undefined term type: </span><span class="si">{</span><span class="n">term</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">I</span><span class="p">)</span>
        <span class="n">utility</span> <span class="o">+=</span> <span class="n">additive_term</span>

    <span class="c1"># ==============================================================================================================</span>
    <span class="c1"># Mask Out Unavailable Items in Each Session.</span>
    <span class="c1"># ==============================================================================================================</span>

    <span class="k">if</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_availability</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># expand to the Monte Carlo sample dimension.</span>
        <span class="c1"># (S, I) -&gt; (P, I) -&gt; (1, P, I) -&gt; (R, P, I)</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_availability</span><span class="p">[</span><span class="n">session_index</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
            <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">utility</span><span class="p">[</span><span class="o">~</span><span class="n">A</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">utility</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">utility</span> <span class="o">=</span> <span class="n">utility</span><span class="p">[:,</span> <span class="n">inverse_indices</span><span class="p">,</span> <span class="p">:]</span>
    <span class="k">assert</span> <span class="n">utility</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="n">I</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_modules</span><span class="p">:</span>
        <span class="n">additive_term</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">utility</span> <span class="o">+=</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">I</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">return_logit</span><span class="p">:</span>
        <span class="c1"># output shape: (num_seeds, len(batch), num_items)</span>
        <span class="k">return</span> <span class="n">utility</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># compute log likelihood log p(choosing item i | user, item latents)</span>
        <span class="c1"># compute log softmax separately within each category.</span>
        <span class="n">log_p</span> <span class="o">=</span> <span class="n">scatter_log_softmax</span><span class="p">(</span>
            <span class="n">utility</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">item_to_category_tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># output shape: (num_seeds, len(batch), num_items)</span>
        <span class="k">return</span> <span class="n">log_p</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bemb.BEMBFlex.log_likelihood_item_index" class="doc doc-heading">
<code class="highlight language-python"><span class="n">log_likelihood_item_index</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">return_logit</span><span class="p">,</span> <span class="n">sample_dict</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>NOTE for developers:
This method is more efficient and only computes log-likelihood/logit(utility) for item in item_index[i] for each
i-th observation.
Developers should use use <code>log_likelihood_all_items</code> for inference purpose and to computes log-likelihoods/utilities
for ALL items for the i-th observation.</p>
<p>Computes the log probability of choosing item_index[i] in each session based on current model parameters.
This method allows for specifying {user, item}_latent_value for Monte Carlo estimation in ELBO.
For actual prediction tasks, use the forward() function, which will use means of variational
distributions for user and item latents.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>ChoiceDataset</code></td>
        <td><p>a ChoiceDataset object containing relevant information.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>return_logit(bool)</code></td>
        <td></td>
        <td><p>if set to True, return the log-probability, otherwise return the logit/utility.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>sample_dict(Dict[str,</code></td>
        <td><code>torch.Tensor]</code></td>
        <td><p>Monte Carlo samples for model coefficients
(i.e., those Greek letters).
sample_dict.keys() should be the same as keys of self.obs2prior_dict, i.e., those
greek letters actually enter the functional form of utility.
The value of sample_dict should be tensors of shape (num_seeds, num_classes, dim)
where num_classes in {num_users, num_items, 1}
and dim in {latent_dim(K), num_item_obs, num_user_obs, 1}.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.Tensor</code></td>
      <td><p>a tensor of shape (num_seeds, len(batch)), where
    out[x, y] is the probabilities of choosing item batch.item[y] in session y
    conditioned on latents to be the x-th Monte Carlo sample.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">log_likelihood_item_index</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">ChoiceDataset</span><span class="p">,</span> <span class="n">return_logit</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">sample_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    NOTE for developers:</span>
<span class="sd">    This method is more efficient and only computes log-likelihood/logit(utility) for item in item_index[i] for each</span>
<span class="sd">    i-th observation.</span>
<span class="sd">    Developers should use use `log_likelihood_all_items` for inference purpose and to computes log-likelihoods/utilities</span>
<span class="sd">    for ALL items for the i-th observation.</span>

<span class="sd">    Computes the log probability of choosing item_index[i] in each session based on current model parameters.</span>
<span class="sd">    This method allows for specifying {user, item}_latent_value for Monte Carlo estimation in ELBO.</span>
<span class="sd">    For actual prediction tasks, use the forward() function, which will use means of variational</span>
<span class="sd">    distributions for user and item latents.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (ChoiceDataset): a ChoiceDataset object containing relevant information.</span>
<span class="sd">        return_logit(bool): if set to True, return the log-probability, otherwise return the logit/utility.</span>
<span class="sd">        sample_dict(Dict[str, torch.Tensor]): Monte Carlo samples for model coefficients</span>
<span class="sd">            (i.e., those Greek letters).</span>
<span class="sd">            sample_dict.keys() should be the same as keys of self.obs2prior_dict, i.e., those</span>
<span class="sd">            greek letters actually enter the functional form of utility.</span>
<span class="sd">            The value of sample_dict should be tensors of shape (num_seeds, num_classes, dim)</span>
<span class="sd">            where num_classes in {num_users, num_items, 1}</span>
<span class="sd">            and dim in {latent_dim(K), num_item_obs, num_user_obs, 1}.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: a tensor of shape (num_seeds, len(batch)), where</span>
<span class="sd">            out[x, y] is the probabilities of choosing item batch.item[y] in session y</span>
<span class="sd">            conditioned on latents to be the x-th Monte Carlo sample.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_seeds</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">sample_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># get category id of the item bought in each row of batch.</span>
    <span class="n">cate_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">item_to_category_tensor</span><span class="p">[</span><span class="n">batch</span><span class="o">.</span><span class="n">item_index</span><span class="p">]</span>

    <span class="c1"># get item ids of all items from the same category of each item bought.</span>
    <span class="n">relevant_item_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">category_to_item_tensor</span><span class="p">[</span><span class="n">cate_index</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">relevant_item_index</span> <span class="o">=</span> <span class="n">relevant_item_index</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>
    <span class="c1"># index were padded with -1&#39;s, drop those dummy entries.</span>
    <span class="n">relevant_item_index</span> <span class="o">=</span> <span class="n">relevant_item_index</span><span class="p">[</span><span class="n">relevant_item_index</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># the first repeats[0] entries in relevant_item_index are for the category of item_index[0]</span>
    <span class="n">repeats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">category_to_size_tensor</span><span class="p">[</span><span class="n">cate_index</span><span class="p">]</span>
    <span class="c1"># argwhere(reverse_indices == k) are positions in relevant_item_index for the category of item_index[k].</span>
    <span class="n">reverse_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">repeats</span><span class="p">)</span>
    <span class="c1"># expand the user_index and session_index.</span>
    <span class="n">user_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">user_index</span><span class="p">,</span> <span class="n">repeats</span><span class="p">)</span>
    <span class="n">session_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">session_index</span><span class="p">,</span> <span class="n">repeats</span><span class="p">)</span>
    <span class="c1"># duplicate the item focused to match.</span>
    <span class="n">item_index_expanded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">item_index</span><span class="p">,</span> <span class="n">repeats</span><span class="p">)</span>

    <span class="c1"># short-hands for easier shape check.</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">num_seeds</span>
    <span class="c1"># total number of relevant items.</span>
    <span class="n">total_computation</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">session_index</span><span class="p">)</span>
    <span class="n">S</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_sessions</span>
    <span class="n">U</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_users</span>
    <span class="n">I</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_items</span>
    <span class="c1"># ==========================================================================================</span>
    <span class="c1"># Helper Functions for Reshaping.</span>
    <span class="c1"># ==========================================================================================</span>

    <span class="k">def</span> <span class="nf">reshape_coef_sample</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="c1"># reshape the monte carlo sample of coefficients to (R, P, I, *).</span>
        <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_user&#39;</span><span class="p">):</span>
            <span class="c1"># (R, U, *) --&gt; (R, total_computation, *)</span>
            <span class="k">return</span> <span class="n">sample</span><span class="p">[:,</span> <span class="n">user_index</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_item&#39;</span><span class="p">):</span>
            <span class="c1"># (R, I, *) --&gt; (R, total_computation, *)</span>
            <span class="k">return</span> <span class="n">sample</span><span class="p">[:,</span> <span class="n">relevant_item_index</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_constant&#39;</span><span class="p">):</span>
            <span class="c1"># (R, *) --&gt; (R, total_computation, *)</span>
            <span class="k">return</span> <span class="n">sample</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span>

    <span class="k">def</span> <span class="nf">reshape_observable</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="c1"># reshape observable to (R, P, I, *) so that it can be multiplied with monte carlo</span>
        <span class="c1"># samples of coefficients.</span>
        <span class="n">O</span> <span class="o">=</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of observables.</span>
        <span class="k">assert</span> <span class="n">O</span> <span class="o">==</span> <span class="n">positive_integer</span>
        <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;item_&#39;</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">relevant_item_index</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;user_&#39;</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">user_index</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;session_&#39;</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">session_index</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;price_&#39;</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">session_index</span><span class="p">,</span> <span class="n">relevant_item_index</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;taste_&#39;</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">user_index</span><span class="p">,</span> <span class="n">relevant_item_index</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span>
        <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">total_computation</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">obs</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># ==========================================================================================</span>
    <span class="c1"># Compute Components related to users and items only.</span>
    <span class="c1"># ==========================================================================================</span>
    <span class="n">utility</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># loop over additive term to utility</span>
    <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">formula</span><span class="p">:</span>
        <span class="c1"># Type I: single coefficient, e.g., lambda_item or lambda_user.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># E.g., lambda_item or lambda_user</span>
            <span class="n">coef_name</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">coef_sample</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">],</span> <span class="n">coef_name</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">coef_sample</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">additive_term</span> <span class="o">=</span> <span class="n">coef_sample</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">)</span>

        <span class="c1"># Type II: factorized coefficient, e.g., &lt;theta_user, lambda_item&gt;.</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">coef_name_0</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">coef_name_1</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">coef_sample_0</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name_0</span><span class="p">],</span> <span class="n">coef_name_0</span><span class="p">)</span>
            <span class="n">coef_sample_1</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name_1</span><span class="p">],</span> <span class="n">coef_name_1</span><span class="p">)</span>

            <span class="k">assert</span> <span class="n">coef_sample_0</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">coef_sample_1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span>
                <span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>

            <span class="n">additive_term</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef_sample_0</span> <span class="o">*</span> <span class="n">coef_sample_1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Type III: single coefficient multiplied by observable, e.g., theta_user * x_obs_item.</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">coef_name</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">coef_sample</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">],</span> <span class="n">coef_name</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">coef_sample</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span>
                <span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>

            <span class="n">obs_name</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">reshape_observable</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">obs_name</span><span class="p">),</span> <span class="n">obs_name</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>

            <span class="n">additive_term</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef_sample</span> <span class="o">*</span> <span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Type IV: factorized coefficient multiplied by observable.</span>
        <span class="c1"># e.g., gamma_user * beta_item * price_obs.</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">coef_name_0</span><span class="p">,</span> <span class="n">coef_name_1</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">coef_sample_0</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name_0</span><span class="p">],</span> <span class="n">coef_name_0</span><span class="p">)</span>
            <span class="n">coef_sample_1</span> <span class="o">=</span> <span class="n">reshape_coef_sample</span><span class="p">(</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name_1</span><span class="p">],</span> <span class="n">coef_name_1</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">coef_sample_0</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">coef_sample_1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span>
                <span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>
            <span class="n">num_obs_times_latent_dim</span> <span class="o">=</span> <span class="n">coef_sample_0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">obs_name</span> <span class="o">=</span> <span class="n">term</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">reshape_observable</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">obs_name</span><span class="p">),</span> <span class="n">obs_name</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="n">positive_integer</span><span class="p">)</span>
            <span class="n">num_obs</span> <span class="o">=</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of observables.</span>

            <span class="k">assert</span> <span class="p">(</span><span class="n">num_obs_times_latent_dim</span> <span class="o">%</span> <span class="n">num_obs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="n">latent_dim</span> <span class="o">=</span> <span class="n">num_obs_times_latent_dim</span> <span class="o">//</span> <span class="n">num_obs</span>

            <span class="n">coef_sample_0</span> <span class="o">=</span> <span class="n">coef_sample_0</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="n">num_obs</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
            <span class="n">coef_sample_1</span> <span class="o">=</span> <span class="n">coef_sample_1</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">,</span> <span class="n">num_obs</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
            <span class="c1"># compute the factorized coefficient with shape (R, P, I, O).</span>
            <span class="n">coef</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef_sample_0</span> <span class="o">*</span> <span class="n">coef_sample_1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">additive_term</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef</span> <span class="o">*</span> <span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Undefined term type: </span><span class="si">{</span><span class="n">term</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">)</span>
        <span class="n">utility</span> <span class="o">+=</span> <span class="n">additive_term</span>

    <span class="c1"># ==========================================================================================</span>
    <span class="c1"># Mask Out Unavailable Items in Each Session.</span>
    <span class="c1"># ==========================================================================================</span>

    <span class="k">if</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_availability</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># expand to the Monte Carlo sample dimension.</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_availability</span><span class="p">[</span><span class="n">session_index</span><span class="p">,</span> <span class="n">relevant_item_index</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
            <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">utility</span><span class="p">[</span><span class="o">~</span><span class="n">A</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">utility</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_modules</span><span class="p">:</span>
        <span class="c1"># current utility shape: (R, total_computation)</span>
        <span class="n">additive_term</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span>
            <span class="n">R</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span> <span class="ow">or</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># TODO: need to make this consistent with log_likelihood_all.</span>
            <span class="c1"># be tolerant for some customized module with BayesianLinear that returns (R, len(batch), 1).</span>
            <span class="n">additive_term</span> <span class="o">=</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
        <span class="c1"># expand to total number of computation, query by reverse_indices.</span>
        <span class="c1"># reverse_indices has length total_computation, and reverse_indices[i] correspond to the row-id that this</span>
        <span class="c1"># computation is responsible for.</span>
        <span class="n">additive_term</span> <span class="o">=</span> <span class="n">additive_term</span><span class="p">[:,</span> <span class="n">reverse_indices</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">additive_term</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">total_computation</span><span class="p">)</span>

    <span class="c1"># compute log likelihood log p(choosing item i | user, item latents)</span>
    <span class="k">if</span> <span class="n">return_logit</span><span class="p">:</span>
        <span class="n">log_p</span> <span class="o">=</span> <span class="n">utility</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># compute the log probability from logits/utilities.</span>
        <span class="n">log_p</span> <span class="o">=</span> <span class="n">scatter_log_softmax</span><span class="p">(</span><span class="n">utility</span><span class="p">,</span> <span class="n">reverse_indices</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># select the log-P of the item actually bought.</span>
    <span class="n">log_p</span> <span class="o">=</span> <span class="n">log_p</span><span class="p">[:,</span> <span class="n">item_index_expanded</span> <span class="o">==</span> <span class="n">relevant_item_index</span><span class="p">]</span>
    <span class="c1"># output shape: (num_seeds, num_purchases, num_items)</span>
    <span class="k">return</span> <span class="n">log_p</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bemb.BEMBFlex.log_prior" class="doc doc-heading">
<code class="highlight language-python"><span class="n">log_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">sample_dict</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>Calculates the log-likelihood of Monte Carlo samples of Bayesian coefficients under their
prior distribution. This method assume coefficients are statistically independent.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>ChoiceDataset</code></td>
        <td><p>a dataset object contains observables for computing the prior distribution
if obs2prior is True.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>sample_dict</code></td>
        <td><code>Dict[str, torch.Tensor]</code></td>
        <td><p>a dictionary coefficient names to Monte Carlo samples.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Exceptions:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>ValueError</code></td>
        <td><p>[description]</p></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.scalar_tensor</code></td>
      <td><p>a tensor with shape (num_seeds,) of [ log P_{prior_distribution}(param[i]) ],
    where param[i] is the i-th Monte Carlo sample.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">log_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">ChoiceDataset</span><span class="p">,</span> <span class="n">sample_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Calculates the log-likelihood of Monte Carlo samples of Bayesian coefficients under their</span>
<span class="sd">    prior distribution. This method assume coefficients are statistically independent.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (ChoiceDataset): a dataset object contains observables for computing the prior distribution</span>
<span class="sd">            if obs2prior is True.</span>
<span class="sd">        sample_dict (Dict[str, torch.Tensor]): a dictionary coefficient names to Monte Carlo samples.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: [description]</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.scalar_tensor: a tensor with shape (num_seeds,) of [ log P_{prior_distribution}(param[i]) ],</span>
<span class="sd">            where param[i] is the i-th Monte Carlo sample.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># assert sample_dict.keys() == self.coef_dict.keys()</span>
    <span class="n">num_seeds</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">sample_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">total</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">coef_name</span><span class="p">,</span> <span class="n">coef</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">obs2prior_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">coef_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_item&#39;</span><span class="p">):</span>
                <span class="n">x_obs</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_obs</span>
            <span class="k">elif</span> <span class="n">coef_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_user&#39;</span><span class="p">):</span>
                <span class="n">x_obs</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">user_obs</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s1">&#39;No observable found to support obs2prior for </span><span class="si">{</span><span class="n">coef_name</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>

            <span class="n">total</span> <span class="o">+=</span> <span class="n">coef</span><span class="o">.</span><span class="n">log_prior</span><span class="p">(</span><span class="n">sample</span><span class="o">=</span><span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">],</span>
                                    <span class="n">H_sample</span><span class="o">=</span><span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span> <span class="o">+</span> <span class="s1">&#39;.H&#39;</span><span class="p">],</span>
                                    <span class="n">x_obs</span><span class="o">=</span><span class="n">x_obs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># log_prob outputs (num_seeds, num_{items, users}), sum to (num_seeds).</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">coef</span><span class="o">.</span><span class="n">log_prior</span><span class="p">(</span>
                <span class="n">sample</span><span class="o">=</span><span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">],</span> <span class="n">H_sample</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">x_obs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_modules</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">module</span><span class="o">.</span><span class="n">log_prior</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">total</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bemb.BEMBFlex.log_variational" class="doc doc-heading">
<code class="highlight language-python"><span class="n">log_variational</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_dict</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>Calculate the log-likelihood of samples in sample_dict under the current variational
distribution.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>sample_dict</code></td>
        <td><code>Dict[str, torch.Tensor]</code></td>
        <td><p>a dictionary coefficient names to Monte Carlo
samples.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.Tensor</code></td>
      <td><p>a tensor of shape (num_seeds) of [ log P_{variational_distribution}(param[i]) ],
    where param[i] is the i-th Monte Carlo sample.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">log_variational</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Calculate the log-likelihood of samples in sample_dict under the current variational</span>
<span class="sd">    distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_dict (Dict[str, torch.Tensor]):  a dictionary coefficient names to Monte Carlo</span>
<span class="sd">            samples.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: a tensor of shape (num_seeds) of [ log P_{variational_distribution}(param[i]) ],</span>
<span class="sd">            where param[i] is the i-th Monte Carlo sample.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_seeds</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">sample_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">total</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_seeds</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">coef_name</span><span class="p">,</span> <span class="n">coef</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># log_prob outputs (num_seeds, num_{items, users}), sum to (num_seeds).</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">coef</span><span class="o">.</span><span class="n">log_variational</span><span class="p">(</span><span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_modules</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
        <span class="c1"># with shape (num_seeds,)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">module</span><span class="o">.</span><span class="n">log_variational</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">total</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bemb.BEMBFlex.sample_coefficient_dictionary" class="doc doc-heading">
<code class="highlight language-python"><span class="n">sample_coefficient_dictionary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_seeds</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>A helper function to sample parameters from coefficients.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>num_seeds</code></td>
        <td><code>int</code></td>
        <td><p>number of random samples.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, torch.Tensor]</code></td>
      <td><p>a dictionary maps coefficient names to tensor of sampled coefficient parameters,
    where the first dimension of the sampled tensor has size <code>num_seeds</code>.
    Each sample tensor has shape (num_seeds, num_classes, dim).</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">sample_coefficient_dictionary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_seeds</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;A helper function to sample parameters from coefficients.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_seeds (int): number of random samples.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, torch.Tensor]: a dictionary maps coefficient names to tensor of sampled coefficient parameters,</span>
<span class="sd">            where the first dimension of the sampled tensor has size `num_seeds`.</span>
<span class="sd">            Each sample tensor has shape (num_seeds, num_classes, dim).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sample_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">coef_name</span><span class="p">,</span> <span class="n">coef</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">coef</span><span class="o">.</span><span class="n">rsample</span><span class="p">(</span><span class="n">num_seeds</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">coef</span><span class="o">.</span><span class="n">obs2prior</span><span class="p">:</span>
            <span class="c1"># sample both obs2prior weight and realization of variable.</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
            <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span> <span class="o">+</span> <span class="s1">&#39;.H&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># only sample the realization of variable.</span>
            <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">sample_dict</span><span class="p">[</span><span class="n">coef_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>
    <span class="k">return</span> <span class="n">sample_dict</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>





  <div class="doc doc-object doc-function">



<h4 id="bemb.model.bemb.parse_utility" class="doc doc-heading">
<code class="highlight language-python"><span class="n">parse_utility</span><span class="p">(</span><span class="n">utility_string</span><span class="p">)</span></code>


</h4>

    <div class="doc doc-contents ">

      <p>A helper function parse utility string into a list of additive terms.</p>

<p><strong>Examples:</strong></p>
    <p>utility_string = 'lambda_item + theta_user * alpha_item + gamma_user * beta_item * price_obs'
output = [
    {
        'coefficient': ['lambda_item'],
        'observable': None
    },
    {
        'coefficient': ['theta_user', 'alpha_item'],
        'observable': None
    },
    {
        'coefficient': ['gamma_user', 'beta_item'],
        'observable': 'price_obs'
    }
    ]</p>

        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">parse_utility</span><span class="p">(</span><span class="n">utility_string</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="kc">None</span><span class="p">]]]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A helper function parse utility string into a list of additive terms.</span>

<span class="sd">    Example:</span>
<span class="sd">        utility_string = &#39;lambda_item + theta_user * alpha_item + gamma_user * beta_item * price_obs&#39;</span>
<span class="sd">        output = [</span>
<span class="sd">            {</span>
<span class="sd">                &#39;coefficient&#39;: [&#39;lambda_item&#39;],</span>
<span class="sd">                &#39;observable&#39;: None</span>
<span class="sd">            },</span>
<span class="sd">            {</span>
<span class="sd">                &#39;coefficient&#39;: [&#39;theta_user&#39;, &#39;alpha_item&#39;],</span>
<span class="sd">                &#39;observable&#39;: None</span>
<span class="sd">            },</span>
<span class="sd">            {</span>
<span class="sd">                &#39;coefficient&#39;: [&#39;gamma_user&#39;, &#39;beta_item&#39;],</span>
<span class="sd">                &#39;observable&#39;: &#39;price_obs&#39;</span>
<span class="sd">            }</span>
<span class="sd">            ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># split additive terms</span>
    <span class="n">coefficient_suffix</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;_item&#39;</span><span class="p">,</span> <span class="s1">&#39;_user&#39;</span><span class="p">,</span> <span class="s1">&#39;_constant&#39;</span><span class="p">)</span>
    <span class="n">observable_prefix</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;item_&#39;</span><span class="p">,</span> <span class="s1">&#39;user_&#39;</span><span class="p">,</span> <span class="s1">&#39;session_&#39;</span><span class="p">,</span> <span class="s1">&#39;price_&#39;</span><span class="p">,</span> <span class="s1">&#39;taste_&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">is_coefficient</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">any</span><span class="p">(</span><span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="n">suffix</span><span class="p">)</span> <span class="k">for</span> <span class="n">suffix</span> <span class="ow">in</span> <span class="n">coefficient_suffix</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">is_observable</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">any</span><span class="p">(</span><span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span> <span class="k">for</span> <span class="n">prefix</span> <span class="ow">in</span> <span class="n">observable_prefix</span><span class="p">)</span>

    <span class="n">additive_terms</span> <span class="o">=</span> <span class="n">utility_string</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; + &#39;</span><span class="p">)</span>
    <span class="n">additive_decomposition</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">additive_terms</span><span class="p">:</span>
        <span class="n">atom</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;coefficient&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;observable&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
        <span class="c1"># split multiplicative terms.</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">term</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; * &#39;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">is_coefficient</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="n">atom</span><span class="p">[</span><span class="s1">&#39;coefficient&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">is_observable</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="n">atom</span><span class="p">[</span><span class="s1">&#39;observable&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1"> term cannot be classified.&#39;</span><span class="p">)</span>
        <span class="n">additive_decomposition</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">atom</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">additive_decomposition</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">



<h3 id="bemb.model.bemb_flex_lightning" class="doc doc-heading">
        <code>bemb_flex_lightning</code>



</h3>

    <div class="doc doc-contents ">

      <p>PyTorch lightning wrapper for the BEMB Flex model, allows for more smooth model training and inference. You can still
use this package without using LitBEMBFlex.</p>
<p>Author: Tianyu Du
Update: Apr. 29, 2022</p>



  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h4 id="bemb.model.bemb_flex_lightning.LitBEMBFlex" class="doc doc-heading">
        <code>
LitBEMBFlex            (<span title="pytorch_lightning.core.lightning.LightningModule">LightningModule</span>)
        </code>



</h4>

    <div class="doc doc-contents ">


        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb_flex_lightning.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">LitBEMBFlex</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">num_seeds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The initialization method of the wrapper model.</span>

<span class="sd">        Args:</span>
<span class="sd">            learning_rate (float, optional): the learning rate of optimization. Defaults to 0.3.</span>
<span class="sd">            num_seeds (int, optional): number of random seeds for the Monte Carlo estimation in the variational inference.</span>
<span class="sd">                Defaults to 1.</span>
<span class="sd">            **kwargs: all keyword arguments used for constructing the wrapped BEMB model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># use kwargs to pass parameter to BEMB Torch.</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">BEMBFlex</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_needs</span> <span class="o">=</span> <span class="n">num_seeds</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calls the forward method of the wrapped BEMB model, please refer to the documentaton of the BEMB class</span>
<span class="sd">            for detailed definitions of the arguments.</span>

<span class="sd">        Args:</span>
<span class="sd">            args (_type_): arguments passed to the forward method of the wrapped BEMB model.</span>
<span class="sd">            kwargs (_type_): keyword arguments passed to the forward method of the wrapped BEMB model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            _type_: returns whatever the wrapped BEMB model returns.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">elbo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">elbo</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_seeds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_needs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;train_elbo&#39;</span><span class="p">,</span> <span class="n">elbo</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">elbo</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">_get_performance_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">pred_item</span><span class="p">:</span>
            <span class="n">log_p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;log_prob&#39;</span><span class="p">,</span>
                               <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;all_items&#39;</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">num_classes</span> <span class="o">=</span> <span class="n">log_p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">log_p</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">y_true</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">item_index</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">performance</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;acc&#39;</span><span class="p">:</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">),</span>
                           <span class="s1">&#39;ll&#39;</span><span class="p">:</span> <span class="o">-</span> <span class="n">metrics</span><span class="o">.</span><span class="n">log_loss</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_p</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_classes</span><span class="p">))}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># making binary station.</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;utility&#39;</span><span class="p">,</span>
                              <span class="n">return_scope</span><span class="o">=</span><span class="s1">&#39;item_index&#39;</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">y_true</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">performance</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;acc&#39;</span><span class="p">:</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)),</span>
                           <span class="s1">&#39;ll&#39;</span><span class="p">:</span> <span class="o">-</span> <span class="n">metrics</span><span class="o">.</span><span class="n">log_loss</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1E-5</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
                           <span class="c1">#    &#39;auc&#39;: metrics.roc_auc_score(y_true=y_true, y_score=y_pred),</span>
                           <span class="c1">#    &#39;f1&#39;: metrics.f1_score(y_true=y_true, y_pred=(y_pred &gt;= 0.5).astype(int))</span>
                           <span class="p">}</span>
        <span class="k">return</span> <span class="n">performance</span>

    <span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="c1"># LL = self.model.forward(batch, return_type=&#39;log_prob&#39;, return_scope=&#39;item_index&#39;, deterministic=True).mean()</span>
        <span class="c1"># self.log(&#39;val_log_likelihood&#39;, LL, prog_bar=True)</span>
        <span class="c1"># pred = self.model(batch)</span>
        <span class="c1"># performance = self.model.get_within_category_accuracy(pred, batch.label)</span>

        <span class="c1"># utility.</span>

        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_performance_dict</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;val_&#39;</span> <span class="o">+</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="c1"># LL = self.model.forward(batch, return_logit=False, all_items=False).mean()</span>
        <span class="c1"># self.log(&#39;test_log_likelihood&#39;, LL)</span>

        <span class="c1"># pred = self.model(batch, return_type=&#39;utility&#39;, return_scope=&#39;item_index&#39;, deterministic=True)</span>
        <span class="c1"># y_pred = torch.sigmoid(pred).cpu().numpy()</span>
        <span class="c1"># y_true = batch.label.cpu().numpy()</span>
        <span class="c1"># performance = {&#39;acc&#39;: metrics.accuracy_score(y_true=y_true, y_pred=(y_pred &gt;= 0.5).astype(int)),</span>
        <span class="c1">#                &#39;ll&#39;: - metrics.log_loss(y_true=y_true, y_pred=y_pred, eps=1E-5, labels=[0, 1]),</span>
        <span class="c1">#             #    &#39;auc&#39;: metrics.roc_auc_score(y_true=y_true, y_score=y_pred),</span>
        <span class="c1">#             #    &#39;f1&#39;: metrics.f1_score(y_true=y_true, y_pred=(y_pred &gt;= 0.5).astype(int))</span>
        <span class="c1">#                }</span>

        <span class="c1"># pred = self.model(batch)</span>
        <span class="c1"># performance = self.model.get_within_category_accuracy(pred, batch.label)</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_performance_dict</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;test_&#39;</span> <span class="o">+</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">optimizer</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bemb_flex_lightning.LitBEMBFlex.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">num_seeds</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h5>

    <div class="doc doc-contents ">

      <p>The initialization method of the wrapper model.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>learning_rate</code></td>
        <td><code>float</code></td>
        <td><p>the learning rate of optimization. Defaults to 0.3.</p></td>
        <td><code>0.3</code></td>
      </tr>
      <tr>
        <td><code>num_seeds</code></td>
        <td><code>int</code></td>
        <td><p>number of random seeds for the Monte Carlo estimation in the variational inference.
Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td></td>
        <td><p>all keyword arguments used for constructing the wrapped BEMB model.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb_flex_lightning.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">num_seeds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The initialization method of the wrapper model.</span>

<span class="sd">    Args:</span>
<span class="sd">        learning_rate (float, optional): the learning rate of optimization. Defaults to 0.3.</span>
<span class="sd">        num_seeds (int, optional): number of random seeds for the Monte Carlo estimation in the variational inference.</span>
<span class="sd">            Defaults to 1.</span>
<span class="sd">        **kwargs: all keyword arguments used for constructing the wrapped BEMB model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># use kwargs to pass parameter to BEMB Torch.</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">BEMBFlex</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_needs</span> <span class="o">=</span> <span class="n">num_seeds</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
</code></pre></div>
        </details>
    </div>

  </div>




  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bemb_flex_lightning.LitBEMBFlex.configure_optimizers" class="doc doc-heading">
<code class="highlight language-python"><span class="n">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you'd need one. But in the case of GANs or similar you might have multiple.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td><p>Any of these 6 options.</p>
<ul>
<li><strong>Single optimizer</strong>.</li>
<li><strong>List or Tuple</strong> of optimizers.</li>
<li><strong>Two lists</strong> - The first list has multiple optimizers, and the second has multiple LR schedulers
  (or multiple <code>lr_scheduler_config</code>).</li>
<li><strong>Dictionary</strong>, with an <code>"optimizer"</code> key, and (optionally) a <code>"lr_scheduler"</code>
  key whose value is a single LR scheduler or <code>lr_scheduler_config</code>.</li>
<li><strong>Tuple of dictionaries</strong> as described above, with an optional <code>"frequency"</code> key.</li>
<li><strong>None</strong> - Fit will run without any optimizer.</li>
</ul></td>
    </tr>
  </tbody>
</table>      <p>The <code>lr_scheduler_config</code> is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.</p>
<p>.. code-block:: python</p>
<pre><code>lr_scheduler_config = {
    # REQUIRED: The scheduler instance
    "scheduler": lr_scheduler,
    # The unit of the scheduler's step size, could also be 'step'.
    # 'epoch' updates the scheduler on epoch end whereas 'step'
    # updates it after a optimizer update.
    "interval": "epoch",
    # How many epochs/steps should pass between calls to
    # `scheduler.step()`. 1 corresponds to updating the learning
    # rate after every epoch/step.
    "frequency": 1,
    # Metric to to monitor for schedulers like `ReduceLROnPlateau`
    "monitor": "val_loss",
    # If set to `True`, will enforce that the value specified 'monitor'
    # is available when the scheduler is updated, thus stopping
    # training if not found. If set to `False`, it will only produce a warning
    "strict": True,
    # If using the `LearningRateMonitor` callback to monitor the
    # learning rate progress, this keyword can be used to specify
    # a custom logged name
    "name": None,
}
</code></pre>
<p>When there are schedulers in which the <code>.step()</code> method is conditioned on a value, such as the
:class:<code>torch.optim.lr_scheduler.ReduceLROnPlateau</code> scheduler, Lightning requires that the
<code>lr_scheduler_config</code> contains the keyword <code>"monitor"</code> set to the metric name that the scheduler
should be conditioned on.</p>
<p>.. testcode::</p>
<pre><code># The ReduceLROnPlateau scheduler requires a monitor
def configure_optimizers(self):
    optimizer = Adam(...)
    return {
        "optimizer": optimizer,
        "lr_scheduler": {
            "scheduler": ReduceLROnPlateau(optimizer, ...),
            "monitor": "metric_to_track",
            "frequency": "indicates how often the metric is updated"
            # If "monitor" references validation metrics, then "frequency" should be set to a
            # multiple of "trainer.check_val_every_n_epoch".
        },
    }


# In the case of two optimizers, only one using the ReduceLROnPlateau scheduler
def configure_optimizers(self):
    optimizer1 = Adam(...)
    optimizer2 = SGD(...)
    scheduler1 = ReduceLROnPlateau(optimizer1, ...)
    scheduler2 = LambdaLR(optimizer2, ...)
    return (
        {
            "optimizer": optimizer1,
            "lr_scheduler": {
                "scheduler": scheduler1,
                "monitor": "metric_to_track",
            },
        },
        {"optimizer": optimizer2, "lr_scheduler": scheduler2},
    )
</code></pre>
<p>Metrics can be made available to monitor by simply logging it using
<code>self.log('metric_to_track', metric_val)</code> in your :class:<code>~pytorch_lightning.core.lightning.LightningModule</code>.</p>
<p>!!! note
    The <code>frequency</code> value specified in a dict along with the <code>optimizer</code> key is an int corresponding
    to the number of sequential batches optimized with the specific optimizer.
    It should be given to none or to all of the optimizers.
    There is a difference between passing multiple optimizers in a list,
    and passing multiple optimizers in dictionaries with a frequency of 1:</p>
<pre><code>    - In the former case, all optimizers will operate on the given batch in each optimization step.
    - In the latter, only one optimizer will operate on the given batch at every step.

This is different from the ``frequency`` value specified in the ``lr_scheduler_config`` mentioned above.

.. code-block:: python

    def configure_optimizers(self):
        optimizer_one = torch.optim.SGD(self.model.parameters(), lr=0.01)
        optimizer_two = torch.optim.SGD(self.model.parameters(), lr=0.01)
        return [
            {"optimizer": optimizer_one, "frequency": 5},
            {"optimizer": optimizer_two, "frequency": 10},
        ]

In this example, the first optimizer will be used for the first 5 steps,
the second optimizer for the next 10 steps and that cycle will continue.
If an LR scheduler is specified for an optimizer using the ``lr_scheduler`` key in the above dict,
the scheduler will only be updated when its optimizer is being used.
</code></pre>
<p>Examples::</p>
<pre><code># most cases. no learning rate scheduler
def configure_optimizers(self):
    return Adam(self.parameters(), lr=1e-3)

# multiple optimizer case (e.g.: GAN)
def configure_optimizers(self):
    gen_opt = Adam(self.model_gen.parameters(), lr=0.01)
    dis_opt = Adam(self.model_dis.parameters(), lr=0.02)
    return gen_opt, dis_opt

# example with learning rate schedulers
def configure_optimizers(self):
    gen_opt = Adam(self.model_gen.parameters(), lr=0.01)
    dis_opt = Adam(self.model_dis.parameters(), lr=0.02)
    dis_sch = CosineAnnealing(dis_opt, T_max=10)
    return [gen_opt, dis_opt], [dis_sch]

# example with step-based learning rate schedulers
# each optimizer has its own scheduler
def configure_optimizers(self):
    gen_opt = Adam(self.model_gen.parameters(), lr=0.01)
    dis_opt = Adam(self.model_dis.parameters(), lr=0.02)
    gen_sch = {
        'scheduler': ExponentialLR(gen_opt, 0.99),
        'interval': 'step'  # called after each training step
    }
    dis_sch = CosineAnnealing(dis_opt, T_max=10) # called every epoch
    return [gen_opt, dis_opt], [gen_sch, dis_sch]

# example with optimizer frequencies
# see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1
# https://arxiv.org/abs/1704.00028
def configure_optimizers(self):
    gen_opt = Adam(self.model_gen.parameters(), lr=0.01)
    dis_opt = Adam(self.model_dis.parameters(), lr=0.02)
    n_critic = 5
    return (
        {'optimizer': dis_opt, 'frequency': n_critic},
        {'optimizer': gen_opt, 'frequency': 1}
    )
</code></pre>
<p>!!! note
    Some things to know:</p>
<pre><code>- Lightning calls ``.backward()`` and ``.step()`` on each optimizer and learning rate scheduler as needed.
- If you use 16-bit precision (``precision=16``), Lightning will automatically handle the optimizers.
- If you use multiple optimizers, :meth:`training_step` will have an additional ``optimizer_idx`` parameter.
- If you use :class:`torch.optim.LBFGS`, Lightning handles the closure function automatically for you.
- If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer
  at each training step.
- If you need to control how often those optimizers step or override the default ``.step()`` schedule,
  override the :meth:`optimizer_step` hook.
</code></pre>

        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb_flex_lightning.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">optimizer</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bemb_flex_lightning.LitBEMBFlex.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>Calls the forward method of the wrapped BEMB model, please refer to the documentaton of the BEMB class
    for detailed definitions of the arguments.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>args</code></td>
        <td><code>_type_</code></td>
        <td><p>arguments passed to the forward method of the wrapped BEMB model.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>kwargs</code></td>
        <td><code>_type_</code></td>
        <td><p>keyword arguments passed to the forward method of the wrapped BEMB model.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>_type_</code></td>
      <td><p>returns whatever the wrapped BEMB model returns.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb_flex_lightning.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calls the forward method of the wrapped BEMB model, please refer to the documentaton of the BEMB class</span>
<span class="sd">        for detailed definitions of the arguments.</span>

<span class="sd">    Args:</span>
<span class="sd">        args (_type_): arguments passed to the forward method of the wrapped BEMB model.</span>
<span class="sd">        kwargs (_type_): keyword arguments passed to the forward method of the wrapped BEMB model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        _type_: returns whatever the wrapped BEMB model returns.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bemb_flex_lightning.LitBEMBFlex.test_step" class="doc doc-heading">
<code class="highlight language-python"><span class="n">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>Operates on a single batch of data from the test set.
In this step you'd normally generate examples or calculate anything of interest
such as accuracy.</p>
<p>.. code-block:: python</p>
<pre><code># the pseudocode for these calls
test_outs = []
for test_batch in test_data:
    out = test_step(test_batch)
    test_outs.append(out)
test_epoch_end(test_outs)
</code></pre>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td></td>
        <td><p>The output of your :class:<code>~torch.utils.data.DataLoader</code>.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td></td>
        <td><p>The index of this batch.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dataloader_id</code></td>
        <td></td>
        <td><p>The index of the dataloader that produced this batch.
(only if multiple test dataloaders used).</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td><p>Any of.</p>
<ul>
<li>Any object or value</li>
<li><code>None</code> - Testing will skip to the next batch</li>
</ul></td>
    </tr>
  </tbody>
</table>      <p>.. code-block:: python</p>
<pre><code># if you have one test dataloader:
def test_step(self, batch, batch_idx):
    ...


# if you have multiple test dataloaders:
def test_step(self, batch, batch_idx, dataloader_idx=0):
    ...
</code></pre>
<p>Examples::</p>
<pre><code># CASE 1: A single test dataset
def test_step(self, batch, batch_idx):
    x, y = batch

    # implement your own
    out = self(x)
    loss = self.loss(out, y)

    # log 6 example images
    # or generated text... or whatever
    sample_imgs = x[:6]
    grid = torchvision.utils.make_grid(sample_imgs)
    self.logger.experiment.add_image('example_images', grid, 0)

    # calculate acc
    labels_hat = torch.argmax(out, dim=1)
    test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

    # log the outputs!
    self.log_dict({'test_loss': loss, 'test_acc': test_acc})
</code></pre>
<p>If you pass in multiple test dataloaders, :meth:<code>test_step</code> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<p>.. code-block:: python</p>
<pre><code># CASE 2: multiple test dataloaders
def test_step(self, batch, batch_idx, dataloader_idx=0):
    # dataloader_idx tells you which dataset this is.
    ...
</code></pre>
<p>!!! note
    If you don't need to test you don't need to implement this method.</p>
<p>!!! note
    When the :meth:<code>test_step</code> is called, the model has been put in eval mode and
    PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
    to training mode and gradients are enabled.</p>

        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb_flex_lightning.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># LL = self.model.forward(batch, return_logit=False, all_items=False).mean()</span>
    <span class="c1"># self.log(&#39;test_log_likelihood&#39;, LL)</span>

    <span class="c1"># pred = self.model(batch, return_type=&#39;utility&#39;, return_scope=&#39;item_index&#39;, deterministic=True)</span>
    <span class="c1"># y_pred = torch.sigmoid(pred).cpu().numpy()</span>
    <span class="c1"># y_true = batch.label.cpu().numpy()</span>
    <span class="c1"># performance = {&#39;acc&#39;: metrics.accuracy_score(y_true=y_true, y_pred=(y_pred &gt;= 0.5).astype(int)),</span>
    <span class="c1">#                &#39;ll&#39;: - metrics.log_loss(y_true=y_true, y_pred=y_pred, eps=1E-5, labels=[0, 1]),</span>
    <span class="c1">#             #    &#39;auc&#39;: metrics.roc_auc_score(y_true=y_true, y_score=y_pred),</span>
    <span class="c1">#             #    &#39;f1&#39;: metrics.f1_score(y_true=y_true, y_pred=(y_pred &gt;= 0.5).astype(int))</span>
    <span class="c1">#                }</span>

    <span class="c1"># pred = self.model(batch)</span>
    <span class="c1"># performance = self.model.get_within_category_accuracy(pred, batch.label)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_performance_dict</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;test_&#39;</span> <span class="o">+</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bemb_flex_lightning.LitBEMBFlex.training_step" class="doc doc-heading">
<code class="highlight language-python"><span class="n">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td></td>
        <td><p>class:<code>~torch.Tensor</code> | (:class:<code>~torch.Tensor</code>, ...) | [:class:<code>~torch.Tensor</code>, ...]):
The output of your :class:<code>~torch.utils.data.DataLoader</code>. A tensor, tuple or list.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>``int``</code></td>
        <td><p>Integer displaying index of this batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>optimizer_idx</code></td>
        <td><code>``int``</code></td>
        <td><p>When using multiple optimizers, this argument will also be present.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>hiddens</code></td>
        <td><code>``Any``</code></td>
        <td><p>Passed in if
:paramref:<code>~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps</code> &gt; 0.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Any of.

- </code></td>
      <td><p>class:<code>~torch.Tensor</code> - The loss tensor
- <code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code>
- <code>None</code> - Training will skip to the next batch. This is only for automatic optimization.
    This is not supported for multi-GPU, TPU, IPU, or DeepSpeed.</p></td>
    </tr>
  </tbody>
</table>      <p>In this step you'd normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example::</p>
<pre><code>def training_step(self, batch, batch_idx):
    x, y, z = batch
    out = self.encoder(x)
    loss = self.loss(out, x)
    return loss
</code></pre>
<p>If you define multiple optimizers, this step will be called with an additional
<code>optimizer_idx</code> parameter.</p>
<p>.. code-block:: python</p>
<pre><code># Multiple optimizers (e.g.: GANs)
def training_step(self, batch, batch_idx, optimizer_idx):
    if optimizer_idx == 0:
        # do training_step with encoder
        ...
    if optimizer_idx == 1:
        # do training_step with decoder
        ...
</code></pre>
<p>If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.</p>
<p>.. code-block:: python</p>
<pre><code># Truncated back-propagation through time
def training_step(self, batch, batch_idx, hiddens):
    # hiddens are the hidden states from the previous truncated backprop step
    out, hiddens = self.lstm(data, hiddens)
    loss = ...
    return {"loss": loss, "hiddens": hiddens}
</code></pre>
<p>!!! note
    The loss value shown in the progress bar is smoothed (averaged) over the last values,
    so it differs from the actual loss returned in train/validation step.</p>

        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb_flex_lightning.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">elbo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">elbo</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_seeds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_needs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;train_elbo&#39;</span><span class="p">,</span> <span class="n">elbo</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">elbo</span>
    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="bemb.model.bemb_flex_lightning.LitBEMBFlex.validation_step" class="doc doc-heading">
<code class="highlight language-python"><span class="n">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span></code>


</h5>

    <div class="doc doc-contents ">

      <p>Operates on a single batch of data from the validation set.
In this step you'd might generate examples or calculate anything of interest like accuracy.</p>
<p>.. code-block:: python</p>
<pre><code># the pseudocode for these calls
val_outs = []
for val_batch in val_data:
    out = validation_step(val_batch)
    val_outs.append(out)
validation_epoch_end(val_outs)
</code></pre>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td></td>
        <td><p>The output of your :class:<code>~torch.utils.data.DataLoader</code>.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td></td>
        <td><p>The index of this batch.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dataloader_idx</code></td>
        <td></td>
        <td><p>The index of the dataloader that produced this batch.
(only if multiple val dataloaders used)</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td><ul>
<li>Any object or value</li>
<li><code>None</code> - Validation will skip to the next batch</li>
</ul></td>
    </tr>
  </tbody>
</table>      <p>.. code-block:: python</p>
<pre><code># pseudocode of order
val_outs = []
for val_batch in val_data:
    out = validation_step(val_batch)
    if defined("validation_step_end"):
        out = validation_step_end(out)
    val_outs.append(out)
val_outs = validation_epoch_end(val_outs)
</code></pre>
<p>.. code-block:: python</p>
<pre><code># if you have one val dataloader:
def validation_step(self, batch, batch_idx):
    ...


# if you have multiple val dataloaders:
def validation_step(self, batch, batch_idx, dataloader_idx=0):
    ...
</code></pre>
<p>Examples::</p>
<pre><code># CASE 1: A single validation dataset
def validation_step(self, batch, batch_idx):
    x, y = batch

    # implement your own
    out = self(x)
    loss = self.loss(out, y)

    # log 6 example images
    # or generated text... or whatever
    sample_imgs = x[:6]
    grid = torchvision.utils.make_grid(sample_imgs)
    self.logger.experiment.add_image('example_images', grid, 0)

    # calculate acc
    labels_hat = torch.argmax(out, dim=1)
    val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

    # log the outputs!
    self.log_dict({'val_loss': loss, 'val_acc': val_acc})
</code></pre>
<p>If you pass in multiple val dataloaders, :meth:<code>validation_step</code> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<p>.. code-block:: python</p>
<pre><code># CASE 2: multiple validation dataloaders
def validation_step(self, batch, batch_idx, dataloader_idx=0):
    # dataloader_idx tells you which dataset this is.
    ...
</code></pre>
<p>!!! note
    If you don't need to validate you don't need to implement this method.</p>
<p>!!! note
    When the :meth:<code>validation_step</code> is called, the model has been put in eval mode
    and PyTorch gradients have been disabled. At the end of validation,
    the model goes back to training mode and gradients are enabled.</p>

        <details class="quote">
          <summary>Source code in <code>bemb/model/bemb_flex_lightning.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># LL = self.model.forward(batch, return_type=&#39;log_prob&#39;, return_scope=&#39;item_index&#39;, deterministic=True).mean()</span>
    <span class="c1"># self.log(&#39;val_log_likelihood&#39;, LL, prog_bar=True)</span>
    <span class="c1"># pred = self.model(batch)</span>
    <span class="c1"># performance = self.model.get_within_category_accuracy(pred, batch.label)</span>

    <span class="c1"># utility.</span>

    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_performance_dict</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;val_&#39;</span> <span class="o">+</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>




  </div>

    </div>

  </div>




  </div>

    </div>

  </div>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../api_torch_choice/" class="md-footer__link md-footer__link--prev" aria-label="Previous: API Reference Torch-Choice" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              API Reference Torch-Choice
            </div>
          </div>
        </a>
      
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.ed9748b7.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>